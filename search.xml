<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[JavaScript是如何工作的(三)]]></title>
    <url>%2F2017%2F10%2F05%2Fjavascript-how-work-3%2F</url>
    <content type="text"><![CDATA[几个礼拜之前我们开始一系列对于JavaScript以及其本质工作原理的深入挖掘：我们认为通过了解JavaScript的构建方式以及它们是如何共同合作的，你就能够写出更好的代码以及应用。 这个系列的第一篇博客专注于介绍对于引擎，运行时以及调用栈的概述。第二篇博客近距离地检测了Google V8 引擎的内部并且提供了一些如何写出更好的JavaScript代码的建议。 在第三篇博客中，我们将会讨论另外一个关键的话题。这个话题由于随着编程语言的逐渐成熟和复杂化，越来越被开发者所忽视，这个话题就是在日常工作中使用到的——内存管理。 概述语言，比如C，具有低层次的内存管理方法，比如malloc()以及free()。开发者利用这些方法精确地为操作系统分配以及释放内存。 同时，JavaScript会在创建一些变量（对象，字符串等等）的时候分配内存，并且会在这些不被使用之后“自动地”释放这些内存，这个过程被称为垃圾收集。这个看起来“自动化的”特性其实就是产生误解的原因，并且给JavaScript（以及其他高层次语言）开发者一个假象，他们不需要关心内存管理。大错特错。 即使是使用高层次语言，开发者应该对于内存管理有一定的理解（或者最基本的理解）。有时候自动的内存管理会存在一些问题（比如一些bug或者垃圾收集器的一些限制等等），对于这些开发者必须能够理解从而能够合适地处理（或者使用最小的代价以及代码债务去绕过这个问题）。 内存生命周期不管你在使用什么编程语言，内存的生命周期基本上都是一样的： 下面是对于周期中每一步所发生的情况的概述： 分配内存——操作系统为你的程序分配内存并且允许其使用。在低层次语言中（比如C），这正是开发者应该处理的操作。在高层次的语言，然而，就由语言帮你实现了。 使用内存——当你的程序确实在使用之前分配的内存的阶段。当你在使用你代码里面分配的变量的时候会发生读以及写操作。 释放内存——这个阶段就是释放你不再需要的内存，从而这些内存被释放并且能够再次被使用。和分配内存操作一样，这在低层次的语言也是开发者需要明确的操作。 对于调用栈以及内存堆有一个快速的概念认识，你可以阅读我们关于这个话题的第一篇博客。 什么是内存？在我们讲述JavaScript内存之前，我们将简要地讨论一下内存是什么以及它们是如何在 nutshell 中工作的。 在硬件层次上，计算机内存由大量的 寄存器 组成。每一个寄存器都包含一些晶体管并且能够存储一比特。单独的寄存器可以通过独特的标识符去访问，因此我们能够读取以及重写它们。因此，从概念上来说，我们可以认为我们的整个计算机内存就是一个我们能够读写的大型比特数组。 因为作为人类，我们不擅长直接基于比特进行思考以及算术，我们将它们组织成大规模群组，它们在一起可以代表一个数字。8个比特称为一个字节。除了字节，还有词（有时候是16比特，有时候是32比特）。 内存中存储了很多东西： 所有程序使用的变量和其他数据 程序的代码，包括操作系统的代码。 编译器和操作系统共同合作为你处理大部分的内存管理，但是我们建议你应该了解其内部的运行原理。 当你编译你的代码的时候，编译器将会检查原始数据类型并且提前计算好它们需要多少内存。需要的内存被分配给程序，这被称为栈空间。这些被分配给变量的空间被称为栈空间，因为一旦函数被调用，它们的内存就会增加到现有内存的上面。当它们终止的时候，它们就会以后进先出(LIFO)的顺序移除。比如，考虑下面的声明。 123int n; // 4 bytesint x[4]; // array of 4 elements, each 4 bytesdouble m; // 8 bytes 编译器能够立即计算出代码需要 4 + 4 × 4 + 8 = 28 字节 那就是它如何对于现有的整形以及双浮点型工作。大约20年前，整形典型都是2个字节，双浮点型是4个字节。你的代码不应该取决于当下基本数据类型的大小。 编译器将会插入能够与操作系统交互的代码，从而在栈上获取你需要存储变量需要的字节数。 在上述的例子中，编译器知道每一个变量的准确的内存地址。事实上，无论我们何时写变量 n ，这都会在内部转化为类似于“内存地址 4127963”的东西。 注意如果我们希望在这访问 x[4] 我们将会需要访问和 m 相关联的数据。这是因为我们在访问数组里面并不存在的元素——它比数组实际分配的最后一个元素 x[3] 要多4个字节，并且最后可能是阅读（或者重写）一些 m 的比特。这将很可能给程序的其他部分带来一些不良的后果。 当函数调用其它函数的时候，当它被调用的时候都会获取它自己的堆栈块。它在那保存了它所有的局部变量，但是还会有一个程序计数器记录它执行的位置。当这个函数执行完毕，它的内存块就可以再次用于其他目的。 动态分配不幸的是，当我们在编译的时候不知道变量需要多少内存的话事情可能就不那么简单。假设我们想做下面的事情：123456int n = readInput(); // reads input from the user...// create an array with "n" elements 在此，在编译阶段中，编译器就没有办法知道数组需要多少内存，因为它取决于用户的输入。 因此，它就不能够为栈上的变量分配空间。相反，我们的程序需要明确地询问操作运行时需要的空间数量。这个内存是从堆空间中分配出来的。动态内存和静态内存分配的区别总结如下表格： 为了深入地理解动态内存分配是如何工作的，我们需要花费更多的时间在指针，这个可能有点偏离这篇博客的话题。如果你感兴趣了解更多，在评论里面告诉我，我将会在后续的博客中挖掘更多的细节。 JavaScript中的分配现在我们将解释JavaScript中的第一步（分配内存）。 JavaScript 将开发者从内存分配的处理中解放出来——JavaScript自身可以利用声明变量来完成这些任务。 12345678910111213141516171819var n = 374; // allocates memory for a numbervar s = 'sessionstack'; // allocates memory for a string var o = &#123; a: 1, b: null&#125;; // allocates memory for an object and its contained valuesvar a = [1, null, 'str']; // (like object) allocates memory for the // array and its contained valuesfunction f(a) &#123; return a + 3;&#125; // allocates a function (which is a callable object)// function expressions also allocate an objectsomeElement.addEventListener('click', function() &#123; someElement.style.backgroundColor = 'blue';&#125;, false); 一些函数调用也会导致一些对象的分配： 1234var d = new Date(); // allocates a Date objectvar e = document.createElement('div'); // allocates a DOM element 能够分配新的值或者对象的方法： 1234567891011var s1 = 'sessionstack';var s2 = s1.substr(0, 3); // s2 is a new string// Since strings are immutable, // JavaScript may decide to not allocate memory, // but just store the [0, 3] range.var a1 = ['str1', 'str2'];var a2 = ['str3', 'str4'];var a3 = a1.concat(a2); // new array with 4 elements being// the concatenation of a1 and a2 elements 在JavaScript中使用内存基本上在JavaScript中分配内存，就意味着在其中读写。 这可以通过对一个变量或者一个对象的属性甚至是向函数传递一个参数来完成。 当内存不再需要的时候释放它大多数的内存管理的问题就来自于这个阶段。 最困难的任务就是如何知道何时被分配的不再需要了。它经常需要开发者决定在程序的什么地方某段内存不再需要了并且对其进行释放。 高层次语言内嵌了一个称为垃圾收集器的软件，他的任务就是跟踪内存分配并且用于需找不再需要的分配过的内存，并且自动地对其进行释放。 不幸的是，这个过程是一个近似，因为知道是否某块内存是需要的问题是不可决定的（无法通过算法解决） 大多数的垃圾收集器通过收集再也无法访问的内存工作，比如：指向它的所有变量都超出了作用域。然而，这依然是对于可以收集的内存空间的预估，因为在任何位置仍可能一些变量在作用域内指向这个内存，然而它再也不能被访问了。 垃圾收集器由于找到一些是“不再需要的”是不可决定的事实，垃圾收集实现了对一般问题的解决方案的限制。这一节将会解释理解主要的垃圾收集算法以及它们的限制的需要注意的事项。 内存引用垃圾收集算法依赖的主要概念之一就是引用。 在内存管理的上下文中，一个对象被称为是对于另外一个对象的引用，如果前者可以访问后者（隐含或明确的）。例如，一个JavaScript对象都有一个指向其原型的引用（隐含的引用） 在这个上下文中，“对象”的概念扩展到比普通的JavaScript对象要广并且包括函数作用域（或者全局词法作用域）。 词法作用域定义了变量名称是如何在嵌套函数中解析的：内部函数包含了父函数的作用域即使父函数已经返回了。 基于引用计数的垃圾收集器这是最简单的垃圾收集器算法。如果没有引用指向这个对象的时候，这个对象就被认为是“可以作为垃圾收集”。 请看如下代码： 1234567891011121314151617181920212223242526272829var o1 = &#123; o2: &#123; x: 1 &#125;&#125;;// 2 objects are created. // 'o2' is referenced by 'o1' object as one of its properties.// None can be garbage-collectedvar o3 = o1; // the 'o3' variable is the second thing that // has a reference to the object pointed by 'o1'. o1 = 1; // now, the object that was originally in 'o1' has a // single reference, embodied by the 'o3' variablevar o4 = o3.o2; // reference to 'o2' property of the object. // This object has now 2 references: one as // a property. // The other as the 'o4' variableo3 = '374'; // The object that was originally in 'o1' has now zero // references to it. // It can be garbage-collected. // However, what was its 'o2' property is still // referenced by the 'o4' variable, so it cannot be // freed.o4 = null; // what was the 'o2' property of the object originally in // 'o1' has zero references to it. // It can be garbage collected. 循环在产生问题当遇到循环的时候就会有一个限制。在下面的实例之中，创建两个对象，并且互相引用，因此就会产生一个循环。当函数调用结束之后它们会走出作用域之外，因此它们就没什么用并且可以被释放。但是，基于引用计数的算法认为这两个对象都会被至少引用一次，所以它俩都不会被垃圾收集器收集。 12345678function f() &#123; var o1 = &#123;&#125;; var o2 = &#123;&#125;; o1.p = o2; // o1 references o2 o2.p = o1; // o2 references o1. This creates a cycle.&#125;f(); 标记-清除算法为了决定哪个对象是需要的，算法会决定是否这个对象是可访问的。 这个算法由以下步骤组成： 这个垃圾收集器构建一个“roots”列表。Root是全局变量，被代码中的引用所保存。在 JavaScript中，“window”就是这样的作为root的全局变量的例子。 所有的root都会被监测并且被标志成活跃的（比如不是垃圾）。所有的子代也会递归地被监测。所有能够由root访问的一切都不会被认为是垃圾。 所有不再被标志成活跃的内存块都被认为是垃圾。这个收集器现在就可以释放这些内存并将它们返还给操作系统。 这个算法要优于之前的因为“一个具有0引用的对象”可以让一个对象不能够再被访问。但是相反的却不一定成立，比如我们遇到循环的时候。 在2012年，所有的现代浏览器都使用标记-清除垃圾收集器。过去几年，JavaScript垃圾收集（代数/增量/并行/并行垃圾收集）领域的所有改进都是对该算法（标记和扫描）的实现进行了改进，但并没有对垃圾收集算法本身的改进， 其目标是确定一个对象是否可达。 在这篇文章中，你可以得到更多关于垃圾收集追踪并且也覆盖到了关于标记-清除算法的优化。 循环不再是一个问题在上述的第一个例子中，在函数调用返回之后，这两个对象不能够被全局对象所访问。因此，垃圾收集器就会发现它们不能够被访问了。 即使在这两个对象之间存在着引用，它们再也不能从root访问了。 列举垃圾收集器的直观行为虽然垃圾收集器很方便，但它们自己也有自己的代价。 其中一个是非确定论。 换句话说，GC是不可预测的。 你不能真正地告诉你什么时候会收集。 这意味着在某些情况下，程序会使用实际需要的更多内存。 在其他情况下，特别敏感的应用程序可能会引起短暂暂停。 虽然非确定性意味着在执行集合时无法确定，但大多数GC实现共享在分配期间执行收集遍历的常见模式。 如果没有执行分配，大多数GC保持空闲状态。 考虑以下情况： 执行相当大的一组分配。 这些元素中的大多数（或全部）被标记为不可访问（假设我们将指向我们不再需要的缓存的引用置空）。 不再执行分配。 在这种情况下，大多数GC不会再运行收集处理。换句话说，即使存在对于收集器来说不可访问的引用，它们也不会被收集器所认领。严格意义来说这并不是泄露，但是依然会导致比平常更多的内存使用。 什么是内存泄露？实质上，内存泄漏可以被定义为应用程序不再需要的内存，但是由于某些原因不会返回到操作系统或可用内存池。 编程语言有支持管理内存的不同方法。 然而，某块内存是否被使用实际上是一个不可判定的问题。 换句话说，只有开发人员可以清楚一个内存是否可以返回到操作系统。 某些编程语言提供了帮助开发者执行此操作的功能。其他的则期望开发人员能够完全明确何时使用一块内存。 维基百科有关于手动和自动内存管理的好文章。 四种常见的JavaScript泄露全局变量JavaScript 使用一种有趣的方式处理未声明的变量：一个未声明变量的引用会在全局对象内部产生一个新的变量。在浏览器的情况，这个全局变量就会是window。换句话说： 123function foo(arg) &#123; bar = "some text";&#125; 等同于： 123function foo(arg) &#123; window.bar = "some text";&#125; 如果bar被期望仅仅在foo函数作用域内保持对变量的引用，并且你忘记使用var去声明它，一个意想不到的全局变量就产生了。 在这个例子中，泄露就仅仅是一个字符串并不会带来太多危害，但是它可能会变得更糟。 另外一种可能产生意外的全局变量的方式是： 1234567function foo() &#123; this.var1 = "potential accidental global";&#125;// Foo called on its own, this points to the global object (window)// rather than being undefined.foo(); 为了阻止这些错误的发生，可以在js文件头部添加’use strict’。这将会使用严格模式来解析 JavaScript 从而阻止意外的全局变量。了解更多关于JavaScript执行的模式。 即使我们讨论了未预期的全局变量，但仍然有很多代码用显式的全局变量填充。 这些定义是不可收集的（除非分配为null或重新分配）。 特别是，用于临时存储和处理大量信息的全局变量值得关注。 如果你必须使用全局变量来存储大量数据，请确保在完成之后将其分配为null或重新分配。 被遗忘的计时器和回调setInterval 在 JavaScript 中是经常被使用的。 大多数提供观察者和其他模式的回调函数库都会在调用自己的实例变得无法访问之后对其任何引用也设置为不可访问。 但是在setInterval的情况下，这样的代码很常见： 1234567var serverData = loadData();setInterval(function() &#123; var renderer = document.getElementById('renderer'); if(renderer) &#123; renderer.innerHTML = JSON.stringify(serverData); &#125;&#125;, 5000); //This will be executed every ~5 seconds. 这个例子说明了计时器可能发生的情况：计时器可能会产生再也不被需要的节点或者数据的引用。 renderer所代表的对象在未来可能被移除，让部分interval 处理器中代码变得不再被需要。然而，这个处理器不能够被收集因为interval依然活跃的（这个interval需要被停止从而表面这种情况）。如果这个interval处理器不能够被收集，那么它的依赖也不能够被收集。这意味这存储大量数据的severData也不能够被收集。 在这种观察者的情况下，做出准确的调用从而在不需要它们的时候立即将其移除是非常重要的（或者相关的对象被置为不可访问的）。 过去，以前特别重要的是某些浏览器（好的老IE 6）无法管理好循环引用（有关更多信息，请参见下文）。 如今，大多数浏览器一旦观察到的对象变得无法访问，就能收集观察者处理器，即使侦听器没有被明确删除。 但是，在处理对象之前，明确删除这些观察者仍然是一个很好的做法。 例如： 1234567891011121314151617var element = document.getElementById('launch-button');var counter = 0;function onClick(event) &#123; counter++; element.innerHtml = 'text ' + counter;&#125;element.addEventListener('click', onClick);// Do stuffelement.removeEventListener('click', onClick);element.parentNode.removeChild(element);// Now when element goes out of scope,// both element and onClick will be collected even in old browsers // that don't handle cycles well. 当今，现在浏览器（报错IE和Edge）都使用了现代的垃圾收集算法，其能够检测到这些循环并且进行适宜的处理。换句话说，再也不是严格需要在将节点置为不可访问之前调用removeEventListener 。 框架和库（如jQuery）在处理节点之前（在为其使用特定的API时）会删除侦听器。 这是由库内部处理的，这也确保没有泄漏，即使在有问题的浏览器下运行，如…是的，IE 6。 闭包JavaScript 开发的一个关键方面是闭包：一个可以访问外部（封闭）函数变量的内部函数。 由于JavaScript运行时的实现细节，可以通过以下方式泄漏内存： 1234567891011121314151617 var theThing = null; var replaceThing = function () &#123; var originalThing = theThing; var unused = function () &#123; if (originalThing) // a reference to 'originalThing' console.log("hi"); &#125;; theThing = &#123; longStr: new Array(1000000).join('*'), someMethod: function () &#123; console.log("message"); &#125; &#125;;&#125;;setInterval(replaceThing, 1000); 这个代码段会做一件事情：每次 replaceThing 被调用时，theThing 都会获取一个一个包含一个大数组的以及一个新的闭包（someMethod）。同时，unused 会保持一个指向originalThing引用的闭包（从上一个调用的theThing到replaceThing）。可能已经很迷惑了，是不是？重要的事情是一旦在相同的父级作用域为闭包产生作用域，这个作用域就会被共享。 在这种情况下，为someMethod闭包产生的作用域就会被unused 所共享。unused 具有对于originaThing的引用。即使 unused 不再被使用，someMethod依然可以通过replaceThing作用域之外的theThing来使用。并且由于somethod和unused 共享闭包作用域，unused指向originalThing的引用强迫其保持活跃（两个闭包之间的整个共享作用域）。这将会阻止垃圾手机。 当这个代码段重复运行时，可以观察到内存使用量的稳定增长。 当GC运行时，这不会变小。 实质上，创建了一个关闭的链接列表（其root以TheThing变量的形式），并且这些闭包的范围中的每一个都对大数组进行间接引用，导致相当大的泄漏。 这个问题由Meteor团队发现，他们有一篇很好的文章，详细描述了这个问题。 DOM 之外的引用有时将DOM节点存储在数据结构中可能是有用的。 假设要快速更新表中的几行内容。 存储对字典或数组中每个DOM行的引用可能是有意义的。 当发生这种情况时，会保留对同一DOM元素的两个引用：一个在DOM树中，另一个在字典中。 如果将来某个时候您决定删除这些行，则需要使两个引用置为不可访问。 1234567891011121314151617var elements = &#123; button: document.getElementById('button'), image: document.getElementById('image')&#125;;function doStuff() &#123; image.src = 'http://example.com/image_name.png';&#125;function removeImage() &#123; // The image is a direct child of the body element. document.body.removeChild(document.getElementById('image')); // At this point, we still have a reference to #button in the //global elements object. In other words, the button element is //still in memory and cannot be collected by the GC.&#125; 还有一个额外的考虑，当涉及对DOM树内部的内部或叶节点的引用时，必须考虑这一点。 假设你在JavaScript代码中保留对表格特定单元格（&lt;td&gt;标记）的引用。 有一天，你决定从DOM中删除该表，但保留对该单元格的引用。 直观地，可以假设GC将收集除了该单元格之外的所有内容。 实际上，这不会发生：该单元格是该表的子节点，并且孩子们保持对父代的引用。 也就是说，从JavaScript代码引用表格单元会导致整个表保留在内存中。 保持对DOM元素的引用时需要仔细考虑。 参考 https://segmentfault.com/a/1190000011229300]]></content>
      <categories>
        <category>javascript</category>
      </categories>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL优化原理]]></title>
    <url>%2F2017%2F09%2F29%2Fmysql-optimization-mechanism%2F</url>
    <content type="text"><![CDATA[说起MySQL的查询优化，相信大家收藏了一堆奇技淫巧：不能使用SELECT *、不使用NULL字段、合理创建索引、为字段选择合适的数据类型….. 你是否真的理解这些优化技巧？是否理解其背后的工作原理？在实际场景下性能真有提升吗？我想未必。因而理解这些优化建议背后的原理就尤为重要，希望本文能让你重新审视这些优化建议，并在实际业务场景下合理的运用。 MySQL逻辑架构如果能在头脑中构建一幅MySQL各组件之间如何协同工作的架构图，有助于深入理解MySQL服务器。下图展示了MySQL的逻辑架构图。MySQL逻辑架构整体分为三层，最上层为客户端层，并非MySQL所独有，诸如：连接处理、授权认证、安全等功能均在这一层处理。 MySQL大多数核心服务均在中间这一层，包括查询解析、分析、优化、缓存、内置函数(比如：时间、数学、加密等函数)。所有的跨存储引擎的功能也在这一层实现：存储过程、触发器、视图等。 最下层为存储引擎，其负责MySQL中的数据存储和提取。和Linux下的文件系统类似，每种存储引擎都有其优势和劣势。中间的服务层通过API与存储引擎通信，这些API接口屏蔽了不同存储引擎间的差异。 MySQL查询过程我们总是希望MySQL能够获得更高的查询性能，最好的办法是弄清楚MySQL是如何优化和执行查询的。一旦理解了这一点，就会发现：很多的查询优化工作实际上就是遵循一些原则让MySQL的优化器能够按照预想的合理方式运行而已。 当向MySQL发送一个请求的时候，MySQL到底做了些什么呢？ 客户端/服务端通信协议MySQL客户端/服务端通信协议是“半双工”的：在任一时刻，要么是服务器向客户端发送数据，要么是客户端向服务器发送数据，这两个动作不能同时发生。一旦一端开始发送消息，另一端要接收完整个消息才能响应它，所以我们无法也无须将一个消息切成小块独立发送，也没有办法进行流量控制。 客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置max_allowed_packet参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。 与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送。因而在实际开发中，尽量保持查询简单且只返回必需的数据，减小通信间数据包的大小和数量是一个非常好的习惯，这也是查询中尽量避免使用SELECT *以及加上LIMIT限制的原因之一。 查询缓存在解析一个查询语句前，如果查询缓存是打开的，那么MySQL会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。 MySQL将缓存存放在一个引用表（不要理解成table，可以认为是类似于HashMap的数据结构），通过一个哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL库中的系统表，其查询结果都不会被缓存。比如函数NOW()或者CURRENT_DATE()会因为不同的查询时间，返回不同的查询结果，再比如包含CURRENT_USER或者CONNECION_ID()的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。 既然是缓存，就会失效，那查询缓存何时失效呢？MySQL的查询缓存系统会跟踪查询中涉及的每个表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。正因为如此，在任何的写操作时，MySQL必须将对应表的所有缓存都设置为失效。如果查询缓存非常大或者碎片很多，这个操作就可能带来很大的系统消耗，甚至导致系统僵死一会儿。而且查询缓存对系统的额外消耗也不仅仅在写操作，读操作也不例外： 任何的查询语句在开始之前都必须经过检查，即使这条SQL语句永远不会命中缓存 如果查询结果可以被缓存，那么执行完成后，会将结果存入缓存，也会带来额外的系统消耗 基于此，我们要知道并不是什么情况下查询缓存都会提高系统性能，缓存和失效都会带来额外消耗，只有当缓存带来的资源节约大于其本身消耗的资源时，才会给系统带来性能提升。但要如何评估打开缓存是否能够带来性能提升是一件非常困难的事情，也不在本文讨论的范畴内。如果系统确实存在一些性能问题，可以尝试打开查询缓存，并在数据库设计上做一些优化，比如： 用多个小表代替一个大表，注意不要过度设计 批量插入代替循环单条插入 合理控制缓存空间大小，一般来说其大小设置为几十兆比较合适 可以通过SQL_CACHE和SQL_NO_CACHE来控制某个查询语句是否需要进行缓存 最后的忠告是不要轻易打开查询缓存，特别是写密集型应用。如果你实在是忍不住，可以将query_cache_type设置为DEMAND，这时只有加入SQL_CACHE的查询才会走缓存，其他查询则不会，这样可以非常自由地控制哪些查询需要被缓存。 当然查询缓存系统本身是非常复杂的，这里讨论的也只是很小的一部分，其他更深入的话题，比如：缓存是如何使用内存的？如何控制内存的碎片化？事务对查询缓存有何影响等等，读者可以自行阅读相关资料，这里权当抛砖引玉吧。 语法解析和预处理MySQL通过关键字将SQL语句进行解析，并生成一颗对应的解析树。这个过程解析器主要通过语法规则来验证和解析。比如SQL中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据MySQL规则进一步检查解析树是否合法。比如检查要查询的数据表和数据列是否存在等。 查询优化经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成查询计划。多数情况下，一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。 MySQL使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在MySQL可以通过查询当前会话的last_query_cost的值来得到其计算当前查询的成本。12mysql&gt; select * from t_message limit 10;...省略结果集 123456mysql&gt; show status like 'last_query_cost';+-----------------+-------------+| Variable_name | Value |+-----------------+-------------+| Last_query_cost | 6391.799000 |+-----------------+-------------+ 示例中的结果表示优化器认为大概需要做6391个数据页的随机查找才能完成上面的查询。这个结果是根据一些列的统计信息计算得来的，这些统计信息包括：每张表或者索引的页面个数、索引的基数、索引和数据行的长度、索引的分布情况等等。 有非常多的原因会导致MySQL选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL认为的最优跟我们想的不一样（我们希望执行时间尽可能短，但MySQL值选择它认为成本小的，但成本小并不意味着执行时间短）等等。 MySQL的查询优化器是一个非常复杂的部件，它使用了非常多的优化策略来生成一个最优的执行计划： 重新定义表的关联顺序（多张表关联查询时，并不一定按照SQL中指定的顺序进行，但有一些技巧可以指定关联顺序） 优化MIN()和MAX()函数（找某列的最小值，如果该列有索引，只需要查找B+Tree索引最左端，反之则可以找到最大值，具体原理见下文） 提前终止查询（比如：使用Limit时，查找到满足数量的结果集后会立即终止查询） 优化排序（在老版本MySQL会使用两次传输排序，即先读取行指针和需要排序的字段在内存中对其排序，然后再根据排序结果去读取数据行，而新版本采用的是单次传输排序，也就是一次读取所有的数据行，然后根据给定的列排序。对于I/O密集型应用，效率会高很多） 随着MySQL的不断发展，优化器使用的优化策略也在不断的进化，这里仅仅介绍几个非常常用且容易理解的优化策略，其他的优化策略，大家自行查阅吧。 查询执行引擎在完成解析和优化阶段以后，MySQL会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为handler API。查询过程中的每一张表由一个handler实例表示。实际上，MySQL在查询优化阶段就为每一张表创建了一个handler实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。 返回结果给客户端查询执行的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL仍然会返回这个查询的相关信息，比如该查询影响到的行数以及执行时间等。 如果查询缓存被打开且这个查询可以被缓存，MySQL也会将结果存放到缓存中。 结果集返回客户端是一个增量且逐步返回的过程。有可能MySQL在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足①中所描述的通信协议的数据包发送，再通过TCP协议进行传输，在传输过程中，可能对MySQL的数据包进行缓存然后批量发送。 总结回头总结一下MySQL整个查询执行过程，总的来说分为6个步骤： 客户端向MySQL服务器发送一条查询请求 服务器首先检查查询缓存，如果命中缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段 服务器进行SQL解析、预处理、再由优化器生成对应的执行计划 MySQL根据执行计划，调用存储引擎的API来执行查询 将结果返回给客户端，同时缓存查询结果 性能优化建议看了这么多，你可能会期待给出一些优化手段，是的，下面会从3个不同方面给出一些优化建议。但请等等，还有一句忠告要先送给你：不要听信你看到的关于优化的“绝对真理”，包括本文所讨论的内容，而应该是在实际的业务场景下通过测试来验证你关于执行计划以及响应时间的假设。 Scheme设计与数据类型优化选择数据类型只要遵循小而简单的原则就好，越小的数据类型通常会更快，占用更少的磁盘、内存，处理时需要的CPU周期也更少。越简单的数据类型在计算时需要更少的CPU周期，比如，整型就比字符操作代价低，因而会使用整型来存储ip地址，使用DATETIME来存储时间，而不是使用字符串。 这里总结几个可能容易理解错误的技巧： 通常来说把可为NULL的列改为NOT NULL不会对性能提升有多少帮助，只是如果计划在列上创建索引，就应该将该列设置为NOT NULL。 对整数类型指定宽度，比如INT(11)，没有任何卵用。INT使用32位（4个字节）存储空间，那么它的表示范围已经确定，所以INT(1)和INT(20)对于存储和计算是相同的。 UNSIGNED表示不允许负值，大致可以使正数的上限提高一倍。比如TINYINT存储范围是-128 ~ 127，而UNSIGNED TINYINT存储的范围却是0 - 255。 通常来讲，没有太大的必要使用DECIMAL数据类型。即使是在需要存储财务数据时，仍然可以使用BIGINT。比如需要精确到万分之一，那么可以将数据乘以一百万然后使用BIGINT存储。这样可以避免浮点数计算不准确和DECIMAL精确计算代价高的问题。 TIMESTAMP使用4个字节存储空间，DATETIME使用8个字节存储空间。因而，TIMESTAMP只能表示1970 - 2038年，比DATETIME表示的范围小得多，而且TIMESTAMP的值因时区不同而不同。 大多数情况下没有使用枚举类型的必要，其中一个缺点是枚举的字符串列表是固定的，添加和删除字符串（枚举选项）必须使用ALTER TABLE（如果只只是在列表末尾追加元素，不需要重建表）。 schema的列不要太多。原因是存储引擎的API工作时需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后在服务器层将缓冲内容解码成各个列，这个转换过程的代价是非常高的。如果列太多而实际使用的列又很少的话，有可能会导致CPU占用过高。 大表ALTER TABLE非常耗时，MySQL执行大部分修改表结果操作的方法是用新的结构创建一个张空表，从旧表中查出所有的数据插入新表，然后再删除旧表。尤其当内存不足而表又很大，而且还有很大索引的情况下，耗时更久。当然有一些奇技淫巧可以解决这个问题，有兴趣可自行查阅。 创建高性能索引索引是提高MySQL查询性能的一个重要途径，但过多的索引可能会导致过高的磁盘使用率以及过高的内存占用，从而影响应用程序的整体性能。应当尽量避免事后才想起添加索引，因为事后可能需要监控大量的SQL才能定位到问题所在，而且添加索引的时间肯定是远大于初始添加索引所需要的时间，可见索引的添加也是非常有技术含量的。 接下来将向你展示一系列创建高性能索引的策略，以及每条策略其背后的工作原理。但在此之前，先了解与索引相关的一些算法和数据结构，将有助于更好的理解后文的内容。 索引相关的数据结构和算法通常我们所说的索引是指B-Tree索引，它是目前关系型数据库中查找数据最为常用和有效的索引，大多数存储引擎都支持这种索引。使用B-Tree这个术语，是因为MySQL在CREATE TABLE或其它语句中使用了这个关键字，但实际上不同的存储引擎可能使用不同的数据结构，比如InnoDB就是使用的B+Tree。 B+Tree中的B是指balance，意为平衡。需要注意的是，B+树索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。 在介绍B+Tree前，先了解一下二叉查找树，它是一种经典的数据结构，其左子树的值总是小于根的值，右子树的值总是大于根的值，如下图①。如果要在这课树中查找值为5的记录，其大致流程：先找到根，其值为6，大于5，所以查找左子树，找到3，而5大于3，接着找3的右子树，总共找了3次。同样的方法，如果查找值为8的记录，也需要查找3次。所以二叉查找树的平均查找次数为(3 + 3 + 3 + 2 + 2 + 1) / 6 = 2.3次，而顺序查找的话，查找值为2的记录，仅需要1次，但查找值为8的记录则需要6次，所以顺序查找的平均查找次数为：(1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.3次，因此大多数情况下二叉查找树的平均查找速度比顺序查找要快。 由于二叉查找树可以任意构造，同样的值，可以构造出如图②的二叉查找树，显然这棵二叉树的查询效率和顺序查找差不多。若想二叉查找数的查询性能最高，需要这棵二叉查找树是平衡的，也即平衡二叉树（AVL树）。 平衡二叉树首先需要符合二叉查找树的定义，其次必须满足任何节点的两个子树的高度差不能大于1。显然图②不满足平衡二叉树的定义，而图①是一课平衡二叉树。平衡二叉树的查找性能是比较高的（性能最好的是最优二叉树），查询性能越好，维护的成本就越大。比如图①的平衡二叉树，当用户需要插入一个新的值9的节点时，就需要做出如下变动。 通过一次左旋操作就将插入后的树重新变为平衡二叉树是最简单的情况了，实际应用场景中可能需要旋转多次。至此我们可以考虑一个问题，平衡二叉树的查找效率还不错，实现也非常简单，相应的维护成本还能接受，为什么MySQL索引不直接使用平衡二叉树？ 随着数据库中数据的增加，索引本身大小随之增加，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级。可以想象一下一棵几百万节点的二叉树的深度是多少？如果将这么大深度的一颗二叉树放磁盘上，每读取一个节点，需要一次磁盘的I/O读取，整个查找的耗时显然是不能够接受的。那么如何减少查找过程中的I/O存取次数？ 一种行之有效的解决方法是减少树的深度，将二叉树变为m叉树（多路搜索树），而B+Tree就是一种多路搜索树。理解B+Tree时，只需要理解其最重要的两个特征即可：第一，所有的关键字（可以理解为数据）都存储在叶子节点（Leaf Page），非叶子节点（Index Page）并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。其次，所有的叶子节点由指针连接。如下图为高度为2的简化了的B+Tree。 怎么理解这两个特征？MySQL将每个节点的大小设置为一个页的整数倍（原因下文会介绍），也就是在节点空间大小一定的情况下，每个节点可以存储更多的内结点，这样每个结点能索引的范围更大更精确。所有的叶子节点使用指针链接的好处是可以进行区间访问，比如上图中，如果查找大于20而小于30的记录，只需要找到节点20，就可以遍历指针依次找到25、30。如果没有链接指针的话，就无法进行区间查找。这也是MySQL使用B+Tree作为索引存储结构的重要原因。 MySQL为何将节点大小设置为页的整数倍，这就需要理解磁盘的存储原理。磁盘本身存取就比主存慢很多，在加上机械运动损耗（特别是普通的机械硬盘），磁盘的存取速度往往是主存的几百万分之一，为了尽量减少磁盘I/O，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存，预读的长度一般为页的整数倍。 “页是计算机管理存储器的逻辑块，硬件及OS往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（许多OS中，页的大小通常为4K）。主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后一起返回，程序继续运行。” MySQL巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了读取一个节点只需一次I/O。假设B+Tree的高度为h，一次检索最多需要h-1I/O（根节点常驻内存），复杂度$O(h) = O(log_{M}N)$。实际应用场景中，M通常较大，常常超过100，因此树的高度一般都比较小，通常不超过3。 最后简单了解下B+Tree节点的操作，在整体上对索引的维护有一个大概的了解，虽然索引可以大大提高查询效率，但维护索引仍要花费很大的代价，因此合理的创建索引也就尤为重要。 仍以上面的树为例，我们假设每个节点只能存储4个内节点。首先要插入第一个节点28，如下图所示。 接着插入下一个节点70，在Index Page中查询后得知应该插入到50 - 70之间的叶子节点，但叶子节点已满，这时候就需要进行也分裂的操作，当前的叶子节点起点为50，所以根据中间值来拆分叶子节点，如下图所示。最后插入一个节点95，这时候Index Page和Leaf Page都满了，就需要做两次拆分，如下图所示。拆分后最终形成了这样一颗树。B+Tree为了保持平衡，对于新插入的值需要做大量的拆分页操作，而页的拆分需要I/O操作，为了尽可能的减少页的拆分操作，B+Tree也提供了类似于平衡二叉树的旋转功能。当Leaf Page已满但其左右兄弟节点没有满的情况下，B+Tree并不急于去做拆分操作，而是将记录移到当前所在页的兄弟节点上。通常情况下，左兄弟会被先检查用来做旋转操作。就比如上面第二个示例，当插入70的时候，并不会去做页拆分，而是左旋操作。 通过旋转操作可以最大限度的减少页分裂，从而减少索引维护过程中的磁盘的I/O操作，也提高索引维护效率。需要注意的是，删除节点跟插入节点类似，仍然需要旋转和拆分操作，这里就不再说明。 高性能策略通过上文，相信你对B+Tree的数据结构已经有了大致的了解，但MySQL中索引是如何组织数据的存储呢？以一个简单的示例来说明，假如有如下数据表：1234567CREATE TABLE People(last_name varchar(50) not null,first_name varchar(50) not null,dob date not null,gender enum(`m`,`f`) not null,key(last_name,first_name,dob)); 对于表中每一行数据，索引中包含了last_name、first_name、dob列的值，下图展示了索引是如何组织数据存储的。 可以看到，索引首先根据第一个字段来排列顺序，当名字相同时，则根据第三个字段，即出生日期来排序，正是因为这个原因，才有了索引的“最左原则”。 MySQL不会使用索引的情况：非独立的列“独立的列”是指索引列不能是表达式的一部分，也不能是函数的参数。比如：1select * from where id + 1 = 5 我们很容易看出其等价于 id = 4，但是MySQL无法自动解析这个表达式，使用函数是同样的道理。 前缀索引如果列很长，通常可以索引开始的部分字符，这样可以有效节约索引空间，从而提高索引效率。 多列索引和索引顺序在多数情况下，在多个列上建立独立的索引并不能提高查询性能。理由非常简单，MySQL不知道选择哪个索引的查询效率更好，所以在老版本，比如MySQL5.0之前就会随便选择一个列的索引，而新的版本会采用合并索引的策略。举个简单的例子，在一张电影演员表中，在actor_id和film_id两个列上都建立了独立的索引，然后有如下查询：1select film_id,actor_id from film_actor where actor_id = 1 or film_id = 1 老版本的MySQL会随机选择一个索引，但新版本做如下的优化：123select film_id,actor_id from film_actor where actor_id = 1union allselect film_id,actor_id from film_actor where film_id = 1 and actor_id &lt;&gt; 1 当出现多个索引做相交操作时（多个AND条件），通常来说一个包含所有相关列的索引要优于多个独立索引。 当出现多个索引做联合操作时（多个OR条件），对结果集的合并、排序等操作需要耗费大量的CPU和内存资源，特别是当其中的某些索引的选择性不高，需要返回合并大量数据时，查询成本更高。所以这种情况下还不如走全表扫描。 因此explain时如果发现有索引合并（Extra字段出现Using union），应该好好检查一下查询和表结构是不是已经是最优的，如果查询和表都没有问题，那只能说明索引建的非常糟糕，应当慎重考虑索引是否合适，有可能一个包含所有相关列的多列索引更适合。 前面我们提到过索引如何组织数据存储的，从图中可以看到多列索引时，索引的顺序对于查询是至关重要的，很明显应该把选择性更高的字段放到索引的前面，这样通过第一个字段就可以过滤掉大多数不符合条件的数据。 索引选择性是指不重复的索引值和数据表的总记录数的比值，选择性越高查询效率越高，因为选择性越高的索引可以让MySQL在查询时过滤掉更多的行。唯一索引的选择性是1，这时最好的索引选择性，性能也是最好的。 理解索引选择性的概念后，就不难确定哪个字段的选择性较高了，查一下就知道了，比如：1SELECT * FROM payment where staff_id = 2 and customer_id = 584 是应该创建(staff_id,customer_id)的索引还是应该颠倒一下顺序？执行下面的查询，哪个字段的选择性更接近1就把哪个字段索引前面就好。123select count(distinct staff_id)/count(*) as staff_id_selectivity,count(distinct customer_id)/count(*) as customer_id_selectivity,count(*) from payment 多数情况下使用这个原则没有任何问题，但仍然注意你的数据中是否存在一些特殊情况。举个简单的例子，比如要查询某个用户组下有过交易的用户信息：1select user_id from trade where user_group_id = 1 and trade_amount &gt; 0 MySQL为这个查询选择了索引(user_group_id,trade_amount)，如果不考虑特殊情况，这看起来没有任何问题，但实际情况是这张表的大多数数据都是从老系统中迁移过来的，由于新老系统的数据不兼容，所以就给老系统迁移过来的数据赋予了一个默认的用户组。这种情况下，通过索引扫描的行数跟全表扫描基本没什么区别，索引也就起不到任何作用。 推广开来说，经验法则和推论在多数情况下是有用的，可以指导我们开发和设计，但实际情况往往会更复杂，实际业务场景下的某些特殊情况可能会摧毁你的整个设计。 避免多个范围条件实际开发中，我们会经常使用多个范围条件，比如想查询某个时间段内登录过的用户：1select user.* from user where login_time &gt; '2017-04-01' and age between 18 and 30; 这个查询有一个问题：它有两个范围条件，login_time列和age列，MySQL可以使用login_time列的索引或者age列的索引，但无法同时使用它们。 覆盖索引如果一个索引包含或者说覆盖所有需要查询的字段的值，那么就没有必要再回表查询，这就称为覆盖索引。覆盖索引是非常有用的工具，可以极大的提高性能，因为查询只需要扫描索引会带来许多好处： 索引条目远小于数据行大小，如果只读取索引，极大减少数据访问量 索引是有按照列值顺序存储的，对于I/O密集型的范围查询要比随机从磁盘读取每一行数据的IO要少的多 使用索引扫描来排序MySQL有两种方式可以生产有序的结果集，其一是对结果集进行排序的操作，其二是按照索引顺序扫描得出的结果自然是有序的。如果explain的结果中type列的值为index表示使用了索引扫描来做排序。 扫描索引本身很快，因为只需要从一条索引记录移动到相邻的下一条记录。但如果索引本身不能覆盖所有需要查询的列，那么就不得不每扫描一条索引记录就回表查询一次对应的行。这个读取操作基本上是随机I/O，因此按照索引顺序读取数据的速度通常要比顺序地全表扫描要慢。 在设计索引时，如果一个索引既能够满足排序，又满足查询，是最好的。 只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向也一样时，才能够使用索引来对结果做排序。如果查询需要关联多张表，则只有ORDER BY子句引用的字段全部为第一张表时，才能使用索引做排序。ORDER BY子句和查询的限制是一样的，都要满足最左前缀的要求（有一种情况例外，就是最左的列被指定为常数，下面是一个简单的示例），其它情况下都需要执行排序操作，而无法利用索引排序。12-- 最左列为常数，索引：(date,staff_id,customer_id)select staff_id,customer_id from demo where date = '2015-06-01' order by staff_id,customer_id 冗余和重复索引冗余索引是指在相同的列上按照相同的顺序创建的相同类型的索引，应当尽量避免这种索引，发现后立即删除。比如有一个索引(A,B)，再创建索引(A)就是冗余索引。冗余索引经常发生在为表添加新索引时，比如有人新建了索引(A,B)，但这个索引不是扩展已有的索引(A)。 大多数情况下都应该尽量扩展已有的索引而不是创建新索引。但有极少情况下出现性能方面的考虑需要冗余索引，比如扩展已有索引而导致其变得过大，从而影响到其他使用该索引的查询。 删除长期未使用的索引定期删除一些长时间未使用过的索引是一个非常好的习惯。 总结关于索引这个话题打算就此打住，最后要说一句，索引并不总是最好的工具，只有当索引帮助提高查询速度带来的好处大于其带来的额外工作时，索引才是有效的。对于非常小的表，简单的全表扫描更高效。对于中到大型的表，索引就非常有效。对于超大型的表，建立和维护索引的代价随之增长，这时候其他技术也许更有效，比如分区表。最后的最后，explain后再提测是一种美德。 特定类型查询优化优化COUNT()查询COUNT()可能是被大家误解最多的函数了，它有两种不同的作用，其一是统计某个列值的数量，其二是统计行数。统计列值时，要求列值是非空的，它不会统计NULL。如果确认括号中的表达式不可能为空时，实际上就是在统计行数。最简单的就是当使用COUNT(*)时，并不是我们所想象的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计所有的行数。 我们最常见的误解也就在这儿，在括号内指定了一列却希望统计结果是行数，而且还常常误以为前者的性能会更好。但实际并非这样，如果要统计行数，直接使用COUNT(*)，意义清晰，且性能更好。 有时候某些业务场景并不需要完全精确的COUNT值，可以用近似值来代替，EXPLAIN出来的行数就是一个不错的近似值，而且执行EXPLAIN并不需要真正地去执行查询，所以成本非常低。通常来说，执行COUNT()都需要扫描大量的行才能获取到精确的数据，因此很难优化，MySQL层面还能做得也就只有覆盖索引了。如果不还能解决问题，只有从架构层面解决了，比如添加汇总表，或者使用redis这样的外部缓存系统。 优化关联查询在大数据场景下，表与表之间通过一个冗余字段来关联，要比直接使用JOIN有更好的性能。如果确实需要使用关联查询的情况下，需要特别注意的是： 确保ON和USING字句中的列上有索引。在创建索引的时候就要考虑到关联的顺序。当表A和表B用列c关联的时候，如果优化器关联的顺序是A、B，那么就不需要在A表的对应列上创建索引。没有用到的索引会带来额外的负担，一般来说，除非有其他理由，只需要在关联顺序中的第二张表的相应列上创建索引（具体原因下文分析）。 确保任何的GROUP BY和ORDER BY中的表达式只涉及到一个表中的列，这样MySQL才有可能使用索引来优化。 要理解优化关联查询的第一个技巧，就需要理解MySQL是如何执行关联查询的。当前MySQL关联执行的策略非常简单，它对任何的关联都执行嵌套循环关联操作，即先在一个表中循环取出单条数据，然后在嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为为止。然后根据各个表匹配的行，返回查询中需要的各个列。 太抽象了？以上面的示例来说明，比如有这样的一个查询：123SELECT A.xx,B.yyFROM A INNER JOIN B USING(c)WHERE A.xx IN (5,6) 假设MySQL按照查询中的关联顺序A、B来进行关联操作，那么可以用下面的伪代码表示MySQL如何完成这个查询：1234567891011outer_iterator = SELECT A.xx,A.c FROM A WHERE A.xx IN (5,6);outer_row = outer_iterator.next;while(outer_row) &#123; inner_iterator = SELECT B.yy FROM B WHERE B.c = outer_row.c; inner_row = inner_iterator.next; while(inner_row) &#123; output[inner_row.yy,outer_row.xx]; inner_row = inner_iterator.next; &#125; outer_row = outer_iterator.next;&#125; 可以看到，最外层的查询是根据A.xx列来查询的，A.c上如果有索引的话，整个关联查询也不会使用。再看内层的查询，很明显B.c上如果有索引的话，能够加速查询，因此只需要在关联顺序中的第二张表的相应列上创建索引即可。 优化LIMIT分页当需要分页操作时，通常会使用LIMIT加上偏移量的办法实现，同时加上合适的ORDER BY字句。如果有对应的索引，通常效率会不错，否则，MySQL需要做大量的文件排序操作。 一个常见的问题是当偏移量非常大的时候，比如：LIMIT 10000 20这样的查询，MySQL需要查询10020条记录然后只返回20条记录，前面的10000条都将被抛弃，这样的代价非常高。 优化这种查询一个最简单的办法就是尽可能的使用覆盖索引扫描，而不是查询所有的列。然后根据需要做一次关联查询再返回所有的列。对于偏移量很大时，这样做的效率会提升非常大。考虑下面的查询：1SELECT film_id,description FROM film ORDER BY title LIMIT 50,5; 如果这张表非常大，那么这个查询最好改成下面的样子：1234SELECT film.film_id,film.descriptionFROM film INNER JOIN (SELECT film_id FROM film ORDER BY title LIMIT 50,5) AS tmp USING(film_id); 这里的延迟关联将大大提升查询效率，让MySQL扫描尽可能少的页面，获取需要访问的记录后在根据关联列回原表查询所需要的列。 有时候如果可以使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描，这样就可以避免使用OFFSET，比如下面的查询：1SELECT id FROM t LIMIT 10000, 10; 改为：1SELECT id FROM t WHERE id &gt; 10000 LIMIT 10; 其它优化的办法还包括使用预先计算的汇总表，或者关联到一个冗余表，冗余表中只包含主键列和需要做排序的列。 优化UNIONMySQL处理UNION的策略是先创建临时表，然后再把各个查询结果插入到临时表中，最后再来做查询。因此很多优化策略在UNION查询中都没有办法很好的时候。经常需要手动将WHERE、LIMIT、ORDER BY等字句“下推”到各个子查询中，以便优化器可以充分利用这些条件先优化。 除非确实需要服务器去重，否则就一定要使用UNION ALL，如果没有ALL关键字，MySQL会给临时表加上DISTINCT选项，这会导致整个临时表的数据做唯一性检查，这样做的代价非常高。当然即使使用ALL关键字，MySQL总是将结果放入临时表，然后再读出，再返回给客户端。虽然很多时候没有这个必要，比如有时候可以直接把每个子查询的结果返回给客户端。 结语理解查询是如何执行以及时间都消耗在哪些地方，再加上一些优化过程的知识，可以帮助大家更好的理解MySQL，理解常见优化技巧背后的原理。希望本文中的原理、示例能够帮助大家更好的将理论和实践联系起来，更多的将理论知识运用到实践中。 其他也没啥说的了，给大家留两个思考题吧，可以在脑袋里想想答案，这也是大家经常挂在嘴边的，但很少有人会思考为什么？ 有非常多的程序员在分享时都会抛出这样一个观点：尽可能不要使用存储过程，存储过程非常不容易维护，也会增加使用成本，应该把业务逻辑放到客户端。既然客户端都能干这些事，那为什么还要存储过程？ JOIN本身也挺方便的，直接查询就好了，为什么还需要视图呢？]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从一个 NullPointerException 探究 Java 的自动装箱拆箱机制]]></title>
    <url>%2F2017%2F09%2F28%2Fjava-box-unbox%2F</url>
    <content type="text"><![CDATA[前天遇到了一个 NullPointerException，触发的代码类似下面这样：1234567891011public class Test &#123; public static long test(long value) &#123; return value; &#125; public static void main(String[] args) &#123; Long value = null; // ... test(value); &#125;&#125; main 方法里的代码实际上相当于调用 test(null);，为什么不直接这样写呢？因为编译不过，会报 错误: 不兼容的类型: &lt;空值&gt;无法转换为long。 抛出问题运行时提示 test(value); 这一行抛出 NullPointerException，但是看着以上代码会有些许困惑：以上代码里一个对象方法都没有调用啊，NullPointerException 从何而来？ 原因分析这时，如果留意到 test 方法接受的参数是 long 类型，而我们传入的是 Long 类型（虽然其实是 null），就会想到这会经历一次从类型 Long 到基本数据类型 long 的自动拆箱过程，那会不会是这个过程中抛出的 NullPointerException 呢？因为以前只知道 Java 为一些基础数据类型与对应的包装器类型之间提供了自动装箱拆箱机制，而并不知这机制的具体实现方法是怎么样的，正好学习一下。 用命令 javap -c Test 将以上代码编译出的 Test.class 文件进行反汇编，可以看到如下输出：1234567891011121314151617181920212223Compiled from "Test.java"public class Test &#123; public Test(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object."&lt;init&gt;":()V 4: return public static long test(long); Code: 0: lload_0 1: lreturn public static void main(java.lang.String[]); Code: 0: aconst_null 1: astore_1 2: aload_1 3: invokevirtual #2 // Method java/lang/Long.longValue:()J 6: invokestatic #3 // Method test:(J)J 9: pop2 10: return&#125; 从以上字节码及对应的注释可以看出，test(value); 这一行被编译后等同于如下代码：12long primitive = value.longValue();test(promitive); 相比实际代码，多出的 long primitive = value.longValue(); 这一行看起来就是自动拆箱的过程了，而我们传入的 value 为 null，value.longValue() 会抛出 NullPointerException，一切就解释得通了。用更简洁的代码表达出了更丰富的含义，这就是所谓的语法糖了。 证实猜想那么我们上面得出的自动拆箱机制的结论是否正确呢？选择一种其它基本数据类型，比如 int，来佐证一下：123456public class Test &#123; public static void main(String[] args) &#123; Integer value = 10; int primitive = value; &#125;&#125; 反汇编后对应的字节码：123456789101112131415161718Compiled from "Test.java"public class Test &#123; public Test(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object."&lt;init&gt;":()V 4: return public static void main(java.lang.String[]); Code: 0: bipush 10 2: invokestatic #2 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 5: astore_1 6: aload_1 7: invokevirtual #3 // Method java/lang/Integer.intValue:()I 10: istore_2 11: return&#125; 由以上字节码我们可以印证下文里的知识点了。 自动装箱与拆箱自动装箱与拆箱是 Java 1.5 引入的新特性，是一种语法糖。 在此之前，我们要创建一个值为 10 的 Integer 对象，只能写作：1Integer value = new Integer(10); 而现在，我们可以更方便地写为：1Integer value = 10; 定义与实现机制自动装箱，是指从基本数据类型值到其对应的包装类对象的自动转换。比如 Integer value = 10;，是通过调用 Integer.valueOf 方法实现转换的。 自动拆箱，是指从包装类对象到其对应的基本数据类型值的自动转换。比如 int primitive = value;，是通过调用 Integer.intValue 方法实现转换的。 基本数据类型 包装类型 装箱方法 拆箱方法 boolean Boolean Boolean.valueOf(boolean) Boolean.booleanValue() byte Byte Byte.valueOf(byte) Byte.byteValue() char Character Character.valueOf(char) Character.charValue() short Short Short.valueOf(short) Short.shortValue() int Integer Integer.valueOf(int) Integer.intValue() long Long Long.valueOf(long) Long.longValue() float Float Float.valueOf(float) Float.floatValue() double Double Double.valueOf(double) Double.doubleValue() 发生时机自动装箱与拆箱主要发生在以下四种时机： 赋值时； 比较时； 算术运算时； 方法调用时。 常见应用场景Case 112Integer value = 10; // 自动装箱（赋值时）int primitive = value; // 自动拆箱（方法调用时） Case 212345Integer value = 1000;// ...if (value &lt;= 1000) &#123; // 自动拆箱（比较时） // ...&#125; Case 31234List&lt;Integer&gt; list = new ArrayList&lt;&gt;();list.add(10); // 自动装箱（方法调用时）int i = list.get(0); // 自动拆箱（赋值时） 注：集合（Collections）里不能直接放入原始类型，集合只接收对象。 Case 41234ThreadLocal&lt;Integer&gt; local = new ThreadLocal&lt;Integer&gt;();local.set(10); // 自动装箱（方法调用时）int i = local.get(); // 自动拆箱（赋值时） 注：ThreadLocal 不能存储基本数据类型，只接收引用类型。 Case 51234567891011121314public void fun1(Integer value) &#123; //&#125;public void fun2(int value) &#123; //&#125;public void test() &#123; fun1(10); // 自动装箱（方法调用时） Integer value = 10; fun2(value); // 自动拆箱（方法调用时）&#125; Case 6123456Integer v1 = new Integer(10);Integer v2 = new Integer(20);int v3 = 30;int sum = v1 + v2; // 自动拆箱（算术运算时）sum = v1 + 30; // 自动拆箱（算术运算时） 相关知识点比较除 == 以外，包装类对象与基本数据类型值的比较，包装类对象与包装类对象之间的比较，都是自动拆箱后对基本数据类型值进行比较，所以，要注意这些类型间进行比较时自动拆箱可能引发的 NullPointerException。 == 比较特殊，因为可以用于判断左右是否为同一对象，所以两个包装类对象之间 ==，会用于判断是否为同一对象，而不会进行自动拆箱操作；包装类对象与基本数据类型值之间 ==，会自动拆箱。 示例代码：1234567891011121314Integer v1 = new Integer(10);Integer v2 = new Integer(20);if (v1 &lt; v2) &#123; // 自动拆箱 // ...&#125;if (v1 == v2) &#123; // 不拆箱 // ...&#125;if (v1 == 10) &#123; // 自动拆箱 // ...&#125; 缓存Java 为整型值包装类 Byte、Character、Short、Integer、Long 设置了缓存，用于存储一定范围内的值，详细如下： 类型 缓存值范围 Byte -128 ~ 127 Character 0 ~ 127 Short -128 ~ 127 Integer -128 ~ 127（可配置） Long -128 ~ 127 在一些情况下，比如自动装箱时，如果值在缓存值范围内，将不创建新对象，直接从缓存里取出对象返回，比如：1234567891011Integer v1 = 10;Integer v2 = 10;Integer v3 = new Integer(10);Integer v4 = 128;Integer v5 = 128;Integer v6 = Integer.valueOf(10);System.out.println(v1 == v2); // trueSystem.out.println(v1 == v3); // falseSystem.out.println(v4 == v5); // falseSystem.out.println(v1 == v6); // true 缓存实现机制这里使用了设计模式享元模式。 以 Short 类实现源码为例：12345678910111213141516171819202122232425// ...public final class Short extends Number implements Comparable&lt;Short&gt; &#123; // ... private static class ShortCache &#123; private ShortCache()&#123;&#125; static final Short cache[] = new Short[-(-128) + 127 + 1]; static &#123; for(int i = 0; i &lt; cache.length; i++) cache[i] = new Short((short)(i - 128)); &#125; &#125; // ... public static Short valueOf(short s) &#123; final int offset = 128; int sAsInt = s; if (sAsInt &gt;= -128 &amp;&amp; sAsInt &lt;= 127) &#123; // must cache return ShortCache.cache[sAsInt + offset]; &#125; return new Short(s); &#125; // ...&#125; 在第一次调用到 Short.valueOf(short) 方法时，将创建 -128 ~ 127 对应的 256 个对象缓存到堆内存里。 这种设计，在频繁用到这个范围内的值的时候效率较高，可以避免重复创建和回收对象，否则有可能闲置较多对象在内存中。 使用不当的情况自动装箱和拆箱这种语法糖为我们写代码带来了简洁和便利，但如果使用不当，也有可能带来负面影响。 性能的损耗1234567Integer sum = 0; for (int i = 1000; i &lt; 5000; i++) &#123; // 1. 先对 sum 进行自动拆箱 // 2. 加法 // 3. 自动装箱赋值给 sum，无法命中缓存，会 new Integer(int) sum = sum + i; &#125; 在循环过程中会分别调用 4000 次 Integer.intValue() 和 Integer.valueOf(int)，并 new 4000 个 Integer 对象，而这些操作将 sum 的类型改为 int 即可避免，节约运行时间和空间，提升性能。 java.lang.NullPointerException尝试对一个值为 null 的包装类对象进行自动拆箱，就有可能造成 NullPointerException。比如：12345678Integer v1 = null;int v2 = v1; // NullPointerExceptionif (v1 &gt; 10) &#123; // NullPointerException // ...&#125;int v3 = v1 + 10; // NullPointerException 还有一种更隐蔽的情形12345public class Test &#123; public static void main(String[] args) &#123; long value = true ? null : 1; // NullPointerException &#125;&#125; 这实际上还是对一个值为 null 的 Long 类型进行自动拆箱，反汇编代码：12345678910111213141516Compiled from "Test.java"public class Test &#123; public Test(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object."&lt;init&gt;":()V 4: return public static void main(java.lang.String[]); Code: 0: aconst_null 1: checkcast #2 // class java/lang/Long 4: invokevirtual #3 // Method java/lang/Long.longValue:()J 7: lstore_1 8: return&#125; 转载 http://mazhuang.org/2017/08/20/java-auto-boxing-unboxing/]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>装箱</tag>
        <tag>拆箱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go基础-iota]]></title>
    <url>%2F2017%2F09%2F18%2Fgo-basic-iota%2F</url>
    <content type="text"><![CDATA[iota这个关键字，用来实现枚举的功能，但是用起来很奇怪，其实最后表示的还是常量。 先上代码12345678910111213141516171819package mainimport ( "fmt")const( a= 10+iota b c d)func main()&#123; fmt.Println(a) fmt.Println(b) fmt.Println(c) fmt.Println(d)&#125; 结果：123410111213 其实上面代码等价于123456const( a= 10+0 b= 10+1 c= 10+2 d= 10+3) 看出规律了吧，只要iota出现一次，就累加一次，而且一旦出现一次，就算后面不使用这个iota关键字，接下来的变量都会套用前面的表达式来计算，所以b,c,d用的就是a的表达式 再上代码1234567891011121314151617181920212223package mainimport ( "fmt")const( a= 10+iota b c d e=1+iota f)func main()&#123; fmt.Println(a) fmt.Println(b) fmt.Println(c) fmt.Println(d) fmt.Println(e) fmt.Println(f)&#125; 这个的输出，应该能猜出来了吧1234561011121356 f很明显是由于e的表达式变了，所以是套用了e的表达式，而不用a的表达式 最后的代码1234567891011121314151617181920212223242526package mainimport ( "fmt")const( a= 10+iota b c)const( d=1+iota e f)func main()&#123; fmt.Println(a) fmt.Println(b) fmt.Println(c) fmt.Println(d) fmt.Println(e) fmt.Println(f)&#125; 结果：123456101112123 很明显，iota只能在一个代码块累加，在另外的代码块就又重置了。 总结其实iota在go就是一个常量，定义在builtin.go这个源文件1234// iota is a predeclared identifier representing the untyped integer ordinal// number of the current const specification in a (usually parenthesized)// const declaration. It is zero-indexed.const iota = 0 // Untyped int. 本文代码在 https://github.com/ejunjsh/go-code/tree/master/iota]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis数据结构-对象]]></title>
    <url>%2F2017%2F09%2F18%2Fredis-object%2F</url>
    <content type="text"><![CDATA[前言在前面的数个章节里， 我们陆续介绍了 Redis 用到的所有主要数据结构， 比如简单动态字符串（SDS）、双端链表、字典、压缩列表、整数集合， 等等。 Redis 并没有直接使用这些数据结构来实现键值对数据库， 而是基于这些数据结构创建了一个对象系统， 这个系统包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象， 每种对象都用到了至少一种我们前面所介绍的数据结构。 通过这五种不同类型的对象， Redis 可以在执行命令之前， 根据对象的类型来判断一个对象是否可以执行给定的命令。 使用对象的另一个好处是， 我们可以针对不同的使用场景， 为对象设置多种不同的数据结构实现， 从而优化对象在不同场景下的使用效率。 除此之外， Redis 的对象系统还实现了基于引用计数技术的内存回收机制： 当程序不再使用某个对象的时候， 这个对象所占用的内存就会被自动释放； 另外， Redis 还通过引用计数技术实现了对象共享机制， 这一机制可以在适当的条件下， 通过让多个数据库键共享同一个对象来节约内存。 最后， Redis 的对象带有访问时间记录信息， 该信息可以用于计算数据库键的空转时长， 在服务器启用了 maxmemory 功能的情况下， 空转时长较大的那些键可能会优先被服务器删除。 本章接下来将逐一介绍以上提到的 Redis 对象系统的各个特性。 对象的类型与编码Redis 使用对象来表示数据库中的键和值， 每次当我们在 Redis 的数据库中新创建一个键值对时， 我们至少会创建两个对象， 一个对象用作键值对的键（键对象）， 另一个对象用作键值对的值（值对象）。 举个例子， 以下 SET 命令在数据库中创建了一个新的键值对， 其中键值对的键是一个包含了字符串值 “msg” 的对象， 而键值对的值则是一个包含了字符串值 “hello world” 的对象：12redis&gt; SET msg "hello world"OK Redis 中的每个对象都由一个 redisObject 结构表示， 该结构中和保存数据有关的三个属性分别是 type 属性、 encoding 属性和 ptr 属性：1234567891011121314typedef struct redisObject &#123; // 类型 unsigned type:4; // 编码 unsigned encoding:4; // 指向底层实现数据结构的指针 void *ptr; // ...&#125; robj; 类型对象的 type 属性记录了对象的类型， 这个属性的值可以是表 8-1 列出的常量的其中一个。 类型常量 对象的名称 REDIS_STRING 字符串对象 REDIS_LIST 列表对象 REDIS_HASH 哈希对象 REDIS_SET 集合对象 REDIS_ZSET 有序集合对象 对于 Redis 数据库保存的键值对来说， 键总是一个字符串对象， 而值则可以是字符串对象、列表对象、哈希对象、集合对象或者有序集合对象的其中一种， 因此： 当我们称呼一个数据库键为“字符串键”时， 我们指的是“这个数据库键所对应的值为字符串对象”； 当我们称呼一个键为“列表键”时， 我们指的是“这个数据库键所对应的值为列表对象”， 诸如此类。 TYPE 命令的实现方式也与此类似， 当我们对一个数据库键执行 TYPE 命令时， 命令返回的结果为数据库键对应的值对象的类型， 而不是键对象的类型：123456789101112131415161718192021222324252627282930313233343536373839# 键为字符串对象，值为字符串对象redis&gt; SET msg "hello world"OKredis&gt; TYPE msgstring# 键为字符串对象，值为列表对象redis&gt; RPUSH numbers 1 3 5(integer) 6redis&gt; TYPE numberslist# 键为字符串对象，值为哈希对象redis&gt; HMSET profile name Tome age 25 career ProgrammerOKredis&gt; TYPE profilehash# 键为字符串对象，值为集合对象redis&gt; SADD fruits apple banana cherry(integer) 3redis&gt; TYPE fruitsset# 键为字符串对象，值为有序集合对象redis&gt; ZADD price 8.5 apple 5.0 banana 6.0 cherry(integer) 3redis&gt; TYPE pricezset 表 8-2 列出了 TYPE 命令在面对不同类型的值对象时所产生的输出。 对象 对象 type 属性的值 TYPE 命令的输出 字符串对象 REDIS_STRING “string” 列表对象 REDIS_LIST “list” 哈希对象 REDIS_HASH “hash” 集合对象 REDIS_SET “set” 有序集合对象 REDIS_ZSET “zset” 编码和底层实现对象的 ptr 指针指向对象的底层实现数据结构， 而这些数据结构由对象的 encoding 属性决定。 encoding 属性记录了对象所使用的编码， 也即是说这个对象使用了什么数据结构作为对象的底层实现， 这个属性的值可以是表 8-3 列出的常量的其中一个。 编码常量 编码所对应的底层数据结构 REDIS_ENCODING_INT long 类型的整数 REDIS_ENCODING_EMBSTR embstr 编码的简单动态字符串 REDIS_ENCODING_RAW 简单动态字符串 REDIS_ENCODING_HT 字典 REDIS_ENCODING_LINKEDLIST 双端链表 REDIS_ENCODING_ZIPLIST 压缩列表 REDIS_ENCODING_INTSET 整数集合 REDIS_ENCODING_SKIPLIST 跳跃表和字典 每种类型的对象都至少使用了两种不同的编码， 表 8-4 列出了每种类型的对象可以使用的编码。 类型 编码 对象 REDIS_STRING REDIS_ENCODING_INT 使用整数值实现的字符串对象。 REDIS_STRING REDIS_ENCODING_EMBSTR 使用 embstr 编码的简单动态字符串实现的字符串对象。 REDIS_STRING REDIS_ENCODING_RAW 使用简单动态字符串实现的字符串对象。 REDIS_LIST REDIS_ENCODING_ZIPLIST 使用压缩列表实现的列表对象。 REDIS_LIST REDIS_ENCODING_LINKEDLIST 使用双端链表实现的列表对象。 REDIS_HASH REDIS_ENCODING_ZIPLIST 使用压缩列表实现的哈希对象。 REDIS_HASH REDIS_ENCODING_HT 使用字典实现的哈希对象。 REDIS_SET REDIS_ENCODING_INTSET 使用整数集合实现的集合对象。 REDIS_SET REDIS_ENCODING_HT 使用字典实现的集合对象。 REDIS_ZSET REDIS_ENCODING_ZIPLIST 使用压缩列表实现的有序集合对象。 REDIS_ZSET REDIS_ENCODING_SKIPLIST 使用跳跃表和字典实现的有序集合对象。 使用 OBJECT ENCODING 命令可以查看一个数据库键的值对象的编码：1234567891011121314151617181920212223redis&gt; SET msg "hello wrold"OKredis&gt; OBJECT ENCODING msg"embstr"redis&gt; SET story "long long long long long long ago ..."OKredis&gt; OBJECT ENCODING story"raw"redis&gt; SADD numbers 1 3 5(integer) 3redis&gt; OBJECT ENCODING numbers"intset"redis&gt; SADD numbers "seven"(integer) 1redis&gt; OBJECT ENCODING numbers"hashtable" 表 8-5 列出了不同编码的对象所对应的 OBJECT ENCODING 命令输出。 对象所使用的底层数据结构 编码常量 OBJECT ENCODING 命令输出 整数 REDIS_ENCODING_INT “int” embstr 编码的简单动态字符串（SDS） REDIS_ENCODING_EMBSTR “embstr” 简单动态字符串 REDIS_ENCODING_RAW “raw” 字典 REDIS_ENCODING_HT “hashtable” 双端链表 REDIS_ENCODING_LINKEDLIST “linkedlist” 压缩列表 REDIS_ENCODING_ZIPLIST “ziplist” 整数集合 REDIS_ENCODING_INTSET “intset” 跳跃表和字典 REDIS_ENCODING_SKIPLIST “skiplist” 通过 encoding 属性来设定对象所使用的编码， 而不是为特定类型的对象关联一种固定的编码， 极大地提升了 Redis 的灵活性和效率， 因为 Redis 可以根据不同的使用场景来为一个对象设置不同的编码， 从而优化对象在某一场景下的效率。 举个例子， 在列表对象包含的元素比较少时， Redis 使用压缩列表作为列表对象的底层实现： 因为压缩列表比双端链表更节约内存， 并且在元素数量较少时， 在内存中以连续块方式保存的压缩列表比起双端链表可以更快被载入到缓存中； 随着列表对象包含的元素越来越多， 使用压缩列表来保存元素的优势逐渐消失时， 对象就会将底层实现从压缩列表转向功能更强、也更适合保存大量元素的双端链表上面； 其他类型的对象也会通过使用多种不同的编码来进行类似的优化。 在接下来的内容中， 我们将分别介绍 Redis 中的五种不同类型的对象， 说明这些对象底层所使用的编码方式， 列出对象从一种编码转换成另一种编码所需的条件， 以及同一个命令在多种不同编码上的实现方法。 字符串对象字符串对象的编码可以是 int 、 raw 或者 embstr 。 如果一个字符串对象保存的是整数值， 并且这个整数值可以用 long 类型来表示， 那么字符串对象会将整数值保存在字符串对象结构的 ptr 属性里面（将 void* 转换成 long ）， 并将字符串对象的编码设置为 int 。 举个例子， 如果我们执行以下 SET 命令， 那么服务器将创建一个如图 8-1 所示的 int 编码的字符串对象作为 number 键的值：12345redis&gt; SET number 10086OKredis&gt; OBJECT ENCODING number"int" 如果字符串对象保存的是一个字符串值， 并且这个字符串值的长度大于 39 字节， 那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串值， 并将对象的编码设置为 raw 。 举个例子， 如果我们执行以下命令， 那么服务器将创建一个如图 8-2 所示的 raw 编码的字符串对象作为 story 键的值：12345678redis&gt; SET story "Long, long, long ago there lived a king ..."OKredis&gt; STRLEN story(integer) 43redis&gt; OBJECT ENCODING story"raw" 如果字符串对象保存的是一个字符串值， 并且这个字符串值的长度小于等于 39 字节， 那么字符串对象将使用 embstr 编码的方式来保存这个字符串值。 embstr 编码是专门用于保存短字符串的一种优化编码方式， 这种编码和 raw 编码一样， 都使用 redisObject 结构和 sdshdr 结构来表示字符串对象， 但 raw 编码会调用两次内存分配函数来分别创建 redisObject 结构和 sdshdr 结构， 而 embstr 编码则通过调用一次内存分配函数来分配一块连续的空间， 空间中依次包含 redisObject 和 sdshdr 两个结构， 如图 8-3 所示。embstr 编码的字符串对象在执行命令时， 产生的效果和 raw 编码的字符串对象执行命令时产生的效果是相同的， 但使用 embstr 编码的字符串对象来保存短字符串值有以下好处： embstr 编码将创建字符串对象所需的内存分配次数从 raw 编码的两次降低为一次。释放 embstr 编码的字符串对象只需要调用一次内存释放函数， 而释放 raw 编码的字符串对象需要调用两次内存释放函数。因为 embstr 编码的字符串对象的所有数据都保存在一块连续的内存里面， 所以这种编码的字符串对象比起 raw 编码的字符串对象能够更好地利用缓存带来的优势。作为例子， 以下命令创建了一个 embstr 编码的字符串对象作为 msg 键的值， 值对象的样子如图 8-4 所示：12345redis&gt; SET msg "hello"OKredis&gt; OBJECT ENCODING msg"embstr" 最后要说的是， 可以用 long double 类型表示的浮点数在 Redis 中也是作为字符串值来保存的： 如果我们要保存一个浮点数到字符串对象里面， 那么程序会先将这个浮点数转换成字符串值， 然后再保存起转换所得的字符串值。 举个例子， 执行以下代码将创建一个包含 3.14 的字符串表示 “3.14” 的字符串对象：12345redis&gt; SET pi 3.14OKredis&gt; OBJECT ENCODING pi"embstr" 在有需要的时候， 程序会将保存在字符串对象里面的字符串值转换回浮点数值， 执行某些操作， 然后再将执行操作所得的浮点数值转换回字符串值， 并继续保存在字符串对象里面。 举个例子， 如果我们执行以下代码的话：12345redis&gt; INCRBYFLOAT pi 2.0"5.14"redis&gt; OBJECT ENCODING pi"embstr" 那么程序首先会取出字符串对象里面保存的字符串值 “3.14” ， 将它转换回浮点数值 3.14 ， 然后把 3.14 和 2.0 相加得出的值 5.14 转换成字符串 “5.14” ， 并将这个 “5.14” 保存到字符串对象里面。 表 8-6 总结并列出了字符串对象保存各种不同类型的值所使用的编码方式。 值 编码 可以用 long 类型保存的整数。 int 可以用 long double 类型保存的浮点数。 embstr 或者 raw 字符串值， 或者因为长度太大而没办法用 long 类型表示的整数， 又或者因为长度太大而没办法用 long double 类型表示的浮点数。 embstr 或者 raw 编码的转换int 编码的字符串对象和 embstr 编码的字符串对象在条件满足的情况下， 会被转换为 raw 编码的字符串对象。 对于 int 编码的字符串对象来说， 如果我们向对象执行了一些命令， 使得这个对象保存的不再是整数值， 而是一个字符串值， 那么字符串对象的编码将从 int 变为 raw 。 在下面的示例中， 我们通过 APPEND 命令， 向一个保存整数值的字符串对象追加了一个字符串值， 因为追加操作只能对字符串值执行， 所以程序会先将之前保存的整数值 10086 转换为字符串值 “10086” ， 然后再执行追加操作， 操作的执行结果就是一个 raw 编码的、保存了字符串值的字符串对象：1234567891011121314redis&gt; SET number 10086OKredis&gt; OBJECT ENCODING number"int"redis&gt; APPEND number " is a good number!"(integer) 23redis&gt; GET number"10086 is a good number!"redis&gt; OBJECT ENCODING number"raw" 另外， 因为 Redis 没有为 embstr 编码的字符串对象编写任何相应的修改程序 （只有 int 编码的字符串对象和 raw 编码的字符串对象有这些程序）， 所以 embstr 编码的字符串对象实际上是只读的： 当我们对 embstr 编码的字符串对象执行任何修改命令时， 程序会先将对象的编码从 embstr 转换成 raw ， 然后再执行修改命令； 因为这个原因， embstr 编码的字符串对象在执行修改命令之后， 总会变成一个 raw 编码的字符串对象。 以下代码展示了一个 embstr 编码的字符串对象在执行 APPEND 命令之后， 对象的编码从 embstr 变为 raw 的例子：1234567891011redis&gt; SET msg "hello world"OKredis&gt; OBJECT ENCODING msg"embstr"redis&gt; APPEND msg " again!"(integer) 18redis&gt; OBJECT ENCODING msg"raw" 列表对象列表对象的编码可以是 ziplist 或者 linkedlist 。 ziplist 编码的列表对象使用压缩列表作为底层实现， 每个压缩列表节点（entry）保存了一个列表元素。 举个例子， 如果我们执行以下 RPUSH 命令， 那么服务器将创建一个列表对象作为 numbers 键的值：12redis&gt; RPUSH numbers 1 "three" 5(integer) 3 如果 numbers 键的值对象使用的是 ziplist 编码， 这个这个值对象将会是图 8-5 所展示的样子。另一方面， linkedlist 编码的列表对象使用双端链表作为底层实现， 每个双端链表节点（node）都保存了一个字符串对象， 而每个字符串对象都保存了一个列表元素。 举个例子， 如果前面所说的 numbers 键创建的列表对象使用的不是 ziplist 编码， 而是 linkedlist 编码， 那么 numbers 键的值对象将是图 8-6 所示的样子。注意， linkedlist 编码的列表对象在底层的双端链表结构中包含了多个字符串对象， 这种嵌套字符串对象的行为在稍后介绍的哈希对象、集合对象和有序集合对象中都会出现， 字符串对象是 Redis 五种类型的对象中唯一一种会被其他四种类型对象嵌套的对象。 编码转换当列表对象可以同时满足以下两个条件时， 列表对象使用 ziplist 编码： 列表对象保存的所有字符串元素的长度都小于 64 字节； 列表对象保存的元素数量小于 512 个； 不能满足这两个条件的列表对象需要使用 linkedlist 编码。 注意以上两个条件的上限值是可以修改的， 具体请看配置文件中关于 list-max-ziplist-value 选项和 list-max-ziplist-entries 选项的说明。 对于使用 ziplist 编码的列表对象来说， 当使用 ziplist 编码所需的两个条件的任意一个不能被满足时， 对象的编码转换操作就会被执行： 原本保存在压缩列表里的所有列表元素都会被转移并保存到双端链表里面， 对象的编码也会从 ziplist 变为 linkedlist 。 以下代码展示了列表对象因为保存了长度太大的元素而进行编码转换的情况：1234567891011121314# 所有元素的长度都小于 64 字节redis&gt; RPUSH blah "hello" "world" "again"(integer) 3redis&gt; OBJECT ENCODING blah"ziplist"# 将一个 65 字节长的元素推入列表对象中redis&gt; RPUSH blah "wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww"(integer) 4# 编码已改变redis&gt; OBJECT ENCODING blah"linkedlist" 除此之外， 以下代码展示了列表对象因为保存的元素数量过多而进行编码转换的情况：1234567891011121314151617# 列表对象包含 512 个元素redis&gt; EVAL "for i=1,512 do redis.call('RPUSH', KEYS[1], i) end" 1 "integers"(nil)redis&gt; LLEN integers(integer) 512redis&gt; OBJECT ENCODING integers"ziplist"# 再向列表对象推入一个新元素，使得对象保存的元素数量达到 513 个redis&gt; RPUSH integers 513(integer) 513# 编码已改变redis&gt; OBJECT ENCODING integers"linkedlist" 哈希对象哈希对象的编码可以是 ziplist 或者 hashtable 。 ziplist 编码的哈希对象使用压缩列表作为底层实现， 每当有新的键值对要加入到哈希对象时， 程序会先将保存了键的压缩列表节点推入到压缩列表表尾， 然后再将保存了值的压缩列表节点推入到压缩列表表尾， 因此： 保存了同一键值对的两个节点总是紧挨在一起， 保存键的节点在前， 保存值的节点在后； 先添加到哈希对象中的键值对会被放在压缩列表的表头方向， 而后来添加到哈希对象中的键值对会被放在压缩列表的表尾方向。 举个例子， 如果我们执行以下 HSET 命令， 那么服务器将创建一个列表对象作为 profile 键的值：12345678redis&gt; HSET profile name "Tom"(integer) 1redis&gt; HSET profile age 25(integer) 1redis&gt; HSET profile career "Programmer"(integer) 1 如果 profile 键的值对象使用的是 ziplist 编码， 那么这个值对象将会是图 8-9 所示的样子， 其中对象所使用的压缩列表如图 8-10 所示。 另一方面， hashtable 编码的哈希对象使用字典作为底层实现， 哈希对象中的每个键值对都使用一个字典键值对来保存： 字典的每个键都是一个字符串对象， 对象中保存了键值对的键； 字典的每个值都是一个字符串对象， 对象中保存了键值对的值。 举个例子， 如果前面 profile 键创建的不是 ziplist 编码的哈希对象， 而是 hashtable 编码的哈希对象， 那么这个哈希对象应该会是图 8-11 所示的样子。 编码转换当哈希对象可以同时满足以下两个条件时， 哈希对象使用 ziplist 编码： 哈希对象保存的所有键值对的键和值的字符串长度都小于 64 字节； 哈希对象保存的键值对数量小于 512 个； 不能满足这两个条件的哈希对象需要使用 hashtable 编码。 注意这两个条件的上限值是可以修改的， 具体请看配置文件中关于 hash-max-ziplist-value 选项和 hash-max-ziplist-entries 选项的说明。 对于使用 ziplist 编码的列表对象来说， 当使用 ziplist 编码所需的两个条件的任意一个不能被满足时， 对象的编码转换操作就会被执行： 原本保存在压缩列表里的所有键值对都会被转移并保存到字典里面， 对象的编码也会从 ziplist 变为 hashtable 。 以下代码展示了哈希对象因为键值对的键长度太大而引起编码转换的情况：1234567891011121314# 哈希对象只包含一个键和值都不超过 64 个字节的键值对redis&gt; HSET book name "Mastering C++ in 21 days"(integer) 1redis&gt; OBJECT ENCODING book"ziplist"# 向哈希对象添加一个新的键值对，键的长度为 66 字节redis&gt; HSET book long_long_long_long_long_long_long_long_long_long_long_description "content"(integer) 1# 编码已改变redis&gt; OBJECT ENCODING book"hashtable" 除了键的长度太大会引起编码转换之外， 值的长度太大也会引起编码转换， 以下代码展示了这种情况的一个示例：1234567891011121314# 哈希对象只包含一个键和值都不超过 64 个字节的键值对redis&gt; HSET blah greeting "hello world"(integer) 1redis&gt; OBJECT ENCODING blah"ziplist"# 向哈希对象添加一个新的键值对，值的长度为 68 字节redis&gt; HSET blah story "many string ... many string ... many string ... many string ... many"(integer) 1# 编码已改变redis&gt; OBJECT ENCODING blah"hashtable" 最后， 以下代码展示了哈希对象因为包含的键值对数量过多而引起编码转换的情况：1234567891011121314151617181920# 创建一个包含 512 个键值对的哈希对象redis&gt; EVAL "for i=1, 512 do redis.call('HSET', KEYS[1], i, i) end" 1 "numbers"(nil)redis&gt; HLEN numbers(integer) 512redis&gt; OBJECT ENCODING numbers"ziplist"# 再向哈希对象添加一个新的键值对，使得键值对的数量变成 513 个redis&gt; HMSET numbers "key" "value"OKredis&gt; HLEN numbers(integer) 513# 编码改变redis&gt; OBJECT ENCODING numbers"hashtable" 集合对象集合对象的编码可以是 intset 或者 hashtable 。 intset 编码的集合对象使用整数集合作为底层实现， 集合对象包含的所有元素都被保存在整数集合里面。 举个例子， 以下代码将创建一个如图 8-12 所示的 intset 编码集合对象：12redis&gt; SADD numbers 1 3 5(integer) 3 另一方面， hashtable 编码的集合对象使用字典作为底层实现， 字典的每个键都是一个字符串对象， 每个字符串对象包含了一个集合元素， 而字典的值则全部被设置为 NULL 。 举个例子， 以下代码将创建一个如图 8-13 所示的 hashtable 编码集合对象：12redis&gt; SADD fruits "apple" "banana" "cherry"(integer) 3 编码的转换当集合对象可以同时满足以下两个条件时， 对象使用 intset 编码： 集合对象保存的所有元素都是整数值； 集合对象保存的元素数量不超过 512 个； 不能满足这两个条件的集合对象需要使用 hashtable 编码。 注意第二个条件的上限值是可以修改的， 具体请看配置文件中关于 set-max-intset-entries 选项的说明。 对于使用 intset 编码的集合对象来说， 当使用 intset 编码所需的两个条件的任意一个不能被满足时， 对象的编码转换操作就会被执行： 原本保存在整数集合中的所有元素都会被转移并保存到字典里面， 并且对象的编码也会从 intset 变为 hashtable 。 举个例子， 以下代码创建了一个只包含整数元素的集合对象， 该对象的编码为 intset ：12345redis&gt; SADD numbers 1 3 5(integer) 3redis&gt; OBJECT ENCODING numbers"intset" 不过， 只要我们向这个只包含整数元素的集合对象添加一个字符串元素， 集合对象的编码转移操作就会被执行：12345redis&gt; SADD numbers "seven"(integer) 1redis&gt; OBJECT ENCODING numbers"hashtable" 除此之外， 如果我们创建一个包含 512 个整数元素的集合对象， 那么对象的编码应该会是 intset ：12345678redis&gt; EVAL "for i=1, 512 do redis.call('SADD', KEYS[1], i) end" 1 integers(nil)redis&gt; SCARD integers(integer) 512redis&gt; OBJECT ENCODING integers"intset" 但是， 只要我们再向集合添加一个新的整数元素， 使得这个集合的元素数量变成 513 ， 那么对象的编码转换操作就会被执行：12345678redis&gt; SADD integers 10086(integer) 1redis&gt; SCARD integers(integer) 513redis&gt; OBJECT ENCODING integers"hashtable" 有序集合对象有序集合的编码可以是 ziplist 或者 skiplist 。 ziplist 编码的有序集合对象使用压缩列表作为底层实现， 每个集合元素使用两个紧挨在一起的压缩列表节点来保存， 第一个节点保存元素的成员（member）， 而第二个元素则保存元素的分值（score）。 压缩列表内的集合元素按分值从小到大进行排序， 分值较小的元素被放置在靠近表头的方向， 而分值较大的元素则被放置在靠近表尾的方向。 举个例子， 如果我们执行以下 ZADD 命令， 那么服务器将创建一个有序集合对象作为 price 键的值：12redis&gt; ZADD price 8.5 apple 5.0 banana 6.0 cherry(integer) 3 如果 price 键的值对象使用的是 ziplist 编码， 那么这个值对象将会是图 8-14 所示的样子， 而对象所使用的压缩列表则会是 8-15 所示的样子。skiplist 编码的有序集合对象使用 zset 结构作为底层实现， 一个 zset 结构同时包含一个字典和一个跳跃表：1234567typedef struct zset &#123; zskiplist *zsl; dict *dict;&#125; zset; zset 结构中的 zsl 跳跃表按分值从小到大保存了所有集合元素， 每个跳跃表节点都保存了一个集合元素： 跳跃表节点的 object 属性保存了元素的成员， 而跳跃表节点的 score 属性则保存了元素的分值。 通过这个跳跃表， 程序可以对有序集合进行范围型操作， 比如 ZRANK 、 ZRANGE 等命令就是基于跳跃表 API 来实现的。 除此之外， zset 结构中的 dict 字典为有序集合创建了一个从成员到分值的映射， 字典中的每个键值对都保存了一个集合元素： 字典的键保存了元素的成员， 而字典的值则保存了元素的分值。 通过这个字典， 程序可以用 O(1) 复杂度查找给定成员的分值， ZSCORE 命令就是根据这一特性实现的， 而很多其他有序集合命令都在实现的内部用到了这一特性。 有序集合每个元素的成员都是一个字符串对象， 而每个元素的分值都是一个 double 类型的浮点数。 值得一提的是， 虽然 zset 结构同时使用跳跃表和字典来保存有序集合元素， 但这两种数据结构都会通过指针来共享相同元素的成员和分值， 所以同时使用跳跃表和字典来保存集合元素不会产生任何重复成员或者分值， 也不会因此而浪费额外的内存。 为什么有序集合需要同时使用跳跃表和字典来实现？在理论上来说， 有序集合可以单独使用字典或者跳跃表的其中一种数据结构来实现， 但无论单独使用字典还是跳跃表， 在性能上对比起同时使用字典和跳跃表都会有所降低。 举个例子， 如果我们只使用字典来实现有序集合， 那么虽然以 O(1) 复杂度查找成员的分值这一特性会被保留， 但是， 因为字典以无序的方式来保存集合元素， 所以每次在执行范围型操作 —— 比如 ZRANK 、 ZRANGE 等命令时， 程序都需要对字典保存的所有元素进行排序， 完成这种排序需要至少 O(N \log N) 时间复杂度， 以及额外的 O(N) 内存空间 （因为要创建一个数组来保存排序后的元素）。 另一方面， 如果我们只使用跳跃表来实现有序集合， 那么跳跃表执行范围型操作的所有优点都会被保留， 但因为没有了字典， 所以根据成员查找分值这一操作的复杂度将从 O(1) 上升为 O(\log N) 。 因为以上原因， 为了让有序集合的查找和范围型操作都尽可能快地执行， Redis 选择了同时使用字典和跳跃表两种数据结构来实现有序集合。 举个例子， 如果前面 price 键创建的不是 ziplist 编码的有序集合对象， 而是 skiplist 编码的有序集合对象， 那么这个有序集合对象将会是图 8-16 所示的样子， 而对象所使用的 zset 结构将会是图 8-17 所示的样子。 注意为了展示方便， 图 8-17 在字典和跳跃表中重复展示了各个元素的成员和分值， 但在实际中， 字典和跳跃表会共享元素的成员和分值， 所以并不会造成任何数据重复， 也不会因此而浪费任何内存。 编码的转换当有序集合对象可以同时满足以下两个条件时， 对象使用 ziplist 编码： 有序集合保存的元素数量小于 128 个； 有序集合保存的所有元素成员的长度都小于 64 字节； 不能满足以上两个条件的有序集合对象将使用 skiplist 编码。 注意以上两个条件的上限值是可以修改的， 具体请看配置文件中关于 zset-max-ziplist-entries 选项和 zset-max-ziplist-value 选项的说明。 对于使用 ziplist 编码的有序集合对象来说， 当使用 ziplist 编码所需的两个条件中的任意一个不能被满足时， 程序就会执行编码转换操作， 将原本储存在压缩列表里面的所有集合元素转移到 zset 结构里面， 并将对象的编码从 ziplist 改为 skiplist 。 以下代码展示了有序集合对象因为包含了过多元素而引发编码转换的情况：123456789101112131415161718192021# 对象包含了 128 个元素redis&gt; EVAL "for i=1, 128 do redis.call('ZADD', KEYS[1], i, i) end" 1 numbers(nil)redis&gt; ZCARD numbers(integer) 128redis&gt; OBJECT ENCODING numbers"ziplist"# 再添加一个新元素redis&gt; ZADD numbers 3.14 pi(integer) 1# 对象包含的元素数量变为 129 个redis&gt; ZCARD numbers(integer) 129# 编码已改变redis&gt; OBJECT ENCODING numbers"skiplist" 以下代码则展示了有序集合对象因为元素的成员过长而引发编码转换的情况：1234567891011121314# 向有序集合添加一个成员只有三字节长的元素redis&gt; ZADD blah 1.0 www(integer) 1redis&gt; OBJECT ENCODING blah"ziplist"# 向有序集合添加一个成员为 66 字节长的元素redis&gt; ZADD blah 2.0 oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo(integer) 1# 编码已改变redis&gt; OBJECT ENCODING blah"skiplist" 重点回顾 Redis 数据库中的每个键值对的键和值都是一个对象。 Redis 共有字符串、列表、哈希、集合、有序集合五种类型的对象， 每种类型的对象至少都有两种或以上的编码方式， 不同的编码可以在不同的使用场景上优化对象的使用效率。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis数据结构-压缩列表]]></title>
    <url>%2F2017%2F09%2F17%2Fredis-ziplist%2F</url>
    <content type="text"><![CDATA[压缩列表（ziplist）是列表键和哈希键的底层实现之一。当一个列表键只包含少量列表项， 并且每个列表项要么就是小整数值， 要么就是长度比较短的字符串， 那么 Redis 就会使用压缩列表来做列表键的底层实现。 压缩列表的构成压缩列表是 Redis 为了节约内存而开发的， 由一系列特殊编码的连续内存块组成的顺序型（sequential）数据结构。一个压缩列表可以包含任意多个节点（entry）， 每个节点可以保存一个字节数组或者一个整数值。图 7-1 展示了压缩列表的各个组成部分， 表 7-1 则记录了各个组成部分的类型、长度、以及用途。 表 7-1 压缩列表各个组成部分的详细说明 属性 类型 长度 用途 zlbytes uint32_t 4 字节 记录整个压缩列表占用的内存字节数：在对压缩列表进行内存重分配， 或者计算 zlend 的位置时使用。 zltail uint32_t 4 字节 记录压缩列表表尾节点距离压缩列表的起始地址有多少字节： 通过这个偏移量，程序无须遍历整个压缩列表就可以确定表尾节点的地址。 zllen uint16_t 2 字节 记录了压缩列表包含的节点数量： 当这个属性的值小于 UINT16_MAX （65535）时， 这个属性的值就是压缩列表包含节点的数量； 当这个值等于 UINT16_MAX 时， 节点的真实数量需要遍历整个压缩列表才能计算得出。 entryX 列表节点 不定 压缩列表包含的各个节点，节点的长度由节点保存的内容决定。 zlend uint8_t 1 字节 特殊值 0xFF （十进制 255 ），用于标记压缩列表的末端。 图 7-2 展示了一个压缩列表示例： 列表 zlbytes 属性的值为 0x50 （十进制 80）， 表示压缩列表的总长为 80 字节。 列表 zltail 属性的值为 0x3c （十进制 60）， 这表示如果我们有一个指向压缩列表起始地址的指针 p ， 那么只要用指针 p 加上偏移量 60 ， 就可以计算出表尾节点 entry3 的地址。 列表 zllen 属性的值为 0x3 （十进制 3）， 表示压缩列表包含三个节点。 图 7-3 展示了另一个压缩列表示例： 列表 zlbytes 属性的值为 0xd2 （十进制 210）， 表示压缩列表的总长为 210 字节。 列表 zltail 属性的值为 0xb3 （十进制 179）， 这表示如果我们有一个指向压缩列表起始地址的指针 p ， 那么只要用指针 p 加上偏移量 179 ， 就可以计算出表尾节点 entry5 的地址。 列表 zllen 属性的值为 0x5 （十进制 5）， 表示压缩列表包含五个节点。 压缩列表节点的构成每个压缩列表节点可以保存一个字节数组或者一个整数值， 其中， 字节数组可以是以下三种长度的其中一种： 长度小于等于 63 （2^{6}-1）字节的字节数组； 长度小于等于 16383 （2^{14}-1） 字节的字节数组； 长度小于等于 4294967295 （2^{32}-1）字节的字节数组； 而整数值则可以是以下六种长度的其中一种： 4 位长，介于 0 至 12 之间的无符号整数； 1 字节长的有符号整数； 3 字节长的有符号整数； int16_t 类型整数； int32_t 类型整数； int64_t 类型整数。 每个压缩列表节点都由 previous_entry_length 、 encoding 、 content 三个部分组成， 如图 7-4 所示。接下来的内容将分别介绍这三个组成部分。 previous_entry_length节点的 previous_entry_length 属性以字节为单位， 记录了压缩列表中前一个节点的长度。 previous_entry_length 属性的长度可以是 1 字节或者 5 字节： 如果前一节点的长度小于 254 字节， 那么 previous_entry_length 属性的长度为 1 字节： 前一节点的长度就保存在这一个字节里面。 如果前一节点的长度大于等于 254 字节， 那么 previous_entry_length 属性的长度为 5 字节： 其中属性的第一字节会被设置为 0xFE （十进制值 254）， 而之后的四个字节则用于保存前一节点的长度。 图 7-5 展示了一个包含一字节长 previous_entry_length 属性的压缩列表节点， 属性的值为 0x05 ， 表示前一节点的长度为 5 字节。图 7-6 展示了一个包含五字节长 previous_entry_length 属性的压缩节点， 属性的值为 0xFE00002766 ， 其中值的最高位字节 0xFE 表示这是一个五字节长的 previous_entry_length 属性， 而之后的四字节 0x00002766 （十进制值 10086 ）才是前一节点的实际长度。 因为节点的 previous_entry_length 属性记录了前一个节点的长度， 所以程序可以通过指针运算， 根据当前节点的起始地址来计算出前一个节点的起始地址。 举个例子， 如果我们有一个指向当前节点起始地址的指针 c ， 那么我们只要用指针 c 减去当前节点 previous_entry_length 属性的值， 就可以得出一个指向前一个节点起始地址的指针 p ， 如图 7-7 所示。 压缩列表的从表尾向表头遍历操作就是使用这一原理实现的： 只要我们拥有了一个指向某个节点起始地址的指针， 那么通过这个指针以及这个节点的 previous_entry_length 属性， 程序就可以一直向前一个节点回溯， 最终到达压缩列表的表头节点。 图 7-8 展示了一个从表尾节点向表头节点进行遍历的完整过程： 首先，我们拥有指向压缩列表表尾节点 entry4 起始地址的指针 p1 （指向表尾节点的指针可以通过指向压缩列表起始地址的指针加上 zltail 属性的值得出）； 通过用 p1 减去 entry4 节点 previous_entry_length 属性的值， 我们得到一个指向 entry4 前一节点 entry3 起始地址的指针 p2 ； 通过用 p2 减去 entry3 节点 previous_entry_length 属性的值， 我们得到一个指向 entry3 前一节点 entry2 起始地址的指针 p3 ； 通过用 p3 减去 entry2 节点 previous_entry_length 属性的值， 我们得到一个指向 entry2 前一节点 entry1 起始地址的指针 p4 ， entry1 为压缩列表的表头节点； 最终， 我们从表尾节点向表头节点遍历了整个列表。 encoding节点的 encoding 属性记录了节点的 content 属性所保存数据的类型以及长度： 一字节、两字节或者五字节长， 值的最高位为 00 、 01 或者 10 的是字节数组编码： 这种编码表示节点的 content 属性保存着字节数组， 数组的长度由编码除去最高两位之后的其他位记录； 一字节长， 值的最高位以 11 开头的是整数编码： 这种编码表示节点的 content 属性保存着整数值， 整数值的类型和长度由编码除去最高两位之后的其他位记录； 表 7-2 记录了所有可用的字节数组编码， 而表 7-3 则记录了所有可用的整数编码。 表格中的下划线 _ 表示留空， 而 b 、 x 等变量则代表实际的二进制数据， 为了方便阅读， 多个字节之间用空格隔开。 表 7-2 字节数组编码 编码 编码长度 content 属性保存的值 00bbbbbb 1 字节 长度小于等于 63 字节的字节数组。 01bbbbbb xxxxxxxx 2 字节 长度小于等于 16383 字节的字节数组。 10__ aaaaaaaa bbbbbbbb cccccccc dddddddd 5 字节 长度小于等于 4294967295 的字节数组。 表 7-3 整数编码 编码 编码长度 content 属性保存的值 11000000 1 字节 int16_t 类型的整数。 11010000 1 字节 int32_t 类型的整数。 11100000 1 字节 int64_t 类型的整数。 11110000 1 字节 24 位有符号整数。 11111110 1 字节 8 位有符号整数。 1111xxxx 1 字节 使用这一编码的节点没有相应的 content 属性， 因为编码本身的 xxxx 四个位已经保存了一个介于 0 和 12 之间的值， 所以它无须 content 属性。 content节点的 content 属性负责保存节点的值， 节点值可以是一个字节数组或者整数， 值的类型和长度由节点的 encoding 属性决定。 图 7-9 展示了一个保存字节数组的节点示例： 编码的最高两位 00 表示节点保存的是一个字节数组； 编码的后六位 001011 记录了字节数组的长度 11 ； content 属性保存着节点的值 “hello world” 。 图 7-10 展示了一个保存整数值的节点示例： 编码 11000000 表示节点保存的是一个 int16_t 类型的整数值； content 属性保存着节点的值 10086 。 连锁更新前面说过， 每个节点的 previous_entry_length 属性都记录了前一个节点的长度： 如果前一节点的长度小于 254 字节， 那么 previous_entry_length 属性需要用 1 字节长的空间来保存这个长度值。 如果前一节点的长度大于等于 254 字节， 那么 previous_entry_length 属性需要用 5 字节长的空间来保存这个长度值。 现在， 考虑这样一种情况： 在一个压缩列表中， 有多个连续的、长度介于 250 字节到 253 字节之间的节点 e1 至 eN ， 如图 7-11 所示。因为 e1 至 eN 的所有节点的长度都小于 254 字节， 所以记录这些节点的长度只需要 1 字节长的 previous_entry_length 属性， 换句话说， e1 至 eN 的所有节点的 previous_entry_length 属性都是 1 字节长的。 这时， 如果我们将一个长度大于等于 254 字节的新节点 new 设置为压缩列表的表头节点， 那么 new 将成为 e1 的前置节点， 如图 7-12 所示。 因为 e1 的 previous_entry_length 属性仅长 1 字节， 它没办法保存新节点 new 的长度， 所以程序将对压缩列表执行空间重分配操作， 并将 e1 节点的 previous_entry_length 属性从原来的 1 字节长扩展为 5 字节长。 现在， 麻烦的事情来了 —— e1 原本的长度介于 250 字节至 253 字节之间， 在为 previous_entry_length 属性新增四个字节的空间之后， e1 的长度就变成了介于 254 字节至 257 字节之间， 而这种长度使用 1 字节长的 previous_entry_length 属性是没办法保存的。 因此， 为了让 e2 的 previous_entry_length 属性可以记录下 e1 的长度， 程序需要再次对压缩列表执行空间重分配操作， 并将 e2 节点的 previous_entry_length 属性从原来的 1 字节长扩展为 5 字节长。 正如扩展 e1 引发了对 e2 的扩展一样， 扩展 e2 也会引发对 e3 的扩展， 而扩展 e3 又会引发对 e4 的扩展……为了让每个节点的 previous_entry_length 属性都符合压缩列表对节点的要求， 程序需要不断地对压缩列表执行空间重分配操作， 直到 eN 为止。 Redis 将这种在特殊情况下产生的连续多次空间扩展操作称之为“连锁更新”（cascade update）， 图 7-13 展示了这一过程。 除了添加新节点可能会引发连锁更新之外， 删除节点也可能会引发连锁更新。 考虑图 7-14 所示的压缩列表， 如果 e1 至 eN 都是大小介于 250 字节至 253 字节的节点， big 节点的长度大于等于 254 字节（需要 5 字节的 previous_entry_length 来保存）， 而 small 节点的长度小于 254 字节（只需要 1 字节的 previous_entry_length 来保存）， 那么当我们将 small 节点从压缩列表中删除之后， 为了让 e1 的 previous_entry_length 属性可以记录 big 节点的长度， 程序将扩展 e1 的空间， 并由此引发之后的连锁更新。因为连锁更新在最坏情况下需要对压缩列表执行 N 次空间重分配操作， 而每次空间重分配的最坏复杂度为 O(N) ， 所以连锁更新的最坏复杂度为 O(N^2) 。 要注意的是， 尽管连锁更新的复杂度较高， 但它真正造成性能问题的几率是很低的： 首先， 压缩列表里要恰好有多个连续的、长度介于 250 字节至 253 字节之间的节点， 连锁更新才有可能被引发， 在实际中， 这种情况并不多见； 其次， 即使出现连锁更新， 但只要被更新的节点数量不多， 就不会对性能造成任何影响： 比如说， 对三五个节点进行连锁更新是绝对不会影响性能的； 因为以上原因， ziplistPush 等命令的平均复杂度仅为 O(N) ， 在实际中， 我们可以放心地使用这些函数， 而不必担心连锁更新会影响压缩列表的性能。 重点回顾 压缩列表是一种为节约内存而开发的顺序型数据结构。 压缩列表被用作列表键和哈希键的底层实现之一。 压缩列表可以包含多个节点，每个节点可以保存一个字节数组或者整数值。 添加新节点到压缩列表， 或者从压缩列表中删除节点， 可能会引发连锁更新操作， 但这种操作出现的几率并不高。 参考 http://redisbook.com]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>redis</tag>
        <tag>压缩列表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis数据结构-整数集合]]></title>
    <url>%2F2017%2F09%2F17%2Fredis-intset%2F</url>
    <content type="text"><![CDATA[整数集合（intset）是集合键的底层实现之一： 当一个集合只包含整数值元素， 并且这个集合的元素数量不多时， Redis 就会使用整数集合作为集合键的底层实现。 整数集合的实现整数集合（intset）是 Redis 用于保存整数值的集合抽象数据结构， 它可以保存类型为 int16_t 、 int32_t 或者 int64_t 的整数值， 并且保证集合中不会出现重复元素。 每个 intset.h/intset 结构表示一个整数集合：123456789101112typedef struct intset &#123; // 编码方式 uint32_t encoding; // 集合包含的元素数量 uint32_t length; // 保存元素的数组 int8_t contents[];&#125; intset; contents 数组是整数集合的底层实现： 整数集合的每个元素都是 contents 数组的一个数组项（item）， 各个项在数组中按值的大小从小到大有序地排列， 并且数组中不包含任何重复项。 length 属性记录了整数集合包含的元素数量， 也即是 contents 数组的长度。 虽然 intset 结构将 contents 属性声明为 int8_t 类型的数组， 但实际上 contents 数组并不保存任何 int8_t 类型的值 —— contents 数组的真正类型取决于 encoding 属性的值： 如果 encoding 属性的值为 INTSET_ENC_INT16 ， 那么 contents 就是一个 int16_t 类型的数组， 数组里的每个项都是一个 int16_t 类型的整数值 （最小值为 -32,768 ，最大值为 32,767 ）。 如果 encoding 属性的值为 INTSET_ENC_INT32 ， 那么 contents 就是一个 int32_t 类型的数组， 数组里的每个项都是一个 int32_t 类型的整数值 （最小值为 -2,147,483,648 ，最大值为 2,147,483,647 ）。 如果 encoding 属性的值为 INTSET_ENC_INT64 ， 那么 contents 就是一个 int64_t 类型的数组， 数组里的每个项都是一个 int64_t 类型的整数值 （最小值为 -9,223,372,036,854,775,808 ，最大值为 9,223,372,036,854,775,807 ）。 图 6-1 展示了一个整数集合示例：encoding 属性的值为 INTSET_ENC_INT16 ， 表示整数集合的底层实现为 int16_t 类型的数组， 而集合保存的都是 int16_t 类型的整数值。length 属性的值为 5 ， 表示整数集合包含五个元素。contents 数组按从小到大的顺序保存着集合中的五个元素。因为每个集合元素都是 int16_t 类型的整数值， 所以 contents 数组的大小等于 sizeof(int16_t) 5 = 16 5 = 80 位。图 6-2 展示了另一个整数集合示例：encoding 属性的值为 INTSET_ENC_INT64 ， 表示整数集合的底层实现为 int64_t 类型的数组， 而数组中保存的都是 int64_t 类型的整数值。length 属性的值为 4 ， 表示整数集合包含四个元素。contents 数组按从小到大的顺序保存着集合中的四个元素。因为每个集合元素都是 int64_t 类型的整数值， 所以 contents 数组的大小为 sizeof(int64_t) 4 = 64 4 = 256 位。虽然 contents 数组保存的四个整数值中， 只有 -2675256175807981027 是真正需要用 int64_t 类型来保存的， 而其他的 1 、 3 、 5 三个值都可以用 int16_t 类型来保存， 不过根据整数集合的升级规则， 当向一个底层为 int16_t 数组的整数集合添加一个 int64_t 类型的整数值时， 整数集合已有的所有元素都会被转换成 int64_t 类型， 所以 contents 数组保存的四个整数值都是 int64_t 类型的， 不仅仅是 -2675256175807981027 。 升级每当我们要将一个新元素添加到整数集合里面， 并且新元素的类型比整数集合现有所有元素的类型都要长时， 整数集合需要先进行升级（upgrade）， 然后才能将新元素添加到整数集合里面。 升级整数集合并添加新元素共分为三步进行： 根据新元素的类型， 扩展整数集合底层数组的空间大小， 并为新元素分配空间。 将底层数组现有的所有元素都转换成与新元素相同的类型， 并将类型转换后的元素放置到正确的位上， 而且在放置元素的过程中， 需要继续维持底层数组的有序性质不变。 将新元素添加到底层数组里面。 举个例子， 假设现在有一个 INTSET_ENC_INT16 编码的整数集合， 集合中包含三个 int16_t 类型的元素， 如图 6-3 所示。因为每个元素都占用 16 位空间， 所以整数集合底层数组的大小为 3 * 16 = 48 位， 图 6-4 展示了整数集合的三个元素在这 48 位里的位置。现在， 假设我们要将类型为 int32_t 的整数值 65535 添加到整数集合里面， 因为 65535 的类型 int32_t 比整数集合当前所有元素的类型都要长， 所以在将 65535 添加到整数集合之前， 程序需要先对整数集合进行升级。 升级首先要做的是， 根据新类型的长度， 以及集合元素的数量（包括要添加的新元素在内）， 对底层数组进行空间重分配。 整数集合目前有三个元素， 再加上新元素 65535 ， 整数集合需要分配四个元素的空间， 因为每个 int32_t 整数值需要占用 32 位空间， 所以在空间重分配之后， 底层数组的大小将是 32 * 4 = 128 位， 如图 6-5 所示。虽然程序对底层数组进行了空间重分配， 但数组原有的三个元素 1 、 2 、 3 仍然是 int16_t 类型， 这些元素还保存在数组的前 48 位里面， 所以程序接下来要做的就是将这三个元素转换成 int32_t 类型， 并将转换后的元素放置到正确的位上面， 而且在放置元素的过程中， 需要维持底层数组的有序性质不变。 首先， 因为元素 3 在 1 、 2 、 3 、 65535 四个元素中排名第三， 所以它将被移动到 contents 数组的索引 2 位置上， 也即是数组 64 位至 95 位的空间内， 如图 6-6 所示。接着， 因为元素 2 在 1 、 2 、 3 、 65535 四个元素中排名第二， 所以它将被移动到 contents 数组的索引 1 位置上， 也即是数组的 32 位至 63 位的空间内， 如图 6-7 所示。之后， 因为元素 1 在 1 、 2 、 3 、 65535 四个元素中排名第一， 所以它将被移动到 contents 数组的索引 0 位置上， 也即是数组的 0 位至 31 位的空间内， 如图 6-8 所示。然后， 因为元素 65535 在 1 、 2 、 3 、 65535 四个元素中排名第四， 所以它将被添加到 contents 数组的索引 3 位置上， 也即是数组的 96 位至 127 位的空间内， 如图 6-9 所示。最后， 程序将整数集合 encoding 属性的值从 INTSET_ENC_INT16 改为 INTSET_ENC_INT32 ， 并将 length 属性的值从 3 改为 4 ， 设置完成之后的整数集合如图 6-10 所示。因为每次向整数集合添加新元素都可能会引起升级， 而每次升级都需要对底层数组中已有的所有元素进行类型转换， 所以向整数集合添加新元素的时间复杂度为 O(N) 。 其他类型的升级操作， 比如从 INTSET_ENC_INT16 编码升级为 INTSET_ENC_INT64 编码， 或者从 INTSET_ENC_INT32 编码升级为 INTSET_ENC_INT64 编码， 升级的过程都和上面展示的升级过程类似。 升级之后新元素的摆放位置因为引发升级的新元素的长度总是比整数集合现有所有元素的长度都大， 所以这个新元素的值要么就大于所有现有元素， 要么就小于所有现有元素： 在新元素小于所有现有元素的情况下， 新元素会被放置在底层数组的最开头（索引 0 ）； 在新元素大于所有现有元素的情况下， 新元素会被放置在底层数组的最末尾（索引 length-1 ）。 升级的好处整数集合的升级策略有两个好处， 一个是提升整数集合的灵活性， 另一个是尽可能地节约内存。 提升灵活性因为 C 语言是静态类型语言， 为了避免类型错误， 我们通常不会将两种不同类型的值放在同一个数据结构里面。 比如说， 我们一般只使用 int16_t 类型的数组来保存 int16_t 类型的值， 只使用 int32_t 类型的数组来保存 int32_t 类型的值， 诸如此类。 但是， 因为整数集合可以通过自动升级底层数组来适应新元素， 所以我们可以随意地将 int16_t 、 int32_t 或者 int64_t 类型的整数添加到集合中， 而不必担心出现类型错误， 这种做法非常灵活。 节约内存当然， 要让一个数组可以同时保存 int16_t 、 int32_t 、 int64_t 三种类型的值， 最简单的做法就是直接使用 int64_t 类型的数组作为整数集合的底层实现。 不过这样一来， 即使添加到整数集合里面的都是 int16_t 类型或者 int32_t 类型的值， 数组都需要使用 int64_t 类型的空间去保存它们， 从而出现浪费内存的情况。 而整数集合现在的做法既可以让集合能同时保存三种不同类型的值， 又可以确保升级操作只会在有需要的时候进行， 这可以尽量节省内存。 比如说， 如果我们一直只向整数集合添加 int16_t 类型的值， 那么整数集合的底层实现就会一直是 int16_t 类型的数组， 只有在我们要将 int32_t 类型或者 int64_t 类型的值添加到集合时， 程序才会对数组进行升级。 降级整数集合不支持降级操作， 一旦对数组进行了升级， 编码就会一直保持升级后的状态。 举个例子， 对于图 6-11 所示的整数集合来说， 即使我们将集合里唯一一个真正需要使用 int64_t 类型来保存的元素 4294967295 删除了， 整数集合的编码仍然会维持 INTSET_ENC_INT64 ， 底层数组也仍然会是 int64_t 类型的， 如图 6-12 所示。 重点回顾 整数集合是集合键的底层实现之一。 整数集合的底层实现为数组， 这个数组以有序、无重复的方式保存集合元素， 在有需要时， 程序会根据新添加元素的类型， 改变这个数组的类型。 升级操作为整数集合带来了操作上的灵活性， 并且尽可能地节约了内存。 整数集合只支持升级操作， 不支持降级操作。 参考 http://redisbook.com]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>redis</tag>
        <tag>整数集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis数据结构-跳跃表]]></title>
    <url>%2F2017%2F09%2F16%2Fredis-skiplist%2F</url>
    <content type="text"><![CDATA[跳跃表（skiplist）是一种有序数据结构， 它通过在每个节点中维持多个指向其他节点的指针， 从而达到快速访问节点的目的。 跳跃表支持平均 O(\log N) 最坏 O(N) 复杂度的节点查找， 还可以通过顺序性操作来批量处理节点。 在大部分情况下， 跳跃表的效率可以和平衡树相媲美， 并且因为跳跃表的实现比平衡树要来得更为简单， 所以有不少程序都使用跳跃表来代替平衡树。Redis 使用跳跃表作为有序集合键的底层实现之一： 如果一个有序集合包含的元素数量比较多， 又或者有序集合中元素的成员（member）是比较长的字符串时， Redis 就会使用跳跃表来作为有序集合键的底层实现。 概括Redis 的跳跃表由 redis.h/zskiplistNode 和 redis.h/zskiplist 两个结构定义， 其中 zskiplistNode 结构用于表示跳跃表节点， 而 zskiplist 结构则用于保存跳跃表节点的相关信息， 比如节点的数量， 以及指向表头节点和表尾节点的指针， 等等。图 5-1 展示了一个跳跃表示例， 位于图片最左边的是 zskiplist 结构， 该结构包含以下属性： header ：指向跳跃表的表头节点。 tail ：指向跳跃表的表尾节点。 level ：记录目前跳跃表内，层数最大的那个节点的层数（表头节点的层数不计算在内）。 length ：记录跳跃表的长度，也即是，跳跃表目前包含节点的数量（表头节点不计算在内）。 位于 zskiplist 结构右方的是四个 zskiplistNode 结构， 该结构包含以下属性： 层（level）：节点中用 L1 、 L2 、 L3 等字样标记节点的各个层， L1 代表第一层， L2 代表第二层，以此类推。每个层都带有两个属性：前进指针和跨度。前进指针用于访问位于表尾方向的其他节点，而跨度则记录了前进指针所指向节点和当前节点的距离。在上面的图片中，连线上带有数字的箭头就代表前进指针，而那个数字就是跨度。当程序从表头向表尾进行遍历时，访问会沿着层的前进指针进行。 后退（backward）指针：节点中用 BW 字样标记节点的后退指针，它指向位于当前节点的前一个节点。后退指针在程序从表尾向表头遍历时使用。 分值（score）：各个节点中的 1.0 、 2.0 和 3.0 是节点所保存的分值。在跳跃表中，节点按各自所保存的分值从小到大排列。 成员对象（obj）：各个节点中的 o1 、 o2 和 o3 是节点所保存的成员对象。 注意表头节点和其他节点的构造是一样的： 表头节点也有后退指针、分值和成员对象， 不过表头节点的这些属性都不会被用到， 所以图中省略了这些部分， 只显示了表头节点的各个层。 本节接下来的内容将对 zskiplistNode 和 zskiplist 两个结构进行更详细的介绍。 跳跃表节点跳跃表节点的实现由 redis.h/zskiplistNode 结构定义：1234567891011121314151617181920212223typedef struct zskiplistNode &#123; // 后退指针 struct zskiplistNode *backward; // 分值 double score; // 成员对象 robj *obj; // 层 struct zskiplistLevel &#123; // 前进指针 struct zskiplistNode *forward; // 跨度 unsigned int span; &#125; level[];&#125; zskiplistNode; 层跳跃表节点的 level 数组可以包含多个元素， 每个元素都包含一个指向其他节点的指针， 程序可以通过这些层来加快访问其他节点的速度， 一般来说， 层的数量越多， 访问其他节点的速度就越快。 每次创建一个新跳跃表节点的时候， 程序都根据幂次定律 （power law，越大的数出现的概率越小） 随机生成一个介于 1 和 32 之间的值作为 level 数组的大小， 这个大小就是层的“高度”。 图 5-2 分别展示了三个高度为 1 层、 3 层和 5 层的节点， 因为 C 语言的数组索引总是从 0 开始的， 所以节点的第一层是 level[0] ， 而第二层是 level[1] ， 以此类推。 前进指针每个层都有一个指向表尾方向的前进指针（level[i].forward 属性）， 用于从表头向表尾方向访问节点。 图 5-3 用虚线表示出了程序从表头向表尾方向， 遍历跳跃表中所有节点的路径： 迭代程序首先访问跳跃表的第一个节点（表头）， 然后从第四层的前进指针移动到表中的第二个节点。 在第二个节点时， 程序沿着第二层的前进指针移动到表中的第三个节点。 在第三个节点时， 程序同样沿着第二层的前进指针移动到表中的第四个节点。 当程序再次沿着第四个节点的前进指针移动时， 它碰到一个 NULL ， 程序知道这时已经到达了跳跃表的表尾， 于是结束这次遍历。 跨度层的跨度（level[i].span 属性）用于记录两个节点之间的距离： 两个节点之间的跨度越大， 它们相距得就越远。 指向 NULL 的所有前进指针的跨度都为 0 ， 因为它们没有连向任何节点。 初看上去， 很容易以为跨度和遍历操作有关， 但实际上并不是这样 —— 遍历操作只使用前进指针就可以完成了， 跨度实际上是用来计算排位（rank）的： 在查找某个节点的过程中， 将沿途访问过的所有层的跨度累计起来， 得到的结果就是目标节点在跳跃表中的排位。 举个例子， 图 5-4 用虚线标记了在跳跃表中查找分值为 3.0 、 成员对象为 o3 的节点时， 沿途经历的层： 查找的过程只经过了一个层， 并且层的跨度为 3 ， 所以目标节点在跳跃表中的排位为 3 。再举个例子， 图 5-5 用虚线标记了在跳跃表中查找分值为 2.0 、 成员对象为 o2 的节点时， 沿途经历的层： 在查找节点的过程中， 程序经过了两个跨度为 1 的节点， 因此可以计算出， 目标节点在跳跃表中的排位为 2 。 后退指针节点的后退指针（backward 属性）用于从表尾向表头方向访问节点： 跟可以一次跳过多个节点的前进指针不同， 因为每个节点只有一个后退指针， 所以每次只能后退至前一个节点。 图 5-6 用虚线展示了如果从表尾向表头遍历跳跃表中的所有节点： 程序首先通过跳跃表的 tail 指针访问表尾节点， 然后通过后退指针访问倒数第二个节点， 之后再沿着后退指针访问倒数第三个节点， 再之后遇到指向 NULL 的后退指针， 于是访问结束。 分值和成员节点的分值（score 属性）是一个 double 类型的浮点数， 跳跃表中的所有节点都按分值从小到大来排序。 节点的成员对象（obj 属性）是一个指针， 它指向一个字符串对象， 而字符串对象则保存着一个 SDS 值。 在同一个跳跃表中， 各个节点保存的成员对象必须是唯一的， 但是多个节点保存的分值却可以是相同的： 分值相同的节点将按照成员对象在字典序中的大小来进行排序， 成员对象较小的节点会排在前面（靠近表头的方向）， 而成员对象较大的节点则会排在后面（靠近表尾的方向）。 举个例子， 在图 5-7 所示的跳跃表中， 三个跳跃表节点都保存了相同的分值 10086.0 ， 但保存成员对象 o1 的节点却排在保存成员对象 o2 和 o3 的节点之前， 而保存成员对象 o2 的节点又排在保存成员对象 o3 的节点之前， 由此可见， o1 、 o2 、 o3 三个成员对象在字典中的排序为 o1 &lt;= o2 &lt;= o3 。 跳跃表虽然仅靠多个跳跃表节点就可以组成一个跳跃表， 如图 5-8 所示。但通过使用一个 zskiplist 结构来持有这些节点， 程序可以更方便地对整个跳跃表进行处理， 比如快速访问跳跃表的表头节点和表尾节点， 又或者快速地获取跳跃表节点的数量（也即是跳跃表的长度）等信息， 如图 5-9 所示。zskiplist 结构的定义如下：123456789101112typedef struct zskiplist &#123; // 表头节点和表尾节点 struct zskiplistNode *header, *tail; // 表中节点的数量 unsigned long length; // 表中层数最大的节点的层数 int level;&#125; zskiplist; header 和 tail 指针分别指向跳跃表的表头和表尾节点， 通过这两个指针， 程序定位表头节点和表尾节点的复杂度为 O(1) 。 通过使用 length 属性来记录节点的数量， 程序可以在 O(1) 复杂度内返回跳跃表的长度。 level 属性则用于在 O(1) 复杂度内获取跳跃表中层高最大的那个节点的层数量， 注意表头节点的层高并不计算在内。 重点回顾 跳跃表是有序集合的底层实现之一， 除此之外它在 Redis 中没有其他应用。 Redis 的跳跃表实现由 zskiplist 和 zskiplistNode 两个结构组成， 其中 zskiplist 用于保存跳跃表信息（比如表头节点、表尾节点、长度）， 而 zskiplistNode 则用于表示跳跃表节点。 每个跳跃表节点的层高都是 1 至 32 之间的随机数。 在同一个跳跃表中， 多个节点可以包含相同的分值， 但每个节点的成员对象必须是唯一的。 跳跃表中的节点按照分值大小进行排序， 当分值相同时， 节点按照成员对象的大小进行排序。 参考 http://redisbook.com]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>redis</tag>
        <tag>跳跃表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis数据结构-字典]]></title>
    <url>%2F2017%2F09%2F15%2Fredis-dict%2F</url>
    <content type="text"><![CDATA[Redis 的数据库就是使用字典来作为底层实现的, 字典还是哈希键的底层实现之一Redis 的字典使用哈希表作为底层实现， 一个哈希表里面可以有多个哈希表节点， 而每个哈希表节点就保存了字典中的一个键值对。 哈希表Redis 字典所使用的哈希表由 dict.h/dictht 结构定义：12345678910111213141516typedef struct dictht &#123; // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值 // 总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used;&#125; dictht; table 属性是一个数组， 数组中的每个元素都是一个指向 dict.h/dictEntry 结构的指针， 每个 dictEntry 结构保存着一个键值对。size 属性记录了哈希表的大小， 也即是 table 数组的大小， 而 used 属性则记录了哈希表目前已有节点（键值对）的数量。sizemask 属性的值总是等于 size - 1 ， 这个属性和哈希值一起决定一个键应该被放到 table 数组的哪个索引上面。图 4-1 展示了一个大小为 4 的空哈希表 （没有包含任何键值对）。 哈希表节点哈希表节点使用 dictEntry 结构表示， 每个 dictEntry 结构都保存着一个键值对：12345678910111213141516typedef struct dictEntry &#123; // 键 void *key; // 值 union &#123; void *val; uint64_t u64; int64_t s64; &#125; v; // 指向下个哈希表节点，形成链表 struct dictEntry *next;&#125; dictEntry; key 属性保存着键值对中的键， 而 v 属性则保存着键值对中的值， 其中键值对的值可以是一个指针， 或者是一个 uint64_t 整数， 又或者是一个 int64_t 整数。next 属性是指向另一个哈希表节点的指针， 这个指针可以将多个哈希值相同的键值对连接在一次， 以此来解决键冲突（collision）的问题。举个例子， 图 4-2 就展示了如何通过 next 指针， 将两个索引值相同的键 k1 和 k0 连接在一起。 字典12345678910111213141516typedef struct dict &#123; // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表 dictht ht[2]; // rehash 索引 // 当 rehash 不在进行时，值为 -1 int rehashidx; /* rehashing not in progress if rehashidx == -1 */&#125; dict; type 属性和 privdata 属性是针对不同类型的键值对， 为创建多态字典而设置的： type 属性是一个指向 dictType 结构的指针， 每个 dictType 结构保存了一簇用于操作特定类型键值对的函数， Redis 会为用途不同的字典设置不同的类型特定函数。 而 privdata 属性则保存了需要传给那些类型特定函数的可选参数。 123456789101112131415161718192021typedef struct dictType &#123; // 计算哈希值的函数 unsigned int (*hashFunction)(const void *key); // 复制键的函数 void *(*keyDup)(void *privdata, const void *key); // 复制值的函数 void *(*valDup)(void *privdata, const void *obj); // 对比键的函数 int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 销毁键的函数 void (*keyDestructor)(void *privdata, void *key); // 销毁值的函数 void (*valDestructor)(void *privdata, void *obj);&#125; dictType; ht 属性是一个包含两个项的数组， 数组中的每个项都是一个 dictht 哈希表， 一般情况下， 字典只使用 ht[0] 哈希表， ht[1] 哈希表只会在对 ht[0] 哈希表进行 rehash 时使用。 除了 ht[1] 之外， 另一个和 rehash 有关的属性就是 rehashidx ： 它记录了 rehash 目前的进度， 如果目前没有在进行 rehash ， 那么它的值为 -1 。 图 4-3 展示了一个普通状态下（没有进行 rehash）的字典： 哈希算法当要将一个新的键值对添加到字典里面时， 程序需要先根据键值对的键计算出哈希值和索引值， 然后再根据索引值， 将包含新键值对的哈希表节点放到哈希表数组的指定索引上面。 Redis 计算哈希值和索引值的方法如下：123456//使用字典设置的哈希函数，计算键 key 的哈希值hash = dict-&gt;type-&gt;hashFunction(key);//使用哈希表的 sizemask 属性和哈希值，计算出索引值//根据情况不同， ht[x] 可以是 ht[0] 或者 ht[1]index = hash &amp; dict-&gt;ht[x].sizemask; 举个例子， 对于图 4-4 所示的字典来说， 如果我们要将一个键值对 k0 和 v0 添加到字典里面， 那么程序会先使用语句：1hash = dict-&gt;type-&gt;hashFunction(k0); 计算键 k0 的哈希值。假设计算得出的哈希值为 8 ， 那么程序会继续使用语句：1index = hash &amp; dict-&gt;ht[0].sizemask = 8 &amp; 3 = 0; 计算出键 k0 的索引值 0 ， 这表示包含键值对 k0 和 v0 的节点应该被放置到哈希表数组的索引 0 位置上， 如图 4-5 所示。当字典被用作数据库的底层实现， 或者哈希键的底层实现时， Redis 使用 MurmurHash2 算法来计算键的哈希值。 MurmurHash 算法最初由 Austin Appleby 于 2008 年发明， 这种算法的优点在于， 即使输入的键是有规律的， 算法仍能给出一个很好的随机分布性，并且算法的计算速度也非常快。 MurmurHash 算法目前的最新版本为 MurmurHash3 ， 而 Redis 使用的是 MurmurHash2 ， 关于 MurmurHash 算法的更多信息可以参考该算法的主页： http://code.google.com/p/smhasher/ 。 解决键冲突当有两个或以上数量的键被分配到了哈希表数组的同一个索引上面时， 我们称这些键发生了冲突（collision）。 Redis 的哈希表使用链地址法（separate chaining）来解决键冲突： 每个哈希表节点都有一个 next 指针， 多个哈希表节点可以用 next 指针构成一个单向链表， 被分配到同一个索引上的多个节点可以用这个单向链表连接起来， 这就解决了键冲突的问题。 举个例子， 假设程序要将键值对 k2 和 v2 添加到图 4-6 所示的哈希表里面， 并且计算得出 k2 的索引值为 2 ， 那么键 k1 和 k2 将产生冲突， 而解决冲突的办法就是使用 next 指针将键 k2 和 k1 所在的节点连接起来， 如图 4-7 所示。因为 dictEntry 节点组成的链表没有指向链表表尾的指针， 所以为了速度考虑， 程序总是将新节点添加到链表的表头位置（复杂度为 O(1)）， 排在其他已有节点的前面。 rehash随着操作的不断执行， 哈希表保存的键值对会逐渐地增多或者减少， 为了让哈希表的负载因子（load factor）维持在一个合理的范围之内， 当哈希表保存的键值对数量太多或者太少时， 程序需要对哈希表的大小进行相应的扩展或者收缩。扩展和收缩哈希表的工作可以通过执行 rehash （重新散列）操作来完成， Redis 对字典的哈希表执行 rehash 的步骤如下： 为字典的 ht[1] 哈希表分配空间， 这个哈希表的空间大小取决于要执行的操作， 以及 ht[0] 当前包含的键值对数量 （也即是 ht[0].used 属性的值）： 如果执行的是扩展操作， 那么 ht[1] 的大小为第一个大于等于 ht[0].used * 2 的 2^n （2 的 n 次方幂）； 如果执行的是收缩操作， 那么 ht[1] 的大小为第一个大于等于 ht[0].used 的 2^n 。 将保存在 ht[0] 中的所有键值对 rehash 到 ht[1] 上面： rehash 指的是重新计算键的哈希值和索引值， 然后将键值对放置到 ht[1] 哈希表的指定位置上。 当 ht[0] 包含的所有键值对都迁移到了 ht[1] 之后 （ht[0] 变为空表）， 释放 ht[0] ， 将 ht[1] 设置为 ht[0] ， 并在 ht[1] 新创建一个空白哈希表， 为下一次 rehash 做准备。 举个例子， 假设程序要对图 4-8 所示字典的 ht[0] 进行扩展操作， 那么程序将执行以下步骤： ht[0].used 当前的值为 4 ， 4 * 2 = 8 ， 而 8 （2^3）恰好是第一个大于等于 4 的 2 的 n 次方， 所以程序会将 ht[1] 哈希表的大小设置为 8 。 图 4-9 展示了 ht[1] 在分配空间之后， 字典的样子。 将 ht[0] 包含的四个键值对都 rehash 到 ht[1] ， 如图 4-10 所示。 释放 ht[0] ，并将 ht[1] 设置为 ht[0] ，然后为 ht[1] 分配一个空白哈希表，如图 4-11 所示。 至此， 对哈希表的扩展操作执行完毕， 程序成功将哈希表的大小从原来的 4 改为了现在的 8 。 哈希表的扩展与收缩当以下条件中的任意一个被满足时， 程序会自动开始对哈希表执行扩展操作： 服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 1 ； 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 5 ； 其中哈希表的负载因子可以通过公式：12// 负载因子 = 哈希表已保存节点数量 / 哈希表大小load_factor = ht[0].used / ht[0].size 计算得出。 比如说， 对于一个大小为 4 ， 包含 4 个键值对的哈希表来说， 这个哈希表的负载因子为：1load_factor = 4 / 4 = 1 又比如说， 对于一个大小为 512 ， 包含 256 个键值对的哈希表来说， 这个哈希表的负载因子为：1load_factor = 256 / 512 = 0.5 根据 BGSAVE 命令或 BGREWRITEAOF 命令是否正在执行， 服务器执行扩展操作所需的负载因子并不相同， 这是因为在执行 BGSAVE 命令或 BGREWRITEAOF 命令的过程中， Redis 需要创建当前服务器进程的子进程， 而大多数操作系统都采用写时复制（copy-on-write）技术来优化子进程的使用效率， 所以在子进程存在期间， 服务器会提高执行扩展操作所需的负载因子， 从而尽可能地避免在子进程存在期间进行哈希表扩展操作， 这可以避免不必要的内存写入操作， 最大限度地节约内存。 另一方面， 当哈希表的负载因子小于 0.1 时， 程序自动开始对哈希表执行收缩操作。 渐进式 rehash上一节说过， 扩展或收缩哈希表需要将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面， 但是， 这个 rehash 动作并不是一次性、集中式地完成的， 而是分多次、渐进式地完成的。 这样做的原因在于， 如果 ht[0] 里只保存着四个键值对， 那么服务器可以在瞬间就将这些键值对全部 rehash 到 ht[1] ； 但是， 如果哈希表里保存的键值对数量不是四个， 而是四百万、四千万甚至四亿个键值对， 那么要一次性将这些键值对全部 rehash 到 ht[1] 的话， 庞大的计算量可能会导致服务器在一段时间内停止服务。 因此， 为了避免 rehash 对服务器性能造成影响， 服务器不是一次性将 ht[0] 里面的所有键值对全部 rehash 到 ht[1] ， 而是分多次、渐进式地将 ht[0] 里面的键值对慢慢地 rehash 到 ht[1] 。 以下是哈希表渐进式 rehash 的详细步骤： 为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表。 在字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始。 在 rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一。 随着字典操作的不断执行， 最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成。渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。 图 4-12 至图 4-17 展示了一次完整的渐进式 rehash 过程， 注意观察在整个 rehash 过程中， 字典的 rehashidx 属性是如何变化的。 渐进式 rehash 执行期间的哈希表操作因为在进行渐进式 rehash 的过程中， 字典会同时使用 ht[0] 和 ht[1] 两个哈希表， 所以在渐进式 rehash 进行期间， 字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行： 比如说， 要在字典里面查找一个键的话， 程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找， 诸如此类。 另外， 在渐进式 rehash 执行期间， 新添加到字典的键值对一律会被保存到 ht[1] 里面， 而 ht[0] 则不再进行任何添加操作： 这一措施保证了 ht[0] 包含的键值对数量会只减不增， 并随着 rehash 操作的执行而最终变成空表。 重点回顾 字典被广泛用于实现 Redis 的各种功能， 其中包括数据库和哈希键。 Redis 中的字典使用哈希表作为底层实现， 每个字典带有两个哈希表， 一个用于平时使用， 另一个仅在进行 rehash 时使用。 当字典被用作数据库的底层实现， 或者哈希键的底层实现时， Redis 使用 MurmurHash2 算法来计算键的哈希值。 哈希表使用链地址法来解决键冲突， 被分配到同一个索引上的多个键值对会连接成一个单向链表。 在对哈希表进行扩展或者收缩操作时， 程序需要将现有哈希表包含的所有键值对 rehash 到新哈希表里面， 并且这个 rehash 过程并不是一次性地完成的， 而是渐进式地完成的。 参考 http://redisbook.com]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>redis</tag>
        <tag>hashtable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis数据结构-链表]]></title>
    <url>%2F2017%2F09%2F14%2Fredis-linkedlist%2F</url>
    <content type="text"><![CDATA[列表键的底层就是一个链表 链表节点每个链表节点使用一个 adlist.h/listNode 结构来表示：123456789101112typedef struct listNode &#123; // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点的值 void *value;&#125; listNode; 多个 listNode 可以通过 prev 和 next 指针组成双端链表， 如图 3-1 所示。 链表虽然仅仅使用多个 listNode 结构就可以组成链表， 但使用 adlist.h/list 来持有链表的话， 操作起来会更方便：123456789101112131415161718192021typedef struct list &#123; // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 链表所包含的节点数量 unsigned long len; // 节点值复制函数 void *(*dup)(void *ptr); // 节点值释放函数 void (*free)(void *ptr); // 节点值对比函数 int (*match)(void *ptr, void *key);&#125; list; list 结构为链表提供了表头指针 head 、表尾指针 tail ， 以及链表长度计数器 len ， 而 dup 、 free 和 match 成员则是用于实现多态链表所需的类型特定函数： dup 函数用于复制链表节点所保存的值； free 函数用于释放链表节点所保存的值； match 函数则用于对比链表节点所保存的值和另一个输入值是否相等。 图 3-2 是由一个 list 结构和三个 listNode 结构组成的链表： Redis 的链表实现的特性可以总结如下： 双端： 链表节点带有 prev 和 next 指针， 获取某个节点的前置节点和后置节点的复杂度都是 O(1) 。 无环： 表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL ， 对链表的访问以 NULL 为终点。 带表头指针和表尾指针： 通过 list 结构的 head 指针和 tail 指针， 程序获取链表的表头节点和表尾节点的复杂度为 O(1) 。 带链表长度计数器： 程序使用 list 结构的 len 属性来对 list 持有的链表节点进行计数， 程序获取链表中节点数量的复杂度为 O(1) 。 多态： 链表节点使用 void* 指针来保存节点值， 并且可以通过 list 结构的 dup 、 free 、 match 三个属性为节点值设置类型特定函数， 所以链表可以用于保存各种不同类型的值。 重点回顾 链表被广泛用于实现 Redis 的各种功能， 比如列表键， 发布与订阅， 慢查询， 监视器， 等等。 每个链表节点由一个 listNode 结构来表示， 每个节点都有一个指向前置节点和后置节点的指针， 所以 Redis 的链表实现是双端链表。 每个链表使用一个 list 结构来表示， 这个结构带有表头节点指针、表尾节点指针、以及链表长度等信息。 因为链表表头节点的前置节点和表尾节点的后置节点都指向 NULL ， 所以 Redis 的链表实现是无环链表。 通过为链表设置不同的类型特定函数， Redis 的链表可以用于保存各种不同类型的值。 参考 http://redisbook.com]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>redis</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM杂谈之JIT]]></title>
    <url>%2F2017%2F09%2F14%2Fjava-jit%2F</url>
    <content type="text"><![CDATA[Just In TimeJust in time编译，也叫做运行时编译，不同于 C / C++ 语言直接被翻译成机器指令，javac把java的源文件翻译成了class文件，而class文件中全都是Java字节码。那么，JVM在加载了这些class文件以后，针对这些字节码，逐条取出，逐条执行，这种方法就是解释执行。 还有一种，就是把这些Java字节码重新编译优化，生成机器码，让CPU直接执行。这样编出来的代码效率会更高。通常，我们不必把所有的Java方法都编译成机器码，只需要把调用最频繁，占据CPU时间最长的方法找出来将其编译成机器码。这种调用最频繁的Java方法就是我们常说的热点方法（Hotspot，说不定这个虚拟机的名字就是从这里来的）。 这种在运行时按需编译的方式就是Just In Time。 主要技术点其实JIT的主要技术点，从大的框架上来说，非常简单，就是申请一块既有写权限又有执行权限的内存，然后把你要编译的Java方法，翻译成机器码，写入到这块内存里。当再需要调用原来的Java方法时，就转向调用这块内存。 我们看一个例子：12345678910#include&lt;stdio.h&gt;int inc(int a) &#123; return a + 1;&#125;int main() &#123; printf("%d\n", inc(3)); return 0;&#125; 上面这个例子很简单，就是把3加1，然后打印出来，我们通过以下命令，查看一下它的机器码：12# gcc -o inc inc.c# objdump -d inc 然后在这一堆输出中，可以找到 inc 方法最终被翻译成了这样的机器码：123456740052d: 55 push %rbp40052e: 48 89 e5 mov %rsp,%rbp400531: 89 7d fc mov %edi,-0x4(%rbp)400534: 8b 45 fc mov -0x4(%rbp),%eax400537: 83 c0 01 add $0x1,%eax40053a: 5d pop %rbp40053b: c3 retq 我来解释一下（读者需要一定的x86汇编语言的知识）。 第一句，保存上一个栈帧的基址，并把当前的栈指针赋给栈基址寄存器，这是进入一个函数的常规操作。我们不去管它。 第三句，把edi存到栈上。在x64处理器上，前6个参数都是使用寄存器传参的。第一个参数会使用rdi，第二个参数使用 rsi，等等。所以 edi 里存的其实就是第一个参数，也就是整数 3，为什么使用rdi的低32位，也就是 edi 呢？因为我们的入参 a 是 int 型啊。大家可以换成 long 型看看效果。 第四句，把上一步存到栈上的那个整数再存进 eax 中。 第五句往后，把 eax 加上 1， 然后就退栈，返回。按照x64的规定（ABI），返回值通过eax传递。 我们看到了，其实第三句，第四句好像根本没有存在的必要，gcc 默认情况下，生成的机器码有点傻，它总要把入参放到栈上，但其实，我们是可以直接把参数从 rdi 中放入到 rax 中的。不满意。那我们可以自己改一下，让它更精简一点。怎么做呢？答案就是运行时修改 inc 的逻辑。12345678910111213141516171819202122232425#include&lt;stdio.h&gt;#include&lt;memory.h&gt;#include&lt;sys/mman.h&gt;typedef int (* inc_func)(int a); int main() &#123; char code[] = &#123; 0x55, // push rbp 0x48, 0x89, 0xe5, // mov rsp, rbp 0x89, 0xf8, // mov edi, eax 0x83, 0xc0, 0x01, // add $1, eax 0x5d, // pop rbp 0xc3 // ret &#125;; void * temp = mmap(NULL, sizeof(code), PROT_WRITE | PROT_EXEC, MAP_ANONYMOUS | MAP_PRIVATE, -1, 0); memcpy(temp, code, sizeof(code)); inc_func p_inc = (inc_func)temp; printf("%d\n", p_inc(7)); return 0;&#125; 在这个例子中，我们使用了 mmap 来申请了一块有写权限和执行权限的内存，然后把我们手写的机器码拷进去，然后使用一个函数指针指向这块内存，并且调用它。通过这种方式我们就可以执行这一段手写的机器码了。 运行一下看看：123# gcc -o inc_a inc_a.c # ./inc_a8 再回想一下这个过程。我们通过手写机器码把原来的 inc 函数代替掉了。在新的例子中，我们是使用程序中定义的数据来重新造了一个 inc 函数。这种在运行的过程创建新的函数的方式，就是JIT的核心操作。 解释器，C1和C2在Hotspot中，解释器是为每一个字节码生成一小段机器码，在执行Java方法的过程中，每次取一条指令，然后就去执行这一个指令所对应的那一段机器码。256条指令，就组成了一个表，在这个表里，每一条指令都对应一段机器码，当执行到某一条指令时，就从这个表里去查这段机器码，并且通过 jmp 指令去执行这段机器码就行了。 这种方式被称为模板解释器。 模板解释器生成的代码有很多冗余，就像我们上面的第一个例子那样。为了生成更精简的机器码，我们可以引入编译器优化手段，例如全局值编码，死代码消除，标量展开，公共子表达式消除，常量传播等等。这样生成出来的机器码会更加优化。 但是，生成机器码的质量越高，所需要的时间也就越长。JIT线程也是要挤占Java 应用线程的资源的。所以C1是一个折衷，编译时间既不会太长，生成的机器码的指令也不是最优化的，但肯定比解释器的效率要高很多。 如果一个Java方法调用得足够频繁，那就更值得花大力气去为它生成更优质的机器码，这时就会触发C2编译，c2是一个运行得更慢，但却能生成更高效代码的编译器。 由此，我们看到，其实Java的运行，几乎全部都依赖运行时生成的机器码上。所以，对于文章开头的那个问题“Java是运行在C++上的吗？”，大家应该都有自己的答案了。这个问题无法简单地回答是或者不是，正确答案就是Java的运行依赖模板解释器和JIT编译器。 多说一点优化我们这节课所举的例子中，可以做更多的优化，例如，既然我进到inc函数以后，完全没有使用栈，那其实，我就不要再为它开辟栈帧了。所以可以把push rbp, pop rbp的逻辑都去掉。 进一步优化成这样：inc_b.c12345char code[] = &#123; 0x89, 0xf8, // mov edi, eax 0x83, 0xc0, 0x01, // add $1, eax 0xc3 // ret&#125;; 可以看到，指令更加精简了。我们重新编译运行，还是能成功打印出8。 根据这个问题：为什么 lea 会被用来计算？ 我们还可以写出更优化的代码来：inc_c.c1234char code[] = &#123; 0x8d, 0x47, 0x01, // lea 0x1(rdi), rax 0xc3 // ret&#125;; 如果开启 gcc 的优化编译，我们也可以得到这样的代码，例如，还是针对这个方法：123int inc(int a) &#123; return a + 1;&#125; 使用 -O2 优化：12# gcc -o inc inc.c -O2# objdump -d inc 就可以看到，inc 的机器码变成这样了：12300000000004005f0 &lt;inc&gt;: 4005f0: 8d 47 01 lea 0x1(%rdi),%eax 4005f3: c3 retq 这和我们手写的优化的机器码是完全一样的了。 实际上，C1和C2所要做的和gcc的优化编译是一样的，就是使用特定的方法生成更高效的机器码。但是从原理上来说，运行时生成机器码这个技术，大家都是相通的。 代码https://github.com/ejunjsh/c-code/tree/master/inc参考https://zhuanlan.zhihu.com/p/28476709]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
        <tag>jit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka学习（一）]]></title>
    <url>%2F2017%2F09%2F13%2Fkafka-design%2F</url>
    <content type="text"><![CDATA[to be continue]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis数据结构-简单动态字符串]]></title>
    <url>%2F2017%2F09%2F13%2Fredis-string%2F</url>
    <content type="text"><![CDATA[Redis 没有直接使用 C 语言传统的字符串表示（以空字符结尾的字符数组，以下简称 C 字符串）， 而是自己构建了一种名为简单动态字符串（simple dynamic string，SDS）的抽象类型， 并将 SDS 用作 Redis 的默认字符串表示。例如键（key）底层就是SDS，而值如果是字符串对象，那这个对象的底层也是个SDS 定义每个 sds.h/sdshdr 结构表示一个 SDS 值：12345678910111213struct sdshdr &#123; // 记录 buf 数组中已使用字节的数量 // 等于 SDS 所保存字符串的长度 int len; // 记录 buf 数组中未使用字节的数量 int free; // 字节数组，用于保存字符串 char buf[];&#125;; 图 2-1 展示了一个 SDS 示例： free 属性的值为 0 ， 表示这个 SDS 没有分配任何未使用空间。 len 属性的值为 5 ， 表示这个 SDS 保存了一个五字节长的字符串。 buf 属性是一个 char 类型的数组， 数组的前五个字节分别保存了 ‘R’ 、 ‘e’ 、 ‘d’ 、 ‘i’ 、 ‘s’ 五个字符， 而最后一个字节则保存了空字符 ‘\0’ 。SDS 遵循 C 字符串以空字符结尾的惯例， 保存空字符的 1 字节空间不计算在 SDS 的 len 属性里面， 并且为空字符分配额外的 1 字节空间， 以及添加空字符到字符串末尾等操作都是由 SDS 函数自动完成的， 所以这个空字符对于 SDS 的使用者来说是完全透明的。 遵循空字符结尾这一惯例的好处是， SDS 可以直接重用一部分 C 字符串函数库里面的函数。 举个例子， 如果我们有一个指向图 2-1 所示 SDS 的指针 s ， 那么我们可以直接使用 stdio.h/printf 函数， 通过执行以下语句：1printf("%s", s-&gt;buf); 来打印出 SDS 保存的字符串值 “Redis” ， 而无须为 SDS 编写专门的打印函数。 图 2-2 展示了另一个 SDS 示例: 这个 SDS 和之前展示的 SDS 一样， 都保存了字符串值 “Redis” 。 这个 SDS 和之前展示的 SDS 的区别在于， 这个 SDS 为 buf 数组分配了五字节未使用空间， 所以它的 free 属性的值为 5 （图中使用五个空格来表示五字节的未使用空间）。 SDS 与 C 字符串的区别根据传统， C 语言使用长度为 N+1 的字符数组来表示长度为 N 的字符串， 并且字符数组的最后一个元素总是空字符 ‘\0’ 。 比如说， 图 2-3 就展示了一个值为 “Redis” 的 C 字符串：C 语言使用的这种简单的字符串表示方式， 并不能满足 Redis 对字符串在安全性、效率、以及功能方面的要求， 本节接下来的内容将详细对比 C 字符串和 SDS 之间的区别， 并说明 SDS 比 C 字符串更适用于 Redis 的原因。 常数复杂度获取字符串长度因为 C 字符串并不记录自身的长度信息， 所以为了获取一个 C 字符串的长度， 程序必须遍历整个字符串， 对遇到的每个字符进行计数， 直到遇到代表字符串结尾的空字符为止， 这个操作的复杂度为 O(N) 。 举个例子， 图 2-4 展示了程序计算一个 C 字符串长度的过程。和 C 字符串不同， 因为 SDS 在 len 属性中记录了 SDS 本身的长度， 所以获取一个 SDS 长度的复杂度仅为 O(1) 。 举个例子， 对于图 2-5 所示的 SDS 来说， 程序只要访问 SDS 的 len 属性， 就可以立即知道 SDS 的长度为 5 字节：又比如说， 对于图 2-6 展示的 SDS 来说， 程序只要访问 SDS 的 len 属性， 就可以立即知道 SDS 的长度为 11 字节。设置和更新 SDS 长度的工作是由 SDS 的 API 在执行时自动完成的， 使用 SDS 无须进行任何手动修改长度的工作。 通过使用 SDS 而不是 C 字符串， Redis 将获取字符串长度所需的复杂度从 O(N) 降低到了 O(1) ， 这确保了获取字符串长度的工作不会成为 Redis 的性能瓶颈。 比如说， 因为字符串键在底层使用 SDS 来实现， 所以即使我们对一个非常长的字符串键反复执行 STRLEN 命令， 也不会对系统性能造成任何影响， 因为 STRLEN 命令的复杂度仅为 O(1) 。 杜绝缓冲区溢出除了获取字符串长度的复杂度高之外， C 字符串不记录自身长度带来的另一个问题是容易造成缓冲区溢出（buffer overflow）。 举个例子， &lt;string.h&gt;/strcat 函数可以将 src 字符串中的内容拼接到 dest 字符串的末尾：1char *strcat(char *dest, const char *src); 因为 C 字符串不记录自身的长度， 所以 strcat 假定用户在执行这个函数时， 已经为 dest 分配了足够多的内存， 可以容纳 src 字符串中的所有内容， 而一旦这个假定不成立时， 就会产生缓冲区溢出。 举个例子， 假设程序里有两个在内存中紧邻着的 C 字符串 s1 和 s2 ， 其中 s1 保存了字符串 “Redis” ， 而 s2 则保存了字符串 “MongoDB” ， 如图 2-7 所示。如果一个程序员决定通过执行：1strcat(s1, " Cluster"); 将 s1 的内容修改为 “Redis Cluster” ， 但粗心的他却忘了在执行 strcat 之前为 s1 分配足够的空间， 那么在 strcat 函数执行之后， s1 的数据将溢出到 s2 所在的空间中， 导致 s2 保存的内容被意外地修改， 如图 2-8 所示。与 C 字符串不同， SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性： 当 SDS API 需要对 SDS 进行修改时， API 会先检查 SDS 的空间是否满足修改所需的要求， 如果不满足的话， API 会自动将 SDS 的空间扩展至执行修改所需的大小， 然后才执行实际的修改操作， 所以使用 SDS 既不需要手动修改 SDS 的空间大小， 也不会出现前面所说的缓冲区溢出问题。 举个例子， SDS 的 API 里面也有一个用于执行拼接操作的 sdscat 函数， 它可以将一个 C 字符串拼接到给定 SDS 所保存的字符串的后面， 但是在执行拼接操作之前， sdscat 会先检查给定 SDS 的空间是否足够， 如果不够的话， sdscat 就会先扩展 SDS 的空间， 然后才执行拼接操作。 比如说， 如果我们执行：1sdscat(s, " Cluster"); 其中 SDS 值 s 如图 2-9 所示， 那么 sdscat 将在执行拼接操作之前检查 s 的长度是否足够， 在发现 s 目前的空间不足以拼接 “ Cluster” 之后， sdscat 就会先扩展 s 的空间， 然后才执行拼接 “ Cluster” 的操作， 拼接操作完成之后的 SDS 如图 2-10 所示。注意图 2-10 所示的 SDS ： sdscat 不仅对这个 SDS 进行了拼接操作， 它还为 SDS 分配了 13 字节的未使用空间， 并且拼接之后的字符串也正好是 13 字节长， 这种现象既不是 bug 也不是巧合， 它和 SDS 的空间分配策略有关， 接下来的小节将对这一策略进行说明 减少修改字符串时带来的内存重分配次数正如前两个小节所说， 因为 C 字符串并不记录自身的长度， 所以对于一个包含了 N 个字符的 C 字符串来说， 这个 C 字符串的底层实现总是一个 N+1 个字符长的数组（额外的一个字符空间用于保存空字符）。 因为 C 字符串的长度和底层数组的长度之间存在着这种关联性， 所以每次增长或者缩短一个 C 字符串， 程序都总要对保存这个 C 字符串的数组进行一次内存重分配操作： 如果程序执行的是增长字符串的操作， 比如拼接操作（append）， 那么在执行这个操作之前， 程序需要先通过内存重分配来扩展底层数组的空间大小 —— 如果忘了这一步就会产生缓冲区溢出。 如果程序执行的是缩短字符串的操作， 比如截断操作（trim）， 那么在执行这个操作之后， 程序需要通过内存重分配来释放字符串不再使用的那部分空间 —— 如果忘了这一步就会产生内存泄漏。 举个例子， 如果我们持有一个值为 “Redis” 的 C 字符串 s ， 那么为了将 s 的值改为 “Redis Cluster” ， 在执行：1strcat(s, " Cluster"); 之前， 我们需要先使用内存重分配操作， 扩展 s 的空间。 之后， 如果我们又打算将 s 的值从 “Redis Cluster” 改为 “Redis Cluster Tutorial” ， 那么在执行：1strcat(s, " Tutorial"); 之前， 我们需要再次使用内存重分配扩展 s 的空间， 诸如此类。 因为内存重分配涉及复杂的算法， 并且可能需要执行系统调用， 所以它通常是一个比较耗时的操作： 在一般程序中， 如果修改字符串长度的情况不太常出现， 那么每次修改都执行一次内存重分配是可以接受的。 但是 Redis 作为数据库， 经常被用于速度要求严苛、数据被频繁修改的场合， 如果每次修改字符串的长度都需要执行一次内存重分配的话， 那么光是执行内存重分配的时间就会占去修改字符串所用时间的一大部分， 如果这种修改频繁地发生的话， 可能还会对性能造成影响。 为了避免 C 字符串的这种缺陷， SDS 通过未使用空间解除了字符串长度和底层数组长度之间的关联： 在 SDS 中， buf 数组的长度不一定就是字符数量加一， 数组里面可以包含未使用的字节， 而这些字节的数量就由 SDS 的 free 属性记录。 通过未使用空间， SDS 实现了空间预分配和惰性空间释放两种优化策略。 空间预分配空间预分配用于优化 SDS 的字符串增长操作： 当 SDS 的 API 对一个 SDS 进行修改， 并且需要对 SDS 进行空间扩展的时候， 程序不仅会为 SDS 分配修改所必须要的空间， 还会为 SDS 分配额外的未使用空间。 其中， 额外分配的未使用空间数量由以下公式决定： 如果对 SDS 进行修改之后， SDS 的长度（也即是 len 属性的值）将小于 1 MB ， 那么程序分配和 len 属性同样大小的未使用空间， 这时 SDS len 属性的值将和 free 属性的值相同。 举个例子， 如果进行修改之后， SDS 的 len 将变成 13 字节， 那么程序也会分配 13 字节的未使用空间， SDS 的 buf 数组的实际长度将变成 13 + 13 + 1 = 27 字节（额外的一字节用于保存空字符）。 如果对 SDS 进行修改之后， SDS 的长度将大于等于 1 MB ， 那么程序会分配 1 MB 的未使用空间。 举个例子， 如果进行修改之后， SDS 的 len 将变成 30 MB ， 那么程序会分配 1 MB 的未使用空间， SDS 的 buf 数组的实际长度将为 30 MB + 1 MB + 1 byte 。 通过空间预分配策略， Redis 可以减少连续执行字符串增长操作所需的内存重分配次数。 举个例子， 对于图 2-11 所示的 SDS 值 s 来说， 如果我们执行：1sdscat(s, " Cluster"); 那么 sdscat 将执行一次内存重分配操作， 将 SDS 的长度修改为 13 字节， 并将 SDS 的未使用空间同样修改为 13 字节， 如图 2-12 所示。如果这时， 我们再次对 s 执行：1sdscat(s, " Tutorial"); 那么这次 sdscat 将不需要执行内存重分配： 因为未使用空间里面的 13 字节足以保存 9 字节的 “ Tutorial” ， 执行 sdscat 之后的 SDS 如图 2-13 所示。在扩展 SDS 空间之前， SDS API 会先检查未使用空间是否足够， 如果足够的话， API 就会直接使用未使用空间， 而无须执行内存重分配。 通过这种预分配策略， SDS 将连续增长 N 次字符串所需的内存重分配次数从必定 N 次降低为最多 N 次。 惰性空间释放惰性空间释放用于优化 SDS 的字符串缩短操作： 当 SDS 的 API 需要缩短 SDS 保存的字符串时， 程序并不立即使用内存重分配来回收缩短后多出来的字节， 而是使用 free 属性将这些字节的数量记录起来， 并等待将来使用。 举个例子， sdstrim 函数接受一个 SDS 和一个 C 字符串作为参数， 从 SDS 左右两端分别移除所有在 C 字符串中出现过的字符。 比如对于图 2-14 所示的 SDS 值 s 来说， 执行：1sdstrim(s, "XY"); // 移除 SDS 字符串中的所有 'X' 和 'Y' 会将 SDS 修改成图 2-15 所示的样子。注意执行 sdstrim 之后的 SDS 并没有释放多出来的 8 字节空间， 而是将这 8 字节空间作为未使用空间保留在了 SDS 里面， 如果将来要对 SDS 进行增长操作的话， 这些未使用空间就可能会派上用场。 举个例子， 如果现在对 s 执行：1sdscat(s, " Redis"); 那么完成这次 sdscat 操作将不需要执行内存重分配： 因为 SDS 里面预留的 8 字节空间已经足以拼接 6 个字节长的 “ Redis” ， 如图 2-16 所示。通过惰性空间释放策略， SDS 避免了缩短字符串时所需的内存重分配操作， 并为将来可能有的增长操作提供了优化。 与此同时， SDS 也提供了相应的 API ， 让我们可以在有需要时， 真正地释放 SDS 里面的未使用空间， 所以不用担心惰性空间释放策略会造成内存浪费。 二进制安全C 字符串中的字符必须符合某种编码（比如 ASCII）， 并且除了字符串的末尾之外， 字符串里面不能包含空字符， 否则最先被程序读入的空字符将被误认为是字符串结尾 —— 这些限制使得 C 字符串只能保存文本数据， 而不能保存像图片、音频、视频、压缩文件这样的二进制数据。 举个例子， 如果有一种使用空字符来分割多个单词的特殊数据格式， 如图 2-17 所示， 那么这种格式就不能使用 C 字符串来保存， 因为 C 字符串所用的函数只会识别出其中的 “Redis” ， 而忽略之后的 “Cluster” 。虽然数据库一般用于保存文本数据， 但使用数据库来保存二进制数据的场景也不少见， 因此， 为了确保 Redis 可以适用于各种不同的使用场景， SDS 的 API 都是二进制安全的（binary-safe）： 所有 SDS API 都会以处理二进制的方式来处理 SDS 存放在 buf 数组里的数据， 程序不会对其中的数据做任何限制、过滤、或者假设 —— 数据在写入时是什么样的， 它被读取时就是什么样。 这也是我们将 SDS 的 buf 属性称为字节数组的原因 —— Redis 不是用这个数组来保存字符， 而是用它来保存一系列二进制数据。 比如说， 使用 SDS 来保存之前提到的特殊数据格式就没有任何问题， 因为 SDS 使用 len 属性的值而不是空字符来判断字符串是否结束， 如图 2-18 所示。通过使用二进制安全的 SDS ， 而不是 C 字符串， 使得 Redis 不仅可以保存文本数据， 还可以保存任意格式的二进制数据。 兼容部分 C 字符串函数虽然 SDS 的 API 都是二进制安全的， 但它们一样遵循 C 字符串以空字符结尾的惯例： 这些 API 总会将 SDS 保存的数据的末尾设置为空字符， 并且总会在为 buf 数组分配空间时多分配一个字节来容纳这个空字符， 这是为了让那些保存文本数据的 SDS 可以重用一部分 &lt;string.h&gt; 库定义的函数。举个例子， 如图 2-19 所示， 如果我们有一个保存文本数据的 SDS 值 sds ， 那么我们就可以重用 &lt;string.h&gt;/strcasecmp 函数， 使用它来对比 SDS 保存的字符串和另一个 C 字符串：1strcasecmp(sds-&gt;buf, "hello world"); 这样 Redis 就不用自己专门去写一个函数来对比 SDS 值和 C 字符串值了。 与此类似， 我们还可以将一个保存文本数据的 SDS 作为 strcat 函数的第二个参数， 将 SDS 保存的字符串追加到一个 C 字符串的后面：1strcat(c_string, sds-&gt;buf); 这样 Redis 就不用专门编写一个将 SDS 字符串追加到 C 字符串之后的函数了。 通过遵循 C 字符串以空字符结尾的惯例， SDS 可以在有需要时重用 &lt;string.h&gt; 函数库， 从而避免了不必要的代码重复。 总结 C 字符串 SDS 获取字符串长度的复杂度为 O(N) 。 获取字符串长度的复杂度为 O(1) 。 API 是不安全的，可能会造成缓冲区溢出。 API 是安全的，不会造成缓冲区溢出。 修改字符串长度 N 次必然需要执行 N 次内存重分配。 修改字符串长度 N 次最多需要执行 N 次内存重分配。 只能保存文本数据。 可以保存文本或者二进制数据。 可以使用所有 &lt;string.h&gt;库中的函数。 可以使用一部分 &lt;string.h&gt;库中的函数。 重点回顾 Redis 只会使用 C 字符串作为字面量， 在大多数情况下， Redis 使用 SDS （Simple Dynamic String，简单动态字符串）作为字符串表示。 比起 C 字符串， SDS 具有以下优点： 常数复杂度获取字符串长度。 杜绝缓冲区溢出。 减少修改字符串长度时所需的内存重分配次数。 二进制安全。 兼容部分 C 字符串函数。 参考 http://redisbook.com]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>redis</tag>
        <tag>字符串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux epoll原理]]></title>
    <url>%2F2017%2F09%2F11%2Fepoll-mechanism%2F</url>
    <content type="text"><![CDATA[epoll是Linux下的一种IO多路复用技术，可以非常高效的处理数以百万计的socket句柄。 先看看使用c封装的3个epoll系统调用： int epoll_create(int size) epoll_create建立一个epoll对象。参数size是内核保证能够正确处理的最大句柄数，多于这个最大数时内核可不保证效果。 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event) epoll_ctl可以操作epoll_create创建的epoll，如将socket句柄加入到epoll中让其监控，或把epoll正在监控的某个socket句柄移出epoll。 int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout) epoll_wait在调用时，在给定的timeout时间内，所监控的句柄中有事件发生时，就返回用户态的进程。 大概看看epoll内部是怎么实现的： epoll初始化时，会向内核注册一个文件系统，用于存储被监控的句柄文件，调用epoll_create时，会在这个文件系统中创建一个file节点。同时epoll会开辟自己的内核高速缓存区，以红黑树的结构保存句柄，以支持快速的查找、插入、删除。还会再建立一个list链表，用于存储准备就绪的事件。 当执行epoll_ctl时，除了把socket句柄放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后，就把socket插入到就绪链表里。 当epoll_wait调用时，仅仅观察就绪链表里有没有数据，如果有数据就返回，否则就sleep，超时时立刻返回。 epoll的两种工作模式： LT：level-trigger，水平触发模式，只要某个socket处于readable/writable状态，无论什么时候进行epoll_wait都会返回该socket。 ET：edge-trigger，边缘触发模式，只有某个socket从unreadable变为readable或从unwritable变为writable时，epoll_wait才会返回该socket。 socket读数据 socket写数据 参考 http://www.jianshu.com/p/0d497fe5484a]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nio</tag>
        <tag>epoll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack安装-keystone]]></title>
    <url>%2F2017%2F09%2F09%2Fopenstack-install-keystone%2F</url>
    <content type="text"><![CDATA[keystone 是openstack中所有service的权限管理和接口入口，所以先安装它这一章都是在controller节点操作。。。 前提 切换到root用户，执行下面命令 1$ mysql 创建keystone数据库: 1MariaDB [(none)]&gt; CREATE DATABASE keystone; 赋予合适权限给keystone数据库： 1234MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \IDENTIFIED BY 'KEYSTONE_DBPASS';MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \IDENTIFIED BY 'KEYSTONE_DBPASS'; 用一个合适的密码覆盖KEYSTONE_DBPASS 退出数据库 安装和配置组件 运行下面命令安装包： 1$ apt install keystone apache2 libapache2-mod-wsgi 编辑/etc/keystone/keystone.conf 在[database] 区域，配置数据库访问连接： 123[database]# ...connection = mysql+pymysql://keystone:KEYSTONE_DBPASS@controller/keystone 替换掉KEYSTONE_DBPASS,密码是上面配置的数据库密码 去掉这个区域里面其他关于connection的属性 在[token] 区域，配置Fernet： 123[token]# ...provider = fernet 部署身份服务（Identity service）数据库： 1$ su -s /bin/sh -c "keystone-manage db_sync" keystone 初始化Fernet 12$ keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone$ keystone-manage credential_setup --keystone-user keystone --keystone-group keystone 启动身份服务（Identity service） 12345$ keystone-manage bootstrap --bootstrap-password ADMIN_PASS \--bootstrap-admin-url http://controller:35357/v3/ \--bootstrap-internal-url http://controller:5000/v3/ \--bootstrap-public-url http://controller:5000/v3/ \--bootstrap-region-id RegionOne 替换ADMIN_PASS 配置Apache因为这个服务是跑在Apache里面的，所以需要配置之。 修改/etc/apache2/apache2.conf文件，配置ServerName选项： 1ServerName controller 重启 1$ service apache2 restart 编写admin环境变量脚本创建一个admin-openrc文件，内容如下：12345678export OS_PROJECT_DOMAIN_NAME=Defaultexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=ADMIN_PASSexport OS_AUTH_URL=http://controller:35357/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2 ADMIN_PASS 用上面的创建的密码替换掉这个 创建域(domain),项目(project),用户(users)和角色(roles)创建这些之前，先执行上面那个脚本，切换到admin用户。1. admin-openrc 因为OpenStack默认创建了default的域，所以这次不用创建域了 创建一个service project 1234567891011121314$ openstack project create --domain default \--description "Service Project" service+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Service Project || domain_id | default || enabled | True || id | 24ac7f19cd944f4cba1d77469b2a73ed || is_domain | False || name | service || parent_id | default |+-------------+----------------------------------+ 创建demoproject,user和role 1234567891011121314$ openstack project create --domain default \--description "Demo Project" demo+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Demo Project || domain_id | default || enabled | True || id | 231ad6e7ebba47d6a1e57e1cc07ae446 || is_domain | False || name | demo || parent_id | default |+-------------+----------------------------------+ 123456789101112131415$ openstack user create --domain default \--password-prompt demoUser Password:Repeat User Password:+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | aeda23aa78f44e859900e22c24817832 || name | demo || options | &#123;&#125; || password_expires_at | None |+---------------------+----------------------------------+ 123456789$ openstack role create user+-----------+----------------------------------+| Field | Value |+-----------+----------------------------------+| domain_id | None || id | 997ce8d05fc143ac97d83fdfb5998552 || name | user |+-----------+----------------------------------+ 1$ openstack role add --project demo --user demo user 创建demo用户的环境脚本 demo-openrc 12345678export OS_PROJECT_DOMAIN_NAME=Defaultexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_NAME=demoexport OS_USERNAME=demoexport OS_PASSWORD=DEMO_PASSexport OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2 DEMO_PASS替换成上一步创建demo用户时保存的密码 好了现在执行一下这个脚本就切换到demo用户了 1. demo-openrc 验证切换到admin用户1. admin-openrc 123456789101112$ openstack token issue+------------+-----------------------------------------------------------------+| Field | Value |+------------+-----------------------------------------------------------------+| expires | 2016-02-12T20:44:35.659723Z || id | gAAAAABWvjYj-Zjfg8WXFaQnUd1DMYTBVrKw4h3fIagi5NoEmh21U72SrRv2trl || | JWFYhLi2_uPR31Igf6A8mH2Rw9kv_bxNo1jbLNPLGzW_u5FC7InFqx0yYtTwa1e || | eq2b0f6-18KZyQhs7F3teAta143kJEWuNEYET-y7u29y0be1_64KYkM7E || project_id | 343d245e850143a096806dfaefa9afdc || user_id | ac3377633149401296f6c0d92d79dc16 |+------------+-----------------------------------------------------------------+ 上面输出上面类似代表正常了。demo用户也是类似，这里就不演示了。 总结这里需要注意的是，用户的环境脚本，其实他只是方便切换用户的，就算不做，都可以通过把环境变量作为openstack命令参数来执行，例如上面验证可以用下面命令：123456789101112131415$ openstack --os-auth-url http://controller:35357/v3 \ --os-project-domain-name Default --os-user-domain-name Default \ --os-project-name admin --os-username admin token issuePassword: #这里要输入admin密码+------------+-----------------------------------------------------------------+| Field | Value |+------------+-----------------------------------------------------------------+| expires | 2016-02-12T20:14:07.056119Z || id | gAAAAABWvi7_B8kKQD9wdXac8MoZiQldmjEO643d-e_j-XXq9AmIegIbA7UHGPv || | atnN21qtOMjCFWX7BReJEQnVOAj3nclRQgAYRsfSU_MrsuWb4EDtnjU7HEpoBb4 || | o6ozsA_NmFWEpLeKy0uNn_WeKbAhYygrsmQGA49dclHVnz-OMVLiyM9ws || project_id | 343d245e850143a096806dfaefa9afdc || user_id | ac3377633149401296f6c0d92d79dc16 |+------------+-----------------------------------------------------------------+]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>ubuntu</tag>
        <tag>kvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go的变量和字面值的类型]]></title>
    <url>%2F2017%2F09%2F09%2Fgo-type-literal%2F</url>
    <content type="text"><![CDATA[前几天逛v2ex，无聊看到一个关于这个的话题 golang 的字面值与类型转换，来猜猜结果，所以现在总结下，免得以后进坑。 论述先上代码1234567891011121314package mainimport "fmt"func main() &#123; a := 1 b := 3 fmt.Printf("%T\n", a / b) fmt.Printf("%T\n", a / 3) fmt.Printf("%T\n", a / 3.0) fmt.Printf("%T\n", 1 / 3) fmt.Printf("%T\n", 1 / 3.0)&#125; 运行代码：12345intintintintfloat64 其实这里有奇怪的第三行和最后一行的输出结果，第三行里面a=1跟最后一行是一样的，为什么结果类型不一样呢，很明显这里有类型转换了，究竟谁类型转换了呢，继续上代码1234567891011package mainimport "fmt"func main() &#123; a := 1 fmt.Println(a / 3) fmt.Println(a / 3.0) //fmt.Println(a / 3.1) //类型错误&#125; 结果12300# literal/test2/main.go:10: constant 3.1 truncated to integer 很明显是字面值转换了类型，最后一行的3.1是转换不了整形的，所以就报错了，而3.0是没问题的，那为什么变量不会转换呢，继续上代码123456789101112package mainimport "fmt"func main() &#123; a := 1 b := 3.0 fmt.Printf("%T\n", a) fmt.Printf("%T\n", b) //fmt.Printf("%T\n", a/b) //编译错误&#125; 结果：123intfloat64#literal/test3/main.go:11: invalid operation: a / b (mismatched types int and float64) 很明显变量类型在初始化赋值的时候就确定，在运算的时候变量不会去类型转换。 总结go里面的变量运算是保证运算变量一定是相同类型才行，否则会编译错误，而且是初始赋值后就确定类型，不会在运算时自动帮你转换。但是字面值不同，在不同的场景会转换成不同的类型，当然前提是可以转换，否则就跟上面的例子3.1一样，没办法转换成整形而报错。所有代码在 https://github.com/ejunjsh/go-code/tree/master/literal]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript是如何工作的(二)]]></title>
    <url>%2F2017%2F08%2F26%2Fjavascript-how-work-2%2F</url>
    <content type="text"><![CDATA[几个星期前，我们开始了一个系列，目标是深入理解javascript和它怎么工作的：通过这些我们可以更容易的写出好代码和apps。 系列的第一篇主要是提供一个关于引擎，运行时和调用栈的概括。这篇文章将深入到Google V8 javascript 引擎。同时，还提供一些快速的意见令你能够写出好的javascript代码。 概览一个javascript引擎是一个解释器也是个程序，它执行javascript代码。它可以被实现为一个标准解释器，也可以是一个即时（just-in-time）的编译器（以不同形式编译javascript成字节码）下面是一个流行的javascript引擎的列表： V8 — 谷歌开源的, 用 C++写的。 Rhino — 由Mozilla基金会管理,开源的, 完全用java写的。 SpiderMonkey — 第一个javascript引擎 Netscape Navigator开发,现在Firefox维护。 JavaScriptCore — 开源，由Apple开发 ，Safari浏览器的引擎。 KJS — KDE的引擎，由 Harri Porten开发，是Konqueror桌面系统的浏览器引擎。 Chakra (JScript9) — IE的 Chakra (JavaScript) — 微软Edge Nashorn - OpenJDK的开源一部分, 由Oracle Java 语言和工具组开发 JerryScript — 一个轻量级引擎. 为什么要创造一个V8引擎？V8引擎由Google创建并开源，c++编写。用于Google的Chrome浏览器。不像其他引擎，V8还是流行的Node.js的运行时引擎。V8是第一个为了性能提升的浏览器引擎。为了达到更好的性能，相比于使用解释器，V8更倾向于使用编译器编译javascript代码成更高效的机器码。它像其他现代化javascript引擎如SpiderMonkey或者Rhino (Mozilla)一样，使用JIT(Just-In-Time)编译器在执行阶段编译代码，唯一不同的是，V8不会生成字节码或任意中间代码。 V8曾经有两个编译器在5.9版本发布（今年早些时候）之前，V8曾经有两个编译器： full-codegen — 一个简单快速的编译器，用来生成简单，相对慢的机器码。 Crankshaft - 一个更复杂(Just-In-Time) 优化的编译，用来生成更优的机器码。 V8引擎内部也用了一些线程： 正如你想的，主线程就是拿到代码，编译代码和执行代码。 还有些线程用来编译和优化代码，协助主线程，让主线程继续执行代码。 一个剖析器线程用来汇报哪些方法需要Crankshaft编译器优化。 其他一些线程用来做垃圾回收 当开始执行javascript代码，V8运用full-codegen来直接翻译解析过的javascript代码为机器码，这个过程没有任何中间转换，所以执行机器码非常快.由于没有用到任何中间的字节码，所以就没有需要解释器的必要了。 当你的代码运行一段时间后，剖析器线程就能收集到足够的数据来确定哪些方法应该被优化。 接下来，Crankshaft 优化编译器开始运行在其他线程。它翻译javascript抽象语法树到一个高级别的静态单赋值(SSA) 形态,又叫Hydrogen(氢？)。然后优化这个Hydrogen图。大部分的优化都在这个层次上完成。 内联（inlining）首个优化方法就是内联，它会提前尽可能的内联更多的代码。内联是一个替换代码的一个过程，用方法体替换到调用的地方（其实就是方法展开）。这样一步简单的优化可以令接下来的优化更有意义。 隐藏类(Hidden class)javascript是一种基于原型的语言：没有类和对象是通过克隆进程创建的(机翻😁)。javascript也是一种动态语言，他能够随意的添加和删除一个对象的属性，即使这个对象已经实例化了。大部分javascript解释器使用类字典的结构(基于哈希函数)来存储对象属性值在内存的位置。相比非动态语言如java和c#，这种结构使得取值是种计算昂贵的操作。对java来说，在编译之前就已经确定对象的属性，运行时也不能随意添加和删除属性的（当然，c#支持动态类型,那就在其他话题里了）。所以，属性的值（或者属性的指针）可以存储在一个连续的缓存里面，属性之间的位移更是固定的，而且位移的长度可以容易的基于属性类型来确定。这些对于javascript来说是不可能的，因为javascript的类型可以在运行时改变。由于这种字典的取地址方式是不高效的，所以V8用了一个不同的方法来取代：隐藏类(Hidden class)。隐藏类的运作方式跟java的固定对象布局类似，除了它们是在运行时创建的。下面举个栗子：12345function Point(x, y) &#123; this.x = x; this.y = y;&#125;var p1 = new Point(1, 2); 一旦new Point(1, 2)被调用，V8将创建一个C0的隐藏类。由于Point没有属性定义,所以C0是空的。 一旦this.x = x(在Point函数)被执行，V8将创建一个基于C0的隐藏类C1。C1描述了x的内存的位置（相对于对象指针），在这个情况下，x的位置存在位移0上，这代表了point对象是一个连续的内存，它的第一个位移对应的是属性x。同时V8也用“类转换”更新了C0，表明了如果一个属性x加到point对象，隐藏类就应该要从C0转换到C1。所以现在point对象的隐藏类为C1。每一次一个新的属性加到一个对象，一条转换的路径更新到旧的隐藏类并指向新的隐藏类。隐藏类转换是很重要的，因为同样方式创建的对象都共享同一个隐藏类。如果两个对象共享一个隐藏类，同时相同的属性加到这个两个对象的话，那么转换将保证这两个对象还是共享同一个新的隐藏类，而且共享同一个隐藏类有益于优化代码。 当this.y = y被执行，一个新的隐藏类C1被创建，同时一个类转换加到C1上面，表明了如果一个属性y加到一个point对象（已经有x属性的），就要把隐藏类转换成C2。现在point对象的隐藏类就是C2了。 隐藏类的转换依赖于属性的加入顺序。看一下下面的代码：12345678910function Point(x, y) &#123; this.x = x; this.y = y;&#125;var p1 = new Point(1, 2);p1.a = 5;p1.b = 6;var p2 = new Point(3, 4);p2.b = 7;p2.a = 8; 现在你肯定会认为p1和p2都是共享同一个隐藏类和转换路径，其实不然。对于p1，第一个属性是a再到b，而p2的话是先b在到a，所以p1和p2是分别两个不同的隐藏类和不同的两条转换路径。所以，对于动态属性最好是用相同的顺序加入到对象里面，这样有利于隐藏类的重用。 内联缓存(Inline caching)V8利用内联缓存技术来优化动态类型语言。内联缓存依赖于观察哪些方法在哪些相同的对象类型被重复调用。更深的介绍可以看这里我们用更通用的概念来说说内联缓存（如果你没时间去看上面的介绍的链接的话。）所以，内联缓存是怎么工作的呢？V8维护一个对象类型的缓存，当一个对象做为参数传递到一个函数调用中，那V8会缓存这个对象，并假设这个对象会在未来会再一次作为参数传递到一个函数调用中。如果V8的这个假设是正确的话，在下次传递对象到一个方法调用的时候，就会绕过查找类型对象的属性的过程，直接使用之前查找隐藏类所储存的信息。 所以隐藏类和内联缓存是怎么样关联起来的呢？无论一个指定对象方法什么时候被执行，V8引擎都会去查找那个对象的隐藏类去决定指定属性的访问位移。在两次成功调用相同隐藏类的相同方法后，V8就会忽略隐藏类的查找并简单的用属性位移和这个对象指针相加来确定地址。对于未来的那个方法的调用，V8都假设这个对象的隐藏类都没有改变，直接使用之前查找后对象内存的位移来访问属性，这样大大增加执行速度。 相同类型的对象共享相同隐藏类是很重要的，原因是内存缓存。如果你创建两个相同类型的对象，但它们的隐藏类不同（前面例子有提到），V8将没办法用到内联缓存，因为尽管类型相同，但是它们对应的隐藏类分配的属性位移是不同的。这两个对象基本上是一样的，但是a和b属性是用不同的顺序创建的。 编译机器码一旦Hydrogen图被优化，Crankshaft降低它为一个低级别的表述，称为Lithium。大多数Lithium实现是架构指定的。注册器分配发生在这个级别。 最后，Lithium被编译为机器码。有一些编译发生在OSR:栈中替换。在我们编译和优化一个明显长时间运行的方法时，我们有可能已经运行了这个方法了。V8不会忘了这个方法重新运行一个优化的版本的方法，而是转换所有的上下文（栈，注册器），这样就可以在执行中切换到优化版本。这是一个复杂的任务，记得在其他优化里，V8已经一开始就内联代码了。V8不是唯一有这能力的引擎。 这里有个保障是，一旦引擎的假设不成立的话，会把优化过的代码回滚回之前未优化的代码。这个保障称之为去优化（deoptimization） 垃圾回收对于垃圾回收，V8使用传统的分代标记清理的方式来清除旧的对象。标记的阶段一般都会停止javascript的执行。为了控制GC的成本和令执行更加稳定，V8用了递增标记来取代全堆标记。递增标记只是在部分堆中递增标记可能的对象，之后回到正常的代码执行。到下次执行GC的时候，会从上次GC标记的堆中开始。这样的话，停止时间很少。之前提及过，清理过程是在不同的线程执行的。 Ignition和TurboFanV8的5.9版本在2017年初发布，一个新的执行管道被引入。这个新的管道使得V8在现实的javascript应用程序中达到更高的性能和更少内存使用。 新的执行管道由V8解释器Ignition和V8最新优化编译器TurboFan 组成。 你可以在这里查阅来自V8团队的博客文章. 自从5.9版本的V8发布，full-codegen和Crankshaft（这两个技术从2010就开始服务V8了）不再被V8用来执行javascript，当V8团队要跟上新的javascript语言特性的步伐和这些特性更需要优化的支持。 这就意味着V8总体来讲将是一个更简单和更容易维护的架构。在网页和Node.js的性能改进 这些改进只是个开始。新的Ignition和TurboFan为更长远的优化铺平了道路，并在这几年提升javascript的性能和缩小Node.js和Chrome的差距。 怎么写出最好的javascript代码最后，这里有些建议帮助你们写出更优更好的javascript。我想，当你看到这里，你心里已经有所感悟了，但是，我还是总结下吧： 对象属性的顺序:初始化对象属性最好要按相同顺序，这样，他们的隐藏类和后续的优化代码能够共享。 动态属性：加一个动态属性会令一个对象的隐藏类改变，和拖慢任何一个方法，因为这个方法已经针对前一个隐藏类优化的了。所以，尽可能分配在构造函数里面分配所有的对象属性。 方法：重复执行相同的方法比一次执行许多不同的方法快（因为内联缓存） 数组：避免稀疏数组，因为它们的key不是递增的。稀疏数组并不是每个索引都有元素，所以它更像个哈希表。还有访问这样的数组是昂贵的。还有就是不要一次分配个大数组，最好按需分配。最后，不要在数组中删除元素，这样就令key稀疏了。 标签值：V8用32位来代表对象和数字。它用一位来区分对象（flag=1）和整形（flag=0），这个整形也叫SMI（SMall Integer）,因为它只有31位。这样的话，如果一个数字值大于31位，V8将会将它转化为一个double并使用一个对象把它装箱。尽可能使用31位的有符号数字，这样能够避免昂贵的装箱操作。 翻译 https://blog.sessionstack.com/how-javascript-works-inside-the-v8-engine-5-tips-on-how-to-write-optimized-code-ac089e62b12e]]></content>
      <categories>
        <category>javascript</category>
      </categories>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript是如何工作的(一)]]></title>
    <url>%2F2017%2F08%2F26%2Fjavascript-how-work%2F</url>
    <content type="text"><![CDATA[简评：JavaScript 是越来越受欢迎了，很多团队都在采用这些语言工作。前端、后端、嵌入式设备等等，都可以看见它的身影。虽然我们知其然，但又知其所以然吗？ 概览大家应该都知道 JavaScript 是单线程的，以及听过 V8 引擎的概念。这篇文章将会介绍这些概念，并解释 JavaScript 是如何运行的。通过了解这些细节，开发者能更好地编写代码，正确利用其提供的 API。 JavaScript 引擎比较流行的一个 JavaScript 引擎示例就是 Google 的 V8 引擎。下图是 V8 引擎在 Chrome 和 Node.js 中使用的一个简化视图： 引擎主要由两个组件组成： 内存堆（Memory Heap ）：这是内存分配的地方 调用堆栈（Call Stack）：这是程序运行时函数的调用过程 运行在浏览器中，例如「setTimeout」这样的 API 已经有很多开发者在用了，然后引擎并没有提供这些 API，所以它们从哪里来的呢？实际情况是这样的：所以，除了引擎之外，还有浏览器提供的 Web API（像 DOM、AJAX、setTimeout 等等）。另外，还有事件循环（event loop）和回调队列（callback queue）。 调用堆栈（Call Stack）JavaScript 是单线程语言，这意味着它只有一个单一的调用堆栈。因此，它每次只能做一件事。 调用堆栈是一个数据结构，按调用顺序保存所有在运行期被调用的方法。既然是个栈，那么它就满足先入后出的特性。 我们来看一个例子：12345678function multiply(x, y) &#123; return x * y;&#125;function printSquare(x) &#123; var s = multiply(x, x); console.log(s);&#125;printSquare(5); 当引擎开始执行这段代码时，调用堆栈将为空。然后，就会有以下步骤： 调用堆栈中的每个条目称为堆栈帧（Stack Frame）。当异常发生时，它基本上是调用堆栈的状态。再看看下面这段代码：12345678910function foo() &#123; throw new Error('SessionStack will help you resolve crashes :)');&#125;function bar() &#123; foo();&#125;function start() &#123; bar();&#125;start(); 如果这是在 Chrome 中执行（假设此代码位于一个名为 foo.js 的文件中），则会产生这种情况： 当你达到最大调用堆栈时，会容易发生这种情况，特别是在没有测试代码时随意使用递归。看看这个示例代码：1234function foo() &#123; foo();&#125;foo(); 代码执行时，首先调用函数「foo」。然而，这是递归函数，调用自身的同时又没有设置终止条件，所以每一次执行，相同的函数都会被添加进堆栈中，看起来就是这样：某些时候，调用堆栈中的函数调用次数超过了调用堆栈的实际大小，那么浏览器就会抛出一个错误，看起来像这样：单线程上编写代码相对多线程来说会简单得多，你不必考虑死锁这样的复杂场景。但单线程也有许多限制，由于 JavaScript 有调用堆栈，当执行代码需要耗费大量时间时是怎样的呢？ 并发和事件循环当你在调用堆栈中进行函数调用，有时候需要大量时间才能进行处理。例如在浏览器中使用JavaScript 进行一些复杂的图像转换。在这个过程中又发生了什么？这个问题的产生是因为，虽然调用堆栈具有执行的功能，但浏览器本身是无法渲染也不能运行其他任何代码，它被卡住了。当你想执行一套流畅的 UI 时，就会产生这样的问题。大多数浏览器通过抛出异常处理错误，询问用户是否要终止网页：这个用户体验很糟糕。那么如何解决呢？答案是异步回调（asynchronous callbacks）。这是后话，下次再讲。 翻译和参考 https://blog.sessionstack.com/how-does-javascript-actually-work-part-1-b0bacc073cf]]></content>
      <categories>
        <category>javascript</category>
      </categories>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack安装准备(二)-环境的安装]]></title>
    <url>%2F2017%2F08%2F19%2Fopenstack-install-prepare-2%2F</url>
    <content type="text"><![CDATA[上图是整个openstack的架构图，里面的椭圆方块都是openstack的服务，所以安装openstack就是要安装这些服务。 按照官方建议，这次openstack安装的服务为： Identity service (keystone) Image service (glance) Compute service (nova) Networking service (neutron) Dashboard (horizon) Block Storage service (cinder) 在安装上面服务前，先要弄好环境的😁这次安装的openstack为最新的release，pike 安装openstack仓库123456# change to root$ sudo -i$ apt install software-properties-common$ add-apt-repository cloud-archive:pike$ apt update &amp;&amp; apt dist-upgrade$ apt install python-openstackclient 上面的步骤两个节点都要安装。以下步骤安装在controller 安装数据库openstack所用到的数据都会存到数据库里，所以安装一个数据库是准备的一个重要步骤。mariadb是官方建议的数据库。 安装和配置mariadb1234567891011$ apt install mariadb-server python-pymysql$ vi /etc/mysql/mariadb.conf.d/99-openstack.cnf# 加一个[mysqld]区，bind-address为管理网络ip[mysqld]bind-address = controller # 192.168.199.10 default-storage-engine = innodbinnodb_file_per_table = onmax_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8 收尾重启服务1$ service mysql restart 设置下root用户的密码，这个密码后面要用到，务必谨记。1$ mysql_secure_installation 安装消息队列openstack用消息队列来异步控制各种service，所以要装一个，rabbitmq是官方推荐，装之。 安装和配置rabbitmq1$ apt install rabbitmq-server 加一个openstack用户。1$ rabbitmqctl add_user openstack RABBIT_PASS #用你的密码替换下RABBIT_PASS，谨记这个密码，后面有用。 赋予更多权限给openstack用户1$ rabbitmqctl set_permissions openstack ".*" ".*" ".*" 安装缓存openstack用到缓存，memcached是官方推荐，还是装之。 安装和配置memcached12345$ apt install memcached python-memcache$ vi /etc/memcached.conf#监听管理网络ip#-l 127.0.0.1 改成下面这样-l controller # 192.168.199.10 收尾重启服务1$ service memcached restart 总结基本上环境已经搭好了，接下来就要安装各种服务了。😈]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>ubuntu</tag>
        <tag>kvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack安装准备(一)]]></title>
    <url>%2F2017%2F08%2F18%2Fopenstack-install-prepare-1%2F</url>
    <content type="text"><![CDATA[准备VMware由于我是习惯了mac上做实验，所以用VMware fusion，随便下个破解版即可。 准备UbuntuUbuntu去官网下载16.04的服务器版本的ISO即可。 准备网络这次实验用到两台虚拟机： controller,compute controller12345678910111213141516171819202122232425262728$ cat /etc/hostnamecontroller$ cat /etc/hosts127.0.0.1 localhost192.168.199.11 compute192.168.199.10 controller$ cat /etc/network/interfaces# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto ens33iface ens33 inet staticaddress 192.168.199.10netmask 255.255.255.0gateway 192.168.199.1dns-nameservers 192.168.199.1auto ens34iface ens34 inet staticaddress 10.170.56.10netmask 255.255.255.0auto ens35iface ens35 inet manual 接口 ip 模式 ens33 192.168.199.10 桥接 ens34 10.170.56.10 私有 ens35 网关192.168.112.2 nat compute12345678910111213141516171819202122232425$ cat /etc/hostnamecompute$ cat /etc/hosts127.0.0.1 localhost192.168.199.11 compute192.168.199.10 controller$ cat /etc/network/interfaces# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto ens33iface ens33 inet staticaddress 192.168.199.11netmask 255.255.255.0gateway 192.168.199.1dns-nameservers 192.168.199.1auto ens34iface ens34 inet staticaddress 10.170.56.11netmask 255.255.255.0 接口 ip 模式 ens33 192.168.199.11 桥接 ens34 10.170.56.11 私有 总结PS: 桥接模式是虚拟机可以更物理机所在网络共享一套网络，例如跟物理机同一个WiFi里面的设备都可以访问物理机里面的虚拟机。这里用来做管理节点的网络。 私有模式代表虚拟机只能跟物理机作为一个网络，其他设备访问不了，一般可以用来做内部网络 nat模式用来给虚拟机访问互联网用 PSPS:接下来会在上面的两台虚拟机安装openstack，安装完openstack后，两台虚拟机对于openstack来说，就是物理机，通过openstack，创建的就是云主机（或者叫租户）了。所以必须要谨记这点。 PSPSPS: 桥接模式的ip必须是你电脑所在网络的任意不冲突的同子网的ip 私有模式的ip可以任意一个子网下的ip，这个网络是用来做租户网络的 nat网络不用配ip，这个给租户用来访问外网的，接下来实验会再提及，注意下他的网关即可，它是你的VMware的nat的一个网关。 上面网络配置好后，可以开搞了，至于怎么安装虚拟机和配置网络，可以搜索相关文章😈。]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>ubuntu</tag>
        <tag>kvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一张图了解三色标记法]]></title>
    <url>%2F2017%2F08%2F16%2Fgc-three-color%2F</url>
    <content type="text"><![CDATA[三色标记法是传统 Mark-Sweep 的一个改进，它是一个并发的 GC 算法。原理如下， 首先创建三个集合：白、灰、黑。 将所有对象放入白色集合中。 然后从根节点开始遍历所有对象（注意这里并不递归遍历），把遍历到的对象从白色集合放入灰色集合。 之后遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，之后将此灰色对象放入黑色集合 重复 4 直到灰色中无任何对象 通过write-barrier检测对象有变化，重复以上操作 收集所有白色对象（垃圾） 这个算法可以实现 “on-the-fly”，也就是在程序执行的同时进行收集，并不需要暂停整个程序。但是也会有一个缺陷，可能程序中的垃圾产生的速度会大于垃圾收集的速度，这样会导致程序中的垃圾越来越多无法被收集掉。]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>gc</tag>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一张图了解标记清除算法]]></title>
    <url>%2F2017%2F08%2F16%2Fgc-mark-sweep%2F</url>
    <content type="text"><![CDATA[这个算法分为两步，标记和清除。 标记：从程序的根节点开始， 递归地 遍历所有对象，将能遍历到的对象打上标记。 清除：讲所有未标记的的对象当作垃圾销毁。 但是这个算法也有一个缺陷，就是人们常常说的 STW 问题（Stop The World）。因为算法在标记时必须暂停整个程序，否则其他线程的代码可能会改变对象状态，从而可能把不应该回收的对象当做垃圾收集掉。当程序中的对象逐渐增多时，递归遍历整个对象树会消耗很多的时间，在大型程序中这个时间可能会是毫秒级别的。让所有的用户等待几百毫秒的 GC 时间这是不能容忍的。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postman的几种body的使用介绍]]></title>
    <url>%2F2017%2F08%2F10%2Fpostman%2F</url>
    <content type="text"><![CDATA[postman,用来模拟发送http请求的工具，里面涉及的请求body有以下几个类型，所以记下，而且也能理解http body的几种格式，分享之。。 form-data就是http请求中的multipart/form-data,它会将表单的数据处理为一条消息，以标签为单元，用分隔符分开。既可以上传键值对，也可以上传文件。当上传的字段是文件时，会有Content-Type来表名文件类型；content-disposition，用来说明字段的一些信息；由于有boundary隔离，所以multipart/form-data既可以上传文件，也可以上传键值对，它采用了键值对的方式，所以可以上传多个文件。 内容为12345678910111213141516171819202122232425POST /login HTTP/1.1Host: 10.170.56.67Content-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gWCache-Control: no-cachePostman-Token: 9843651a-5bf9-0544-03c1-fcc2a16f484b------WebKitFormBoundary7MA4YWxkTrZu0gWContent-Disposition: form-data; name=&quot;username&quot;admin------WebKitFormBoundary7MA4YWxkTrZu0gWContent-Disposition: form-data; name=&quot;password&quot;admin123------WebKitFormBoundary7MA4YWxkTrZu0gWContent-Disposition: form-data; name=&quot;abc&quot;; filename=&quot;&quot;Content-Type: ------WebKitFormBoundary7MA4YWxkTrZu0gWContent-Disposition: form-data; name=&quot;tttt&quot;; filename=&quot;&quot;Content-Type: ------WebKitFormBoundary7MA4YWxkTrZu0gW-- x-www-form-urlencoded就是application/x-www-from-urlencoded,会将表单内的数据转换为键值对，并以urlencode为格式内容为1234567POST /login HTTP/1.1Host: 10.170.56.67Content-Type: application/x-www-form-urlencodedCache-Control: no-cachePostman-Token: e6887900-a46e-2ff4-8232-de878b75f5fdusername=admin&amp;password=admin123 raw可以上传任意格式的文本，可以上传text、json、xml、html等内容为12345678910POST /login HTTP/1.1Host: 10.170.56.67Content-Type: application/jsonCache-Control: no-cachePostman-Token: 233df0e0-c6d9-98c7-4d7e-736329322683&#123; &quot;abc&quot;:&quot;cba&quot;, &quot;cba&quot;:&quot;abc&quot;&#125; 从图片和内容对比，可以发现，基本，粘什么，就发什么，不会进行任何转意。 binary相当于Content-Type:application/octet-stream,从字面意思得知，只可以上传二进制数据，通常用来上传文件，由于没有键值，所以，一次只能上传一个文件。 multipart/form-data与x-www-form-urlencoded区别 multipart/form-data：既可以上传文件等二进制数据，也可以上传表单键值对，只是最后会转化为一条信息； x-www-form-urlencoded：只能上传键值对，并且键值对都是间隔分开的]]></content>
      <categories>
        <category>http</category>
      </categories>
      <tags>
        <tag>postman</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go的是否需要用goroutine pool？]]></title>
    <url>%2F2017%2F08%2F03%2Fgo-worker-pool-if-need%2F</url>
    <content type="text"><![CDATA[这几天无聊，想到java有自己的线程池，是否对应go也有它的goroutine pool呢，所以搜了下，标准库没有，github有，但都大同小异，所以自己实现了一个。 一个简单的goroutine pool12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package workerpoolimport ( "sync" "fmt")type task func()type worker struct &#123; stopC chan bool&#125;type WorkerPool struct &#123; num int sync.Mutex taskQ chan task workers []*worker&#125;func NewWorkerPool(workerNum int,queueCap int) *WorkerPool&#123; return &amp;WorkerPool&#123;num:workerNum,taskQ:make(chan task,queueCap),workers:make([]*worker,workerNum)&#125;&#125;func (wp *WorkerPool) Execute(t task)&#123; wp.taskQ&lt;-t&#125;func (wp *WorkerPool) Start() *WorkerPool&#123; for i:=0;i&lt;wp.num;i++&#123; wp.workers[i]=&amp;worker&#123; make(chan bool)&#125; w:=wp.workers[i] go func(i int) &#123; for &#123; stop:=false select &#123; case f:=&lt;-wp.taskQ: f() case stop=&lt;-w.stopC: break &#125; if stop&#123; break &#125; &#125; fmt.Println("stop") &#125;(i) &#125; return wp&#125;func (wp *WorkerPool) Stop()&#123; for _,w:=range wp.workers&#123; w.stopC&lt;- true &#125;&#125; 代码很简单，就是NewWorkerPool一个池子的时候设置goroutine的数量和任务队列的大小。Start后就创建那么多goroutine去任务队列取任务执行，取不到任务就自循。Execute方法是把任务压进队列，如果队列满了就阻塞。 性能要测试性能，肯定要有对比，以下是没有使用pool:1234567891011121314func nopool() &#123; wg := new(sync.WaitGroup) //执行1000000次，每次都启动一个goroutine for i := 0; i &lt; 1000000; i++ &#123; wg.Add(1) go func(n int) &#123; for j := 0; j &lt; 100000; j++ &#123; &#125; defer wg.Done() &#125;(i) &#125; wg.Wait()&#125; 以下是简单版的只是单纯限制goroutine数量和任务队列的代码，没有任何封装的:12345678910111213141516171819202122232425262728func gopool() &#123; wg := new(sync.WaitGroup) //队列100 data := make(chan int, 100) //goroutine 数量10个 for i := 0; i &lt; 10; i++ &#123; wg.Add(1) go func(n int) &#123; defer wg.Done() for _ = range data &#123; func()&#123; for j := 0; j &lt; 100000; j++ &#123; &#125; &#125;() &#125; &#125;(i) &#125; //执行1000000个任务 for i := 0; i &lt; 1000000; i++ &#123; data &lt;- i &#125; close(data) wg.Wait()&#125; 然后是主角:123456789101112131415161718func workerpool() &#123; wg := new(sync.WaitGroup) //十个goroutine，队列容量100 wp:=NewWorkerPool(10,100) wp.Start() //提交1000000任务 for i := 0; i &lt; 1000000; i++ &#123; wg.Add(1) wp.Execute(func() &#123; for j := 0; j &lt; 100000; j++ &#123; &#125; wg.Done() &#125;) &#125; wg.Wait()&#125; 上面代码基本都是做同样一件事，但是后两个只开10个goroutine，第一个就开了1000000个，结果：123BenchmarkNopool-8 1 7966900091 ns/opBenchmarkGopool-8 1 7949844269 ns/opBenchmarkWorkerPool-8 1 7997732135 ns/op 可以看出来，没有区别，重新run几次基本没有多大变化。 总结由于go本身有对goroutine有调度，所以自己实现的池子来调度其实好像没有什么用。还有可能我自己能力实现不好，没发挥池子的作用😀。但是用更少的goroutine能完成同样的事情，应该是一种优化，而且这里的goroutine执行都是简单的循环，没有复杂的业务，一旦业务复杂，更少goroutine能够减少内存和goroutine切换时的cpu资源，有可能上面性能的比较会拉开。]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用树莓派搭建一个简易的NAS]]></title>
    <url>%2F2017%2F07%2F20%2Fraspberry-nas%2F</url>
    <content type="text"><![CDATA[准备 raspberry pi 3 硬盘（格式化过ext4的） 连接raspberry用的终端 安装samba12sudo apt-get updatesudo apt-get install samba samba-common-bin 配置samba12sudo cp /etc/samba/smb.conf /etc/samba/smb.conf.backsudo vim /etc/samba/smb.conf 1234567891011121314151617# 在末尾加入如下内容# 分享名称[MyNAS]# 说明信息comment = NAS Storage# 可以访问的用户valid users = pi,root# 共享文件的路径,raspberry pi 会自动将连接到其上的外接存储设备挂载到/media/pi/目录下。path = /media/pi/# 可被其他人看到资源名称（非内容）browseable = yes# 可写writable = yes# 新建文件的权限为 664create mask = 0664# 新建目录的权限为 775directory mask = 0775 可以把配置文件中你不需要的分享名称删除，例如 [homes], [printers] 等。测试配置文件是否有错误，根据提示做相应修改1testparm 添加登陆账户并创建密码，必须是 linux 已存在的用户1sudo smbpasswd -a pi 重启 samba 服务1sudo /etc/init.d/samba restart 连接一般树莓派跟你的WiFi相连的话，你的网络就能看到跟上面配置一样的分享名称，如mac上面这样的显示：如果显示没权限，可以断开连接，用你上面添加的账号登录。 问题总结 基本上不是ext4格式的硬盘都不用上传到NAS上了，因为树莓对其他格式的硬盘只有读权限。 如果是ext4格式，也不要高兴，那上传速度可以😭的 如果是其他格式的话，上面都说只能读，一般情况拷进硬盘的片片是可以用pi用户读的，如果遇到连pi用户都没有读权限的话，而且登上树莓派进到硬盘里面强制改权限都是改不了的。所以，老老实实加个root用户吧。 1sudo smbpasswd -a root 用root用户连上去基本没有不能读的。但是还是不能写。老老实实还是拔硬盘到电脑烤吧。]]></content>
      <categories>
        <category>raspberrypi</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>raspberrypi</tag>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用树莓派实现一个能播放天气的闹钟]]></title>
    <url>%2F2017%2F07%2F18%2Fraspberry-weather-clock%2F</url>
    <content type="text"><![CDATA[前提你要有个pi😄 获取天气接口这里我是用图灵机器人来获取天气的接口，你可以自己上去注册一个，下面代码URL的Key是我注册的机器人给的。1234567891011def getWeatherText(): try: response = requests.get( "http://www.tuling123.com/openapi/api?key=652ae4a714794fe6b01faa990d7a981f&amp;info=%s" % "广州今日天气") json = response.json() if json["code"] == 100000: return json["text"] else: return "" except: return "" 播放文字利用百度的接口可以转换文本为语音。默认只有女声12345def text2voice(text): url = 'http://tts.baidu.com/text2audio?idx=1&amp;tex=&#123;0&#125;&amp;cuid=baidu_speech_' \ 'demo&amp;cod=2&amp;lan=zh&amp;ctp=1&amp;pdt=1&amp;spd=4&amp;per=4&amp;vol=5&amp;pit=5'.format(text) # 用mplayer播放语音 os.system('mplayer "%s"' % url) 安装播放媒体软件上面代码你看到的mplayer,就是用来播放语音的，传个url作为参数12sudo apt-get install mplayerusage: mplayer [url] 播放音乐有了上面这个神器，你可以给播报语音前后加一首音乐😄12def playMusic(path): os.system('mplayer %s' % path) 总结利用上面的东东，可以组合些好玩的东西了，至于闹钟的唤醒，可以cron job 做，也可以代码里面实现，enjoy…😄全部代码地址 https://github.com/ejunjsh/raspberrypi-code/blob/master/clock/weather.py]]></content>
      <categories>
        <category>raspberrypi</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>raspberrypi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用go实现一个简单的restful接口]]></title>
    <url>%2F2017%2F07%2F18%2Fgo-first-rest%2F</url>
    <content type="text"><![CDATA[前言go的标准库http已经封装好很多接口，可以很简单实现一个web服务器。12345678910111213// 定义 handlerfunc HelloServer(w http.ResponseWriter, req *http.Request) &#123; io.WriteString(w, "hello, world!\n")&#125;func main() &#123; //绑定pattern和handler http.HandleFunc("/hello", HelloServer) err := http.ListenAndServe(":12345", nil) if err != nil &#123; log.Fatal("ListenAndServe: ", err) &#125;&#125; 基于上面例子可以封装一个restful接口，不是难事。 实现从上面例子可以看到，一个url pattern对应一个handler，即对应一个处理，就可以处理http请求了，所以下面的实现是基于对这两个东西的封装开始 封装一个restful app 结构12345678910111213type App struct &#123; //一个map，key是pattern，value是handler handlers map[string]func(r *HttpRequest,w HttpResponse)error //pattern数组，用来保证加入pattern的顺序，因为上面的map是无顺序的 patterns []string //一个map，key是pattern，value是http method methods map[string]string //用来实现在url path取出参数的。 regexps map[string]*regexp.Regexp pathparamanmes map[string][]string //用来处理异常的handler errHandler func( err error, r *HttpRequest,w HttpResponse)&#125; 初始化函数12345678910111213func NewApp() *App &#123; return &amp;App&#123; handlers: make(map[string]func(r *HttpRequest,w HttpResponse)error), patterns:make([]string,0), methods:make(map[string]string), regexps:make(map[string]*regexp.Regexp), pathparamanmes:make(map[string][]string), //一个默认的异常处理，直接返回异常内容 errHandler: func(err error, r *HttpRequest, w HttpResponse) &#123; w.Write( []byte(err.Error())) &#125;, &#125;&#125; 映射绑定123456789101112131415161718192021222324252627282930313233343536func(a *App) handle(method string,pattern string, handler func(r *HttpRequest,w HttpResponse) error)&#123; //绑定pattern和handler a.handlers[pattern]=handler //绑定pattern和method a.methods[pattern]=method //绑定pattern 正则，用来匹配url pattern,和获取url path 参数 a.regexps[pattern],a.pathparamanmes[pattern]=convertPatterntoRegex(pattern) for _,s:=range a.patterns&#123; if s==pattern&#123; return &#125; &#125; //加入数组，方便用此数组确定顺序 a.patterns=append(a.patterns,pattern)&#125;//绑定GETfunc (a *App) Get(pattern string, handler func(r *HttpRequest,w HttpResponse)error) &#123; a.handle("GET",pattern,handler)&#125;//绑定POSTfunc (a *App) Post(pattern string, handler func(r *HttpRequest,w HttpResponse)error) &#123; a.handle("POST",pattern,handler)&#125;//绑定DELETEfunc (a *App) Delete(pattern string, handler func(r *HttpRequest,w HttpResponse)error) &#123; a.handle("DELETE",pattern,handler)&#125;//绑定PUTfunc (a *App) Put(pattern string, handler func(r *HttpRequest,w HttpResponse) error) &#123; a.handle("PUT",pattern,handler)&#125;func (a *App) Error(handler func(err error,r *HttpRequest,w HttpResponse)) &#123; a.errHandler=handler&#125; 有了Restful接口的四个方法映射绑定，剩下的就要请求能进到来，所以接下来要写个入口才行。 编写http入口12345678910111213141516171819//http 入口func(a *App) Run(address string) error&#123; fmt.Printf("Server listens on %s",address) err:=http.ListenAndServe(address,&amp;hodler&#123;app:a&#125;) if err!=nil&#123; return err &#125; return nil&#125;//https 入口func(a *App) RunTls(address string,cert string,key string) error&#123; fmt.Printf("Server listens on %s",address) err:=http.ListenAndServeTLS(address,cert,key,&amp;hodler&#123;app:a&#125;) if err!=nil&#123; return err &#125; return nil&#125; 入口函数主要调用http库来启动http服务，然后把请求处理函数作为ListenAndServe第二个参数传入。这里由holder来实现这个处理函数。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func (h *hodler) ServeHTTP(w http.ResponseWriter, r *http.Request)&#123; //封装一下，附加更多功能 request:= newHttpRequest(r) response:=newHttpResponse(w) //捕获panic，并让errhandler处理返回。 defer func() &#123; if err:=recover();err!=nil&#123; if e,ok:=err.(error);ok&#123; h.app.errHandler(InternalError&#123;e,""&#125;,request,response) &#125; if e,ok:=err.(string);ok&#123; h.app.errHandler(InternalError&#123;nil,e&#125;,request,response) &#125; &#125; &#125;() //根据pattern的添加顺序，循环判断 for _,p:=range h.app.patterns&#123; if reg,ok:= h.app.regexps[p];ok&#123; //匹配method if method,ok:=h.app.methods[p];ok&amp;&amp;r.Method==method&#123; //匹配pattern if reg.Match([]byte(r.URL.Path)) &#123; //抽取url path parameters matchers:=reg.FindSubmatch([]byte(r.URL.Path)) pathParamMap:=make(map[string]string) if len(matchers)&gt;1&#123; if pathParamNames,ok:=h.app.pathparamanmes[p];ok&#123; for i:=1;i&lt;len(matchers);i++&#123; pathParamMap[pathParamNames[i]]=string(matchers[i]) &#125; &#125; &#125; //PathParams是封装后的request独有的属性 request.PathParams=pathParamMap if handler,ok:=h.app.handlers[p];ok&#123; //执行handler err:=handler(request,response) if err!=nil&#123; //执行errhandler h.app.errHandler(err,request,response) &#125; return &#125; &#125; &#125; &#125; &#125; //执行no found errhandler h.app.errHandler(NoFoundError&#123;&#125;,request,response)&#125; 基本一个请求的流程如下：requset-&gt;ServeHTTP()-&gt;匹配url pattern-&gt;匹配method-&gt;匹配到你的handler-&gt;执行你的handler-&gt;你的handler返回结果 返回结果由于返回结果可以有很多，所以封装了http库的http.ResponseWriter来实现WriteString,WriteJson,WriteXml,WriteFile等方法。1234567891011121314151617181920212223242526272829//封装request，附件一个PathParams来保存url path parameters.type HttpRequest struct &#123; *http.Request PathParams map[string] string&#125;type HttpResponse struct &#123; http.ResponseWriter&#125;//用来返回字符func (response *HttpResponse) WriteString(str string) error &#123; ...&#125;//返回JSONfunc (response *HttpResponse) WriteJson(jsonObj interface&#123;&#125;) error &#123; ...&#125;//返回XMLfunc (response *HttpResponse) WriteXml(xmlObj interface&#123;&#125;) error &#123; ...&#125;//返回文件func (response *HttpResponse) WriteFile(filepath string) error &#123; ...&#125;//返回一个模板htmlfunc (response *HttpResponse) WriteTemplates(data interface&#123;&#125;,tplPath ...string) error &#123; ...&#125; 例子123456789101112131415161718192021222324func main()&#123; //new 一个restful接口 app:=gorest.NewApp() //绑定 app.Get("/json", func(r *gorest.HttpRequest, w gorest.HttpResponse) error &#123; a:= struct &#123; Abc string `json:"abc"` Cba string `json:"cba"` &#125;&#123;"123","321"&#125; //返回json作为结果 return w.WriteJson(a) &#125;) app.Error(func(err error, r *gorest.HttpRequest, w gorest.HttpResponse)&#123; if e,ok:=err.(gorest.NoFoundError);ok &#123; w.Write([]byte(e.Error())) &#125; if e,ok:=err.(gorest.InternalError);ok &#123; w.Write([]byte(e.Error())) &#125; &#125;) //启动 app.Run(":8081")&#125; 收工😄 总结go的标准库封装了很多了，所以实现这个其实还是比较轻松的😄详细代码见https://github.com/ejunjsh/gorest]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一张图了解一致性hash]]></title>
    <url>%2F2017%2F07%2F16%2Fconsistent-hash%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java数据结构-linkedlist]]></title>
    <url>%2F2017%2F07%2F16%2Fjava-linkedlist%2F</url>
    <content type="text"><![CDATA[LinkedList简介 LinkedList 是一个继承于AbstractSequentialList的双向链表。它也可以被当作堆栈、队列或双端队列进行操作。 LinkedList 实现 List 接口，能对它进行队列操作。 LinkedList 实现 Deque 接口，即能将LinkedList当作双端队列使用。 LinkedList 实现了Cloneable接口，即覆盖了函数clone()，能克隆。 LinkedList 实现java.io.Serializable接口，这意味着LinkedList支持序列化，能通过序列化去传输。 LinkedList 是非同步的。 LinkedList相对于ArrayList来说，是可以快速添加，删除元素，ArrayList添加删除元素的话需移动数组元素，可能还需要考虑到扩容数组长度。 LinkedList属性LinkedList本身的 的属性比较少，主要有三个，一个是size，表名当前有多少个节点；一个是first代表第一个节点；一个是last代表最后一个节点。1234567891011public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable&#123; //当前有多少个节点 transient int size = 0; //第一个节点 transient Node&lt;E&gt; first; //最后一个节点 transient Node&lt;E&gt; last; //省略内部类和方法。。 &#125; LinkedList构造方法默认构造方法是空的，什么都没做，表示初始化的时候size为0，first和last的节点都为空：12public LinkedList() &#123; &#125; 另一个构造方法是带Collection值得对象作为入参的构造函数的，下面是执行逻辑： 使用this（）调用默认的无参构造函数。 调用addAll（）方法，传入当前的节点个数size，此时size为0，并将collection对象传递进去 检查index有没有数组越界的嫌疑 将collection转换成数组对象a 循环遍历a数组，然后将a数组里面的元素创建成拥有前后连接的节点，然后一个个按照顺序连起来。 修改当前的节点个数size的值 操作次数modCount自增1. 下面是实现的源代码：默认构造函数1234public LinkedList(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c); &#125; 调用带参数的addAll方法123public boolean addAll(Collection&lt;? extends E&gt; c) &#123; return addAll(size, c); &#125; 将collection对象转换成数组链表1234567891011121314151617181920212223242526272829303132333435363738public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; checkPositionIndex(index); Object[] a = c.toArray(); int numNew = a.length; if (numNew == 0) return false; Node&lt;E&gt; pred, succ; if (index == size) &#123; succ = null; pred = last; &#125; else &#123; succ = node(index); pred = succ.prev; &#125; for (Object o : a) &#123; @SuppressWarnings("unchecked") E e = (E) o; Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null); if (pred == null) first = newNode; else pred.next = newNode; pred = newNode; &#125; if (succ == null) &#123; last = pred; &#125; else &#123; pred.next = succ; succ.prev = pred; &#125; size += numNew; modCount++; return true; &#125; add方法add(E e)方法该方法直接将新增的元素放置链表的最后面，然后链表的长度（size）加1，修改的次数（modCount）加1123456789101112131415public boolean add(E e) &#123; linkLast(e); return true; &#125; void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++; &#125; add(int index, E element)方法指定位置往数组链表中添加元素 检查添加的位置index 有没有小于等于当前的长度链表size，并且要求大于0 如果是index是等于size，那么直接往链表的最后面添加元素，相当于调用add(E e)方法 如果index不等于size，则先是索引到处于index位置的元素，然后在index的位置前面添加新增的元素。 12345678public void add(int index, E element) &#123; checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index)); &#125; 把索引到的元素添加到新增的元素之后123456789101112void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++; &#125; get方法get方法首先是判断索引位置有没有越界，确定完成之后开始遍历链表的元素，那么从头开始遍历还是从结尾开始遍历呢，这里其实是要索引的位置与当前链表长度的一半去做对比，如果索引位置大于当前链表长度的一半，否则从结尾开始遍历1234public E get(int index) &#123; checkElementIndex(index); return node(index).item; &#125; 遍历链表元素123456789101112131415Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125; &#125; getfirst方法直接将第一个元素返回123456public E getFirst() &#123; final Node&lt;E&gt; f = first; if (f == null) throw new NoSuchElementException(); return f.item; &#125; getlast方法直接将最后一个元素返回123456public E getLast() &#123; final Node&lt;E&gt; l = last; if (l == null) throw new NoSuchElementException(); return l.item; &#125; remove方法remove（）方法remove方法本质调用的还是removeFirst方法123public E remove() &#123; return removeFirst(); &#125; removeFirst（）方法移除第一个节点，将第一个节点置空，让下一个节点变成第一个节点，链表长度减1，修改次数加1，返回移除的第一个节点。123456789101112131415161718192021public E removeFirst() &#123; final Node&lt;E&gt; f = first; if (f == null) throw new NoSuchElementException(); return unlinkFirst(f); &#125; private E unlinkFirst(Node&lt;E&gt; f) &#123; // assert f == first &amp;&amp; f != null; final E element = f.item; final Node&lt;E&gt; next = f.next; f.item = null; f.next = null; // help GC first = next; if (next == null) last = null; else next.prev = null; size--; modCount++; return element; &#125; removeLast（）方法移除最后一个节点，将最后一个节点置空，最后一个节点的上一个节点变成last节点，，链表长度减1，修改次数加1，返回移除的最后一个节点。123456789101112131415161718192021public E removeLast() &#123; final Node&lt;E&gt; l = last; if (l == null) throw new NoSuchElementException(); return unlinkLast(l); &#125; private E unlinkLast(Node&lt;E&gt; l) &#123; // assert l == last &amp;&amp; l != null; final E element = l.item; final Node&lt;E&gt; prev = l.prev; l.item = null; l.prev = null; // help GC last = prev; if (prev == null) first = null; else prev.next = null; size--; modCount++; return element; &#125; remove（int index）方法先是检查移除的位置是否在链表长度的范围内，如果不在则抛出异常，根据索引index获取需要移除的节点，将移除的节点置空，让其上一个节点和下一个节点对接起来。1234public E remove(int index) &#123; checkElementIndex(index); return unlink(node(index)); &#125; set方法检查设置元素位然后置是否越界，如果没有，则索引到index位置的节点，将index位置的节点内容替换成新的内容element，同时返回旧值。1234567public E set(int index, E element) &#123; checkElementIndex(index); Node&lt;E&gt; x = node(index); E oldVal = x.item; x.item = element; return oldVal; &#125; clear方法将所有链表元素置空，然后将链表长度修改成0，修改次数加112345678910111213141516public void clear() &#123; // Clearing all of the links between nodes is "unnecessary", but: // - helps a generational GC if the discarded nodes inhabit // more than one generation // - is sure to free memory even if there is a reachable Iterator for (Node&lt;E&gt; x = first; x != null; ) &#123; Node&lt;E&gt; next = x.next; x.item = null; x.next = null; x.prev = null; x = next; &#125; first = last = null; size = 0; modCount++; &#125; push和pop方法push其实就是调用addFirst(e)方法，pop调用的就是removeFirst()方法。 toArray方法创建一个Object的数组对象，然后将所有的节点都添加到Object对象中，返回Object数组对象。1234567public Object[] toArray() &#123; Object[] result = new Object[size]; int i = 0; for (Node&lt;E&gt; x = first; x != null; x = x.next) result[i++] = x.item; return result; &#125; listIterator方法这个方法返回的是一个内部类ListIterator，用户可以使用这个内部类变量遍历当前的链表元素，但是由于LinkedList也是非线程安全的类，所以和上一篇文章中的java数据结构-arraylist Iterator一样，多线程下面使用，也可能会产生多线程修改的异常。1234public ListIterator&lt;E&gt; listIterator(int index) &#123; checkPositionIndex(index); return new ListItr(index); &#125; 总结用关于Java集合的小抄里面的关于linkedlist作为总结吧。 以双向链表实现。链表无容量限制，但双向链表本身使用了更多空间，每插入一个元素都要构造一个额外的Node对象，也需要额外的链表指针操作。 按下标访问元素－get（i）、set（i,e） 要悲剧的部分遍历链表将指针移动到位 （如果i&gt;数组大小的一半，会从末尾移起）。 插入、删除元素时修改前后节点的指针即可，不再需要复制移动。但还是要部分遍历链表的指针才能移动到下标所指的位置。 只有在链表两头的操作－add（）、addFirst（）、removeLast（）或用iterator（）上的remove（）倒能省掉指针的移动。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>数据结构</tag>
        <tag>linkedlist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java数据结构-arraylist]]></title>
    <url>%2F2017%2F07%2F16%2Fjava-arraylist%2F</url>
    <content type="text"><![CDATA[概述关于Java集合的小抄中是这样描述的： 以数组实现。节约空间，但数组有容量限制。超出限制时会增加50%容量，用System.arraycopy()复制到新的数组，因此最好能给出数组大小的预估值。默认第一次插入元素时创建大小为10的数组。 按数组下标访问元素—get(i)/set(i,e) 的性能很高，这是数组的基本优势。 直接在数组末尾加入元素—add(e)的性能也高，但如果按下标插入、删除元素—add(i,e), remove(i), remove(e)，则要用System.arraycopy()来移动部分受影响的元素，性能就变差了，这是基本劣势。 ArrayList是一个相对来说比较简单的数据结构，最重要的一点就是它的自动扩容，可以认为就是我们常说的“动态数组”。 来看一段简单的代码：12345ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add("语文: 99");list.add("数学: 98");list.add("英语: 100");list.remove(0); 在执行这四条语句时，是这么变化的：接下来看看几个常用的方法的源码分析。 ArrayList属性ArrayList属性主要就是当前数组长度size，以及存放数组的对象elementData数组，除此之外还有一个经常用到的属性就是从AbstractList继承过来的modCount属性，代表ArrayList集合的修改次数。123456789101112131415161718public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, Serializable &#123; // 序列化id private static final long serialVersionUID = 8683452581122892189L; // 默认初始的容量 private static final int DEFAULT_CAPACITY = 10; // 一个空对象 private static final Object[] EMPTY_ELEMENTDATA = new Object[0]; // 一个空对象，如果使用默认构造函数创建，则默认对象内容默认是该值 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = new Object[0]; // 当前数据对象存放地方，当前对象不参与序列化 transient Object[] elementData; // 当前数组长度 private int size; // 数组最大长度 private static final int MAX_ARRAY_SIZE = 2147483639; // 省略方法。。 &#125; 构造函数：ArrayList提供了三种方式的构造器，可以构造一个空列表、构造一个指定初始容量的空列表以及构造一个包含指定collection的元素的列表，这些元素按照该collection的迭代器返回它们的顺序排列的。12345678910111213141516171819202122232425262728293031// ArrayList带容量大小的构造函数。 public ArrayList(int initialCapacity) &#123; //创建指定大小的数组 if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; //创建空数组 &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125;&#125; //默认构造函数创建，创建一个空数组public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; // 创建一个包含collection的ArrayList public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; &#125;&#125; add方法当我们在ArrayList中增加元素的时候，会使用add函数。他会将元素放到末尾。具体实现如下：1234567public boolean add(E e) &#123; //插入时先看看能不能扩容 ensureCapacityInternal(size + 1); //放到最后的元素的后面 elementData[size++] = e; return true;&#125; 我们可以看到他的实现其实最核心的内容就是ensureCapacityInternal。这个函数其实就是自动扩容机制的核心。我们依次来看一下他的具体实现12345678910111213141516171819202122232425262728293031private void ensureCapacityInternal(int minCapacity) &#123; //这里用来判断是不是空数组 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; //如果是空数组，则在插入时默认扩容成DEFAULT_CAPACITY=10 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity);&#125; private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // 超出数组容量了，扩容之 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125; private void grow(int minCapacity) &#123; // 旧容量 int oldCapacity = elementData.length; // 扩展为原来的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //如果扩容1.5倍，还不够，就用传进来的参数。 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 也就是说，当增加数据的时候，如果ArrayList的大小已经不满足需求时，那么就将数组变为原长度的1.5倍，之后的操作就是把老的数组拷到新的数组里面。例如，默认的数组大小是10，也就是说当我们add10个元素之后，再进行一次add时，就会发生自动扩容，数组长度由10变为了15具体情况如下所示： set和get方法Array的set和get函数就比较简单了，先做index检查，然后执行赋值或访问操作：12345678910111213public E set(int index, E element) &#123; rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; public E get(int index) &#123; rangeCheck(index); return elementData(index);&#125; 索引检查12345private void rangeCheck(int index) &#123; //如果索引大于arraylist元素大小，直接爆异常 if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; contains方法调用indexOf方法，遍历数组中的每一个元素作对比，如果找到对于的元素，则返回true，没有找到则返回false。123public boolean contains(Object o) &#123; return indexOf(o) &gt;= 0; &#125; 123456789101112public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1; &#125; remove方法根据索引remove 判断索引有没有越界 自增修改次数 将指定位置（index）上的元素保存到oldValue 将指定位置（index）上的元素都往前移动一位 将最后面的一个元素置空，好让垃圾回收器回收 将原来的值oldValue返回 1234567891011121314public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // 把最后的置null，好让gc回收 return oldValue; &#125; 注意：调用这个方法不会缩减数组的长度，只是将最后一个数组元素置空而已。 根据对象remove循环遍历所有对象，得到对象所在索引位置，然后调用fastRemove方法，执行remove操作12345678910111213141516public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; 定位到需要remove的元素索引，先将index后面的元素往前面移动一位（调用System.arraycooy实现），然后将最后一个元素置空。12345678private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work &#125; clear方法添加操作次数（modCount），将数组内的元素都置空，等待垃圾收集器收集，不减小数组容量。123456789public void clear() &#123; modCount++; // clear to let GC do its work for (int i = 0; i &lt; size; i++) elementData[i] = null; size = 0; &#125; sublist方法我们看到代码中是创建了一个ArrayList 类里面的一个内部类SubList对象，传入的值中第一个参数时this参数，其实可以理解为返回当前list的部分视图，真实指向的存放数据内容的地方还是同一个地方，如果修改了sublist返回的内容的话，那么原来的list也会变动。1234public List&lt;E&gt; subList(int arg0, int arg1) &#123; subListRangeCheck(arg0, arg1, this.size); return new ArrayList.SubList(this, 0, arg0, arg1); &#125; trimToSize方法 修改次数加1 将elementData中空余的空间（包括null值）去除，例如：数组长度为10，其中只有前三个元素有值，其他为空，那么调用该方法之后，数组的长度变为3. 12345678public void trimToSize() &#123; modCount++; if (size &lt; elementData.length) &#123; elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125; &#125; iterator方法interator方法返回的是一个内部类，由于内部类的创建默认含有外部的this指针，所以这个内部类可以调用到外部类的属性。123public Iterator&lt;E&gt; iterator() &#123; return new Itr(); &#125; 一般的话，调用完iterator之后，我们会使用iterator做遍历，这里使用next做遍历的时候有个需要注意的地方，就是调用next的时候，可能会引发ConcurrentModificationException，当修改次数，与期望的修改次数（调用iterator方法时候的修改次数）不一致的时候，会发生该异常，详细我们看一下代码实现：123456789101112@SuppressWarnings("unchecked") public E next() &#123; checkForComodification(); int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; &#125; expectedModCount这个值是在用户调用ArrayList的iterator方法时候确定的，但是在这之后用户add，或者remove了ArrayList的元素，那么modCount就会改变，那么这个值就会不相等，将会引发ConcurrentModificationException异常，这个是在多线程使用情况下，比较常见的一个异常。1234final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125; 数组拷贝arraylist插入或删除都会涉及到数组的拷贝。 System.arraycopy 方法这个方法是个native方法。 参数 说明 src 原数组 srcPos 原数组 dest 目标数组 destPos 目标数组的起始位置 length 要复制的数组元素的数目 Arrays.copyOf方法original - 要复制的数组newLength - 要返回的副本的长度newType - 要返回的副本的类型 其实Arrays.copyOf底层也是调用System.arraycopy实现的源码如下：1234567//基本数据类型（其他类似byte，short···） public static int[] copyOf(int[] original, int newLength) &#123; int[] copy = new int[newLength]; System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy; &#125; 总结 ArrayList基于数组实现，可以通过下标索引直接查找到指定位置的元素，因此查找效率高，但每次插入或删除元素，就要大量地移动元素，插入删除元素的效率低。 从上面add的方法中并未对null做判断，所以ArrayList中允许元素为null。 注意其三个不同的构造方法。无参构造方法构造的ArrayList的容量默认为空（jdk8）在后面添加元素时才会扩容为10。带有Collection参数的构造方法，将Collection转化为数组赋给ArrayList的实现数组elementData。 注意扩充容量的方法ensureCapacityInternal。ArrayList在每次增加元素（可能是1个，也可能是一组）时，都要调用该方法来确保足够的容量。当容量不足以容纳当前的元素个数时，就设置新的容量为旧的容量的1.5倍，而后用Arrays.copyof()方法将元素拷贝到新的数组。从中可以看出，当容量不够时，每次增加元素，都要将原来的元素拷贝到一个新的数组中，非常之耗时，也因此建议在事先能确定元素数量的情况下，才使用ArrayList，否则建议使用LinkedList。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>arraylist</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java数据结构-hashset]]></title>
    <url>%2F2017%2F07%2F15%2Fjava-hashset%2F</url>
    <content type="text"><![CDATA[HashSet概述对于HashSet而言，它是基于HashMap实现的，HashSet底层使用HashMap来保存所有元素，因此HashSet 的实现比较简单，相关HashSet的操作，基本上都是直接调用底层HashMap的相关方法来完成，其实看过了HashMap源码的同学，就会发现HashSet的实现好简单。 HashSet特点非线程安全允许null值添加值得时候会先获取对象的hashCode方法，如果hashCode 方法返回的值一致，则再调用equals方法判断是否一致，如果不一致才add元素。注意： 对于HashSet中保存的对象，请注意正确重写其equals和hashCode方法，以保证放入的对象的唯一性。 HashSet源码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169public class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123; static final long serialVersionUID = -5024744406713321676L; // 底层使用HashMap来保存HashSet中所有元素。 private transient HashMap&lt;E,Object&gt; map; // 定义一个虚拟的Object对象作为HashMap的value，将此对象定义为static final。 private static final Object PRESENT = new Object(); /** * 默认的无参构造器，构造一个空的HashSet。 * * 实际底层会初始化一个空的HashMap，并使用默认初始容量为16和加载因子0.75。 */ public HashSet() &#123; map = new HashMap&lt;E,Object&gt;(); &#125; /** * 构造一个包含指定collection中的元素的新set。 * * 实际底层使用默认的加载因子0.75和足以包含指定 * collection中所有元素的初始容量来创建一个HashMap。 * @param c 其中的元素将存放在此set中的collection。 */ public HashSet(Collection&lt;? extends E&gt; c) &#123; map = new HashMap&lt;E,Object&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c); &#125; /** * 以指定的initialCapacity和loadFactor构造一个空的HashSet。 * * 实际底层以相应的参数构造一个空的HashMap。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 */ public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;E,Object&gt;(initialCapacity, loadFactor); &#125; /** * 以指定的initialCapacity构造一个空的HashSet。 * * 实际底层以相应的参数及加载因子loadFactor为0.75构造一个空的HashMap。 * @param initialCapacity 初始容量。 */ public HashSet(int initialCapacity) &#123; map = new HashMap&lt;E,Object&gt;(initialCapacity); &#125; /** * 以指定的initialCapacity和loadFactor构造一个新的空链接哈希集合。 * 此构造函数为包访问权限，不对外公开，实际只是是对LinkedHashSet的支持。 * * 实际底层会以指定的参数构造一个空LinkedHashMap实例来实现。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 * @param dummy 标记。 */ HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;E,Object&gt;(initialCapacity, loadFactor); &#125; /** * 返回对此set中元素进行迭代的迭代器。返回元素的顺序并不是特定的。 * * 底层实际调用底层HashMap的keySet来返回所有的key。 * 可见HashSet中的元素，只是存放在了底层HashMap的key上， * value使用一个static final的Object对象标识。 * @return 对此set中元素进行迭代的Iterator。 */ public Iterator&lt;E&gt; iterator() &#123; return map.keySet().iterator(); &#125; /** * 返回此set中的元素的数量（set的容量）。 * * 底层实际调用HashMap的size()方法返回Entry的数量，就得到该Set中元素的个数。 * @return 此set中的元素的数量（set的容量）。 */ public int size() &#123; return map.size(); &#125; /** * 如果此set不包含任何元素，则返回true。 * * 底层实际调用HashMap的isEmpty()判断该HashSet是否为空。 * @return 如果此set不包含任何元素，则返回true。 */ public boolean isEmpty() &#123; return map.isEmpty(); &#125; /** * 如果此set包含指定元素，则返回true。 * 更确切地讲，当且仅当此set包含一个满足(o==null ? e==null : o.equals(e)) * 的e元素时，返回true。 * * 底层实际调用HashMap的containsKey判断是否包含指定key。 * @param o 在此set中的存在已得到测试的元素。 * @return 如果此set包含指定元素，则返回true。 */ public boolean contains(Object o) &#123; return map.containsKey(o); &#125; /** * 如果此set中尚未包含指定元素，则添加指定元素。 * 更确切地讲，如果此 set 没有包含满足(e==null ? e2==null : e.equals(e2)) * 的元素e2，则向此set 添加指定的元素e。 * 如果此set已包含该元素，则该调用不更改set并返回false。 * * 底层实际将将该元素作为key放入HashMap。 * 由于HashMap的put()方法添加key-value对时，当新放入HashMap的Entry中key * 与集合中原有Entry的key相同（hashCode()返回值相等，通过equals比较也返回true）， * 新添加的Entry的value会将覆盖原来Entry的value，但key不会有任何改变， * 因此如果向HashSet中添加一个已经存在的元素时，新添加的集合元素将不会被放入HashMap中， * 原来的元素也不会有任何改变，这也就满足了Set中元素不重复的特性。 * @param e 将添加到此set中的元素。 * @return 如果此set尚未包含指定元素，则返回true。 */ public boolean add(E e) &#123; return map.put(e, PRESENT)==null; &#125; /** * 如果指定元素存在于此set中，则将其移除。 * 更确切地讲，如果此set包含一个满足(o==null ? e==null : o.equals(e))的元素e， * 则将其移除。如果此set已包含该元素，则返回true * （或者：如果此set因调用而发生更改，则返回true）。（一旦调用返回，则此set不再包含该元素）。 * * 底层实际调用HashMap的remove方法删除指定Entry。 * @param o 如果存在于此set中则需要将其移除的对象。 * @return 如果set包含指定元素，则返回true。 */ public boolean remove(Object o) &#123; return map.remove(o)==PRESENT; &#125; /** * 从此set中移除所有元素。此调用返回后，该set将为空。 * * 底层实际调用HashMap的clear方法清空Entry中所有元素。 */ public void clear() &#123; map.clear(); &#125; /** * 返回此HashSet实例的浅表副本：并没有复制这些元素本身。 * * 底层实际调用HashMap的clone()方法，获取HashMap的浅表副本，并设置到 HashSet中。 */ public Object clone() &#123; try &#123; HashSet&lt;E&gt; newSet = (HashSet&lt;E&gt;) super.clone(); newSet.map = (HashMap&lt;E, Object&gt;) map.clone(); return newSet; &#125; catch (CloneNotSupportedException e) &#123; throw new InternalError(); &#125; &#125; &#125; 总结基本上hashset就是hashmap的一个包装类，基本所有方法都是包装了hashmap的方法。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>数据结构</tag>
        <tag>hashset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java数据结构-hashmap]]></title>
    <url>%2F2017%2F07%2F15%2Fjava-hashmap%2F</url>
    <content type="text"><![CDATA[摘要HashMap是Java程序员使用频率最高的用于映射(键值对)处理的数据类型。随着JDK（Java Developmet Kit）版本的更新，JDK1.8对HashMap底层的实现进行了优化，例如引入红黑树的数据结构和扩容的优化等。本文结合JDK1.7和JDK1.8的区别，深入探讨HashMap的结构实现和功能原理。 简介Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示：下面针对各个实现类的特点做一些说明： HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 内部实现搞清楚HashMap，首先需要知道HashMap是什么，即它的存储结构-字段；其次弄明白它能干什么，即它的功能实现-方法。下面我们针对这两个方面详细展开讲解。 存储结构-字段从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。这里需要讲明白两个问题：数据底层具体存储的是什么？这样的存储方式有什么优点呢？(1) 从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。我们来看Node[JDK1.8]是何物。1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 (2) HashMap就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题，Java中HashMap采用了链地址法。链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。例如程序执行下面代码：1map.put("美团","小美"); 系统将调用”美团”这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法和扩容机制。在理解Hash和扩容流程之前，我们得先了解下HashMap的几个字段。从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下：1234int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考http://blog.csdn.net/liuqiyao_01/article/details/14475159 ，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。本文不再对红黑树展开讨论，想了解更多红黑树数据结构的工作原理可以参考 http://blog.csdn.net/v_july_v/article/details/6105630。 功能实现-方法HashMap的内部功能实现很多，本文主要从根据key获取哈希桶数组索引位置、put方法的详细执行、扩容过程三个具有代表性的点深入展开讲解。 确定哈希桶数组索引位置不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。先看看源码的实现(方法一+方法二):1234567891011//方法一：static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;//方法二：static int indexFor(int h, int length) &#123; //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h &amp; (length-1); //第三步 取模运算&#125; 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 下面举例说明下，n为table的长度。 分析HashMap的put方法HashMap的put方法执行过程可以通过下图来理解，自己有兴趣可以去对比源码更清楚地研究学习。 判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； 根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向6，如果table[i]不为空，转向3； 判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向4，这里的相同指的是hashCode以及equals； 判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向5； 遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； 插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 JDK1.8HashMap的put方法源码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 扩容机制扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。 我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。12345678910111213void resize(int newCapacity) &#123; //传入新的容量 Entry[] oldTable = table; //引用扩容前的Entry数组 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; //扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return; &#125; Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 transfer(newTable); //！！将数据转移到新的Entry数组里 table = newTable; //HashMap的table属性引用新的Entry数组 threshold = (int)(newCapacity * loadFactor);//修改阈值 &#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。1234567891011121314151617void transfer(Entry[] newTable) &#123; Entry[] src = table; //src引用了旧的Entry数组 int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 if (e != null) &#123; src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1] newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 &#125; while (e != null); &#125; &#125; &#125; newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化：因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图：这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。有兴趣的同学可以研究下JDK1.8的resize源码，写的很赞，如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes"，"unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 链表优化重hash的代码块 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 线程安全性在多线程使用场景中，应该尽量避免使用线程不安全的HashMap，而使用线程安全的ConcurrentHashMap。那么为什么说HashMap是线程不安全的，下面举例子说明在并发的多线程使用场景中使用HashMap可能造成死循环。代码例子如下(便于理解，仍然使用JDK1.7的环境)：1234567891011121314151617181920public class HashMapInfiniteLoop &#123; private static HashMap&lt;Integer,String&gt; map = new HashMap&lt;Integer,String&gt;(2，0.75f); public static void main(String[] args) &#123; map.put(5， "C"); new Thread("Thread1") &#123; public void run() &#123; map.put(7, "B"); System.out.println(map); &#125;; &#125;.start(); new Thread("Thread2") &#123; public void run() &#123; map.put(3, "A"); System.out.println(map); &#125;; &#125;.start(); &#125; &#125; 其中，map初始化为一个长度为2的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当put第二个key的时候，map就需要进行resize。 通过设置断点让线程1和线程2同时debug到transfer方法(3.4小节代码块)的首行。注意此时两个线程已经成功添加数据。放开thread1的断点至transfer方法的“Entry next = e.next;” 这一行；然后放开线程2的的断点，让线程2进行resize。结果如下图。 注意，Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。 线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是e = next，导致了e指向了key(7)，而下一次循环的next = e.next导致了next指向了key(3)。e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。于是，当我们用线程一调用map.get(11)时，悲剧就出现了——Infinite Loop。 JDK1.8与JDK1.7的性能对比HashMap中，如果key经过hash算法得出的数组索引位置全部不相同，即Hash算法非常好，那样的话，getKey方法的时间复杂度就是O(1)，如果Hash算法技术的结果碰撞非常多，假如Hash算极其差，所有的Hash算法结果得出的索引位置一样，那样所有的键值对都集中到一个桶中，或者在一个链表中，或者在一个红黑树中，时间复杂度分别为O(n)和O(lgn)。 鉴于JDK1.8做了多方面的优化，总体性能优于JDK1.7，下面我们从两个方面用例子证明这一点。 Hash较均匀的情况为了便于测试，我们先写一个类Key，如下：123456789101112131415161718192021222324252627class Key implements Comparable&lt;Key&gt; &#123; private final int value; Key(int value) &#123; this.value = value; &#125; @Override public int compareTo(Key o) &#123; return Integer.compare(this.value, o.value); &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Key key = (Key) o; return value == key.value; &#125; @Override public int hashCode() &#123; return value; &#125;&#125; 这个类复写了equals方法，并且提供了相当好的hashCode函数，任何一个值的hashCode都不会相同，因为直接使用value当做hashcode。为了避免频繁的GC，我将不变的Key实例缓存了起来，而不是一遍一遍的创建它们。代码如下：123456789101112131415public class Keys &#123; public static final int MAX_KEY = 10_000_000; private static final Key[] KEYS_CACHE = new Key[MAX_KEY]; static &#123; for (int i = 0; i &lt; MAX_KEY; ++i) &#123; KEYS_CACHE[i] = new Key(i); &#125; &#125; public static Key of(int value) &#123; return KEYS_CACHE[value]; &#125;&#125; 现在开始我们的试验，测试需要做的仅仅是，创建不同size的HashMap（1、10、100、……10000000），屏蔽了扩容的情况，代码如下：1234567891011121314151617181920static void test(int mapSize) &#123; HashMap&lt;Key, Integer&gt; map = new HashMap&lt;Key,Integer&gt;(mapSize); for (int i = 0; i &lt; mapSize; ++i) &#123; map.put(Keys.of(i), i); &#125; long beginTime = System.nanoTime(); //获取纳秒 for (int i = 0; i &lt; mapSize; i++) &#123; map.get(Keys.of(i)); &#125; long endTime = System.nanoTime(); System.out.println(endTime - beginTime); &#125; public static void main(String[] args) &#123; for(int i=10;i&lt;= 1000 0000;i*= 10)&#123; test(i); &#125; &#125; 在测试中会查找不同的值，然后度量花费的时间，为了计算getKey的平均时间，我们遍历所有的get方法，计算总的时间，除以key的数量，计算一个平均值，主要用来比较，绝对值可能会受很多环境因素的影响。结果如下：通过观测测试结果可知，JDK1.8的性能要高于JDK1.7 15%以上，在某些size的区域上，甚至高于100%。由于Hash算法较均匀，JDK1.8引入的红黑树效果不明显，下面我们看看Hash不均匀的的情况。 Hash极不均匀的情况假设我们又一个非常差的Key，它们所有的实例都返回相同的hashCode值。这是使用HashMap最坏的情况。代码修改如下：123456789class Key implements Comparable&lt;Key&gt; &#123; //... @Override public int hashCode() &#123; return 1; &#125;&#125; 仍然执行main方法，得出的结果如下表所示： 从表中结果中可知，随着size的变大，JDK1.7的花费时间是增长的趋势，而JDK1.8是明显的降低趋势，并且呈现对数增长稳定。当一个链表太长的时候，HashMap会动态的将它替换成一个红黑树，会将时间复杂度从O(n)降为O(logn)。hash算法均匀和不均匀所花费的时间明显也不相同，这两种情况的相对比较，可以说明一个好的hash算法的重要性。 测试环境：处理器为2.2 GHz Intel Core i7，内存为16 GB 1600 MHz DDR3，SSD硬盘，使用默认的JVM参数，运行在64位的OS X 10.10.1上。 小结 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 HashMap是线程不安全的，不要在并发的环境中同时操作HashMap，建议使用ConcurrentHashMap。 JDK1.8引入红黑树大程度优化了HashMap的性能。 还没升级JDK1.8的，现在开始升级吧。HashMap的性能提升仅仅是JDK1.8的冰山一角。 参考 https://tech.meituan.com/java-hashmap.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>数据结构</tag>
        <tag>hashmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO浅析]]></title>
    <url>%2F2017%2F05%2F14%2Fjava-nio%2F</url>
    <content type="text"><![CDATA[NIO（Non-blocking I/O，在Java领域，也称为New I/O），是一种同步非阻塞的I/O模型，也是I/O多路复用的基础，已经被越来越多地应用到大型应用服务器，成为解决高并发与大量连接、I/O处理问题的有效方式。 那么NIO的本质是什么样的呢？它是怎样与事件模型结合来解放线程、提高系统吞吐的呢？ 本文会从传统的阻塞I/O和线程池模型面临的问题讲起，然后对比几种常见I/O模型，一步步分析NIO怎么利用事件模型处理I/O，解决线程池瓶颈处理海量连接，包括利用面向事件的方式编写服务端/客户端程序。最后延展到一些高级主题，如Reactor与Proactor模型的对比、Selector的唤醒、Buffer的选择等。 注：本文的代码都是伪代码，主要是为了示意，不可用于生产环境。 传统BIO模型分析让我们先回忆一下传统的服务器端同步阻塞I/O处理（也就是BIO，Blocking I/O）的经典编程模型：1234567891011121314151617181920212223242526&#123; ExecutorService executor = Excutors.newFixedThreadPollExecutor(100);//线程池 ServerSocket serverSocket = new ServerSocket(); serverSocket.bind(8088); while(!Thread.currentThread.isInturrupted())&#123;//主线程死循环等待新连接到来 Socket socket = serverSocket.accept(); executor.submit(new ConnectIOnHandler(socket));//为新的连接创建新的线程&#125;class ConnectIOnHandler extends Thread&#123; private Socket socket; public ConnectIOnHandler(Socket socket)&#123; this.socket = socket; &#125; public void run()&#123; while(!Thread.currentThread.isInturrupted()&amp;&amp;!socket.isClosed())&#123;死循环处理读写事件 String someThing = socket.read()....//读取数据 if(someThing!=null)&#123; ......//处理数据 socket.write()....//写数据 &#125; &#125; &#125;&#125; 这是一个经典的每连接每线程的模型，之所以使用多线程，主要原因在于socket.accept()、socket.read()、socket.write()三个主要函数都是同步阻塞的，当一个连接在处理I/O的时候，系统是阻塞的，如果是单线程的话必然就挂死在那里；但CPU是被释放出来的，开启多线程，就可以让CPU去处理更多的事情。其实这也是所有使用多线程的本质： 利用多核。 当I/O阻塞系统，但CPU空闲的时候，可以利用多线程使用CPU资源。 现在的多线程一般都使用线程池，可以让线程的创建和回收成本相对较低。在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的I/O并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。 不过，这个模型最本质的问题在于，严重依赖于线程。但线程是很”贵”的资源，主要表现在： 线程的创建和销毁成本很高，在Linux这样的操作系统中，线程本质上就是一个进程。创建和销毁都是重量级的系统函数。 线程本身占用较大内存，像Java的线程栈，一般至少分配512K～1M的空间，如果系统中的线程数过千，恐怕整个JVM的内存都会被吃掉一半。 线程的切换成本是很高的。操作系统发生线程切换的时候，需要保留线程的上下文，然后执行系统调用。如果线程数过高，可能执行线程切换的时间甚至会大于线程执行的时间，这时候带来的表现往往是系统load偏高、CPU sy使用率特别高（超过20%以上)，导致系统几乎陷入不可用的状态。 容易造成锯齿状的系统负载。因为系统负载是用活动线程数或CPU核心数，一旦线程数量高但外部网络环境不是很稳定，就很容易造成大量请求的结果同时返回，激活大量阻塞线程从而使系统负载压力过大。 所以，当面对十万甚至百万级连接的时候，传统的BIO模型是无能为力的。随着移动端应用的兴起和各种网络游戏的盛行，百万级长连接日趋普遍，此时，必然需要一种更高效的I/O处理模型。 NIO是怎么工作的很多刚接触NIO的人，第一眼看到的就是Java相对晦涩的API，比如：Channel，Selector，Socket什么的；然后就是一坨上百行的代码来演示NIO的服务端Demo……瞬间头大有没有？ 我们不管这些，抛开现象看本质，先分析下NIO是怎么工作的。 常见I/O模型对比所有的系统I/O都分为两个阶段：等待就绪和操作。举例来说，读函数，分为等待系统可读和真正的读；同理，写函数分为等待网卡可以写和真正的写。 需要说明的是等待就绪的阻塞是不使用CPU的，是在“空等”；而真正的读写操作的阻塞是使用CPU的，真正在”干活”，而且这个过程非常快，属于memory copy，带宽通常在1GB/s级别以上，可以理解为基本不耗时。 下图是几种常见I/O模型的对比： 以socket.read()为例子： 传统的BIO里面socket.read()，如果TCP RecvBuffer里没有数据，函数会一直阻塞，直到收到数据，返回读到的数据。 对于NIO，如果TCP RecvBuffer有数据，就把数据从网卡读到内存，并且返回给用户；反之则直接返回0，永远不会阻塞。 最新的AIO(Async I/O)里面会更进一步：不但等待就绪是非阻塞的，就连数据从网卡到内存的过程也是异步的。 换句话说，BIO里用户最关心“我要读”，NIO里用户最关心”我可以读了”，在AIO模型里用户更需要关注的是“读完了”。 NIO一个重要的特点是：socket主要的读、写、注册和接收函数，在等待就绪阶段都是非阻塞的，真正的I/O操作是同步阻塞的（消耗CPU但性能非常高）。 如何结合事件模型使用NIO同步非阻塞特性回忆BIO模型，之所以需要多线程，是因为在进行I/O操作的时候，一是没有办法知道到底能不能写、能不能读，只能”傻等”，即使通过各种估算，算出来操作系统没有能力进行读写，也没法在socket.read()和socket.write()函数中返回，这两个函数无法进行有效的中断。所以除了多开线程另起炉灶，没有好的办法利用CPU。 NIO的读写函数可以立刻返回，这就给了我们不开线程利用CPU的最好机会：如果一个连接不能读写（socket.read()返回0或者socket.write()返回0），我们可以把这件事记下来，记录的方式通常是在Selector上注册标记位，然后切换到其它就绪的连接（channel）继续进行读写。 下面具体看下如何利用事件模型单线程处理所有I/O请求： NIO的主要事件有几个：读就绪、写就绪、有新连接到来。 我们首先需要注册当这几个事件到来的时候所对应的处理器。然后在合适的时机告诉事件选择器：我对这个事件感兴趣。对于写操作，就是写不出去的时候对写事件感兴趣；对于读操作，就是完成连接和系统没有办法承载新读入的数据的时；对于accept，一般是服务器刚启动的时候；而对于connect，一般是connect失败需要重连或者直接异步调用connect的时候。 其次，用一个死循环选择就绪的事件，会执行系统调用（Linux 2.6之前是select、poll，2.6之后是epoll，Windows是IOCP），还会阻塞的等待新事件的到来。新事件到来的时候，会在selector上注册标记位，标示可读、可写或者有连接到来。 注意，select是阻塞的，无论是通过操作系统的通知（epoll）还是不停的轮询(select，poll)，这个函数是阻塞的。所以你可以放心大胆地在一个while(true)里面调用这个函数而不用担心CPU空转。 所以我们的程序大概的模样是：123456789101112131415161718192021222324252627interface ChannelHandler&#123; void channelReadable(Channel channel); void channelWritable(Channel channel); &#125; class Channel&#123; Socket socket; Event event;//读，写或者连接 &#125; //IO线程主循环: class IoThread extends Thread&#123; public void run()&#123; Channel channel; while(channel=Selector.select())&#123;//选择就绪的事件和对应的连接 if(channel.event==accept)&#123; registerNewChannelHandler(channel);//如果是新连接，则注册一个新的读写处理器 &#125; if(channel.event==write)&#123; getChannelHandler(channel).channelWritable(channel);//如果可以写，则执行写事件 &#125; if(channel.event==read)&#123; getChannelHandler(channel).channelReadable(channel);//如果可以读，则执行读事件 &#125; &#125; &#125; Map&lt;Channel，ChannelHandler&gt; handlerMap;//所有channel的对应事件处理器 &#125; 这个程序很简短，也是最简单的Reactor模式：注册所有感兴趣的事件处理器，单线程轮询选择就绪事件，执行事件处理器。 优化线程模型由上面的示例我们大概可以总结出NIO是怎么解决掉线程的瓶颈并处理海量连接的： NIO由原来的阻塞读写（占用线程）变成了单线程轮询事件，找到可以进行读写的网络描述符进行读写。除了事件的轮询是阻塞的（没有可干的事情必须要阻塞），剩余的I/O操作都是纯CPU操作，没有必要开启多线程。 并且由于线程的节约，连接数大的时候因为线程切换带来的问题也随之解决，进而为处理海量连接提供了可能。 单线程处理I/O的效率确实非常高，没有线程切换，只是拼命的读、写、选择事件。但现在的服务器，一般都是多核处理器，如果能够利用多核心进行I/O，无疑对效率会有更大的提高。 仔细分析一下我们需要的线程，其实主要包括以下几种： 事件分发器，单线程选择就绪的事件。 I/O处理器，包括connect、read、write等，这种纯CPU操作，一般开启CPU核心个线程就可以。 业务线程，在处理完I/O后，业务一般还会有自己的业务逻辑，有的还会有其他的阻塞I/O，如DB操作，RPC等。只要有阻塞，就需要单独的线程。 Java的Selector对于Linux系统来说，有一个致命限制：同一个channel的select不能被并发的调用。因此，如果有多个I/O线程，必须保证：一个socket只能属于一个IoThread，而一个IoThread可以管理多个socket。 另外连接的处理和读写的处理通常可以选择分开，这样对于海量连接的注册和读写就可以分发。虽然read()和write()是比较高效无阻塞的函数，但毕竟会占用CPU，如果面对更高的并发则无能为力。 NIO在客户端的魔力通过上面的分析，可以看出NIO在服务端对于解放线程，优化I/O和处理海量连接方面，确实有自己的用武之地。那么在客户端上，NIO又有什么使用场景呢? 常见的客户端BIO+连接池模型，可以建立n个连接，然后当某一个连接被I/O占用的时候，可以使用其他连接来提高性能。 但多线程的模型面临和服务端相同的问题：如果指望增加连接数来提高性能，则连接数又受制于线程数、线程很贵、无法建立很多线程，则性能遇到瓶颈。 每连接顺序请求的Redis对于Redis来说，由于服务端是全局串行的，能够保证同一连接的所有请求与返回顺序一致。这样可以使用单线程＋队列，把请求数据缓冲。然后pipeline发送，返回future，然后channel可读时，直接在队列中把future取回来，done()就可以了。 伪代码如下：123456789101112131415161718192021222324252627class RedisClient Implements ChannelHandler&#123; private BlockingQueue CmdQueue; private EventLoop eventLoop; private Channel channel; class Cmd&#123; String cmd; Future result; &#125; public Future get(String key)&#123; Cmd cmd= new Cmd(key); queue.offer(cmd); eventLoop.submit(new Runnable()&#123; List list = new ArrayList(); queue.drainTo(list); if(channel.isWritable())&#123; channel.writeAndFlush(list); &#125; &#125;);&#125; public void ChannelReadFinish(Channel channel，Buffer Buffer)&#123; List result = handleBuffer();//处理数据 //从cmdQueue取出future，并设值，future.done();&#125; public void ChannelWritable(Channel channel)&#123; channel.flush();&#125;&#125; 这样做，能够充分的利用pipeline来提高I/O能力，同时获取异步处理能力。 多连接短连接的HttpClient类似于竞对抓取的项目，往往需要建立无数的HTTP短连接，然后抓取，然后销毁，当需要单机抓取上千网站线程数又受制的时候，怎么保证性能呢? 何不尝试NIO，单线程进行连接、写、读操作？如果连接、读、写操作系统没有能力处理，简单的注册一个事件，等待下次循环就好了。 如何存储不同的请求/响应呢？由于http是无状态没有版本的协议，又没有办法使用队列，好像办法不多。比较笨的办法是对于不同的socket，直接存储socket的引用作为map的key。 常见的RPC框架，如Thrift，Dubbo这种框架内部一般维护了请求的协议和请求号，可以维护一个以请求号为key，结果的result为future的map，结合NIO+长连接，获取非常不错的性能。 NIO高级主题Proactor与Reactor一般情况下，I/O 复用机制需要事件分发器（event dispatcher）。 事件分发器的作用，即将那些读写事件源分发给各读写事件的处理者，就像送快递的在楼下喊: 谁谁谁的快递到了， 快来拿吧！开发人员在开始的时候需要在分发器那里注册感兴趣的事件，并提供相应的处理者（event handler)，或者是回调函数；事件分发器在适当的时候，会将请求的事件分发给这些handler或者回调函数。 涉及到事件分发器的两种模式称为：Reactor和Proactor。 Reactor模式是基于同步I/O的，而Proactor模式是和异步I/O相关的。在Reactor模式中，事件分发器等待某个事件或者可应用或个操作的状态发生（比如文件描述符可读写，或者是socket可读写），事件分发器就把这个事件传给事先注册的事件处理函数或者回调函数，由后者来做实际的读写操作。 而在Proactor模式中，事件处理者（或者代由事件分发器发起）直接发起一个异步读写操作（相当于请求），而实际的工作是由操作系统来完成的。发起时，需要提供的参数包括用于存放读到数据的缓存区、读的数据大小或用于存放外发数据的缓存区，以及这个请求完后的回调函数等信息。事件分发器得知了这个请求，它默默等待这个请求的完成，然后转发完成事件给相应的事件处理者或者回调。举例来说，在Windows上事件处理者投递了一个异步IO操作（称为overlapped技术），事件分发器等IO Complete事件完成。这种异步模式的典型实现是基于操作系统底层异步API的，所以我们可称之为“系统级别”的或者“真正意义上”的异步，因为具体的读写是由操作系统代劳的。 举个例子，将有助于理解Reactor与Proactor二者的差异，以读操作为例（写操作类似）。 在Reactor中实现读 注册读就绪事件和相应的事件处理器。 事件分发器等待事件。 事件到来，激活分发器，分发器调用事件对应的处理器。 事件处理器完成实际的读操作，处理读到的数据，注册新的事件，然后返还控制权。 在Proactor中实现读 处理器发起异步读操作（注意：操作系统必须支持异步IO）。在这种情况下，处理器无视IO就绪事件，它关注的是完成事件。 事件分发器等待操作完成事件。 在分发器等待过程中，操作系统利用并行的内核线程执行实际的读操作，并将结果数据存入用户自定义缓冲区，最后通知事件分发器读操作完成。 事件分发器呼唤处理器。 事件处理器处理用户自定义缓冲区中的数据，然后启动一个新的异步操作，并将控制权返回事件分发器。 可以看出，两个模式的相同点，都是对某个I/O事件的事件通知（即告诉某个模块，这个I/O操作可以进行或已经完成)。在结构上，两者也有相同点：事件分发器负责提交IO操作（异步)、查询设备是否可操作（同步)，然后当条件满足时，就回调handler；不同点在于，异步情况下（Proactor)，当回调handler时，表示I/O操作已经完成；同步情况下（Reactor)，回调handler时，表示I/O设备可以进行某个操作（can read 或 can write)。 下面，我们将尝试应对为Proactor和Reactor模式建立可移植框架的挑战。在改进方案中，我们将Reactor原来位于事件处理器内的Read/Write操作移至分发器（不妨将这个思路称为“模拟异步”），以此寻求将Reactor多路同步I/O转化为模拟异步I/O。以读操作为例子，改进过程如下： 注册读就绪事件和相应的事件处理器。并为分发器提供数据缓冲区地址，需要读取数据量等信息。 分发器等待事件（如在select()上等待）。 事件到来，激活分发器。分发器执行一个非阻塞读操作（它有完成这个操作所需的全部信息），最后调用对应处理器。 事件处理器处理用户自定义缓冲区的数据，注册新的事件（当然同样要给出数据缓冲区地址，需要读取的数据量等信息），最后将控制权返还分发器。 如我们所见，通过对多路I/O模式功能结构的改造，可将Reactor转化为Proactor模式。改造前后，模型实际完成的工作量没有增加，只不过参与者间对工作职责稍加调换。没有工作量的改变，自然不会造成性能的削弱。对如下各步骤的比较，可以证明工作量的恒定： 标准/典型的Reactor 步骤1：等待事件到来（Reactor负责）。 步骤2：将读就绪事件分发给用户定义的处理器（Reactor负责）。 步骤3：读数据（用户处理器负责）。 步骤4：处理数据（用户处理器负责）。 改进实现的模拟Proactor 步骤1：等待事件到来（Proactor负责）。 步骤2：得到读就绪事件，执行读数据（现在由Proactor负责）。 步骤3：将读完成事件分发给用户处理器（Proactor负责）。 步骤4：处理数据（用户处理器负责）。 对于不提供异步I/O API的操作系统来说，这种办法可以隐藏Socket API的交互细节，从而对外暴露一个完整的异步接口。借此，我们就可以进一步构建完全可移植的，平台无关的，有通用对外接口的解决方案。 代码示例如下：123456789101112131415161718192021222324252627282930313233343536interface ChannelHandler&#123; void channelReadComplate(Channel channel，byte[] data); void channelWritable(Channel channel); &#125; class Channel&#123; Socket socket; Event event;//读，写或者连接 &#125; //IO线程主循环： class IoThread extends Thread&#123; public void run()&#123; Channel channel; while(channel=Selector.select())&#123;//选择就绪的事件和对应的连接 if(channel.event==accept)&#123; registerNewChannelHandler(channel);//如果是新连接，则注册一个新的读写处理器 Selector.interested(read); &#125; if(channel.event==write)&#123; getChannelHandler(channel).channelWritable(channel);//如果可以写，则执行写事件 &#125; if(channel.event==read)&#123; byte[] data = channel.read(); if(channel.read()==0)//没有读到数据，表示本次数据读完了 &#123; getChannelHandler(channel).channelReadComplate(channel，data;//处理读完成事件 &#125; if(过载保护)&#123; Selector.interested(read); &#125; &#125; &#125; &#125; Map&lt;Channel，ChannelHandler&gt; handlerMap;//所有channel的对应事件处理器 &#125; Selector.wakeup()主要作用解除阻塞在Selector.select()/select(long)上的线程，立即返回。 两次成功的select之间多次调用wakeup等价于一次调用。 如果当前没有阻塞在select上，则本次wakeup调用将作用于下一次select——“记忆”作用。 为什么要唤醒？ 注册了新的channel或者事件。 channel关闭，取消注册。 优先级更高的事件触发（如定时器事件），希望及时处理。 原理Linux上利用pipe调用创建一个管道，Windows上则是一个loopback的tcp连接。这是因为win32的管道无法加入select的fd set，将管道或者TCP连接加入select fd set。 wakeup往管道或者连接写入一个字节，阻塞的select因为有I/O事件就绪，立即返回。可见，wakeup的调用开销不可忽视。 Buffer的选择通常情况下，操作系统的一次写操作分为两步： 将数据从用户空间拷贝到系统空间。从系统空间往网卡写。同理，读操作也分为两步： 将数据从网卡拷贝到系统空间； 将数据从系统空间拷贝到用户空间。对于NIO来说，缓存的使用可以使用DirectByteBuffer和HeapByteBuffer。如果使用了DirectByteBuffer，一般来说可以减少一次系统空间到用户空间的拷贝。但Buffer创建和销毁的成本更高，更不宜维护，通常会用内存池来提高性能。 如果数据量比较小的中小应用情况下，可以考虑使用heapBuffer；反之可以用directBuffer。 NIO存在的问题使用NIO != 高性能，当连接数&lt;1000，并发程度不高或者局域网环境下NIO并没有显著的性能优势。 NIO并没有完全屏蔽平台差异，它仍然是基于各个操作系统的I/O系统实现的，差异仍然存在。使用NIO做网络编程构建事件驱动模型并不容易，陷阱重重。 推荐大家使用成熟的NIO框架，如Netty，MINA等。解决了很多NIO的陷阱，并屏蔽了操作系统的差异，有较好的性能和编程模型。 总结最后总结一下到底NIO给我们带来了些什么： 事件驱动模型 避免多线程 单线程处理多任务 非阻塞I/O，I/O读写不再阻塞，而是返回0 基于block的传输，通常比基于流的传输更高效 更高级的IO函数，zero-copy IO多路复用大大提高了Java网络应用的可伸缩性和实用性 本文抛砖引玉，诠释了一些NIO的思想和设计理念以及应用场景，这只是从冰山一角。关于NIO可以谈的技术点其实还有很多，期待未来有机会和大家继续探讨。 参考 https://zhuanlan.zhihu.com/p/23488863]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis基础、高级特性与性能调优]]></title>
    <url>%2F2017%2F04%2F09%2Fredis-summary%2F</url>
    <content type="text"><![CDATA[本文将从Redis的基本特性入手，通过讲述Redis的数据结构和主要命令对Redis的基本能力进行直观介绍。之后概览Redis提供的高级能力，并在部署、维护、性能调优等多个方面进行更深入的介绍和指导。本文适合使用Redis的普通开发人员，以及对Redis进行选型、架构设计和性能调优的架构设计人员。 概述Redis是一个开源的，基于内存的结构化数据存储媒介，可以作为数据库、缓存服务或消息服务使用。Redis支持多种数据结构，包括字符串、哈希表、链表、集合、有序集合、位图、Hyperloglogs等。Redis具备LRU淘汰、事务实现、以及不同级别的硬盘持久化等能力，并且支持副本集和通过Redis Sentinel实现的高可用方案，同时还支持通过Redis Cluster实现的数据自动分片能力。 Redis的主要功能都基于单线程模型实现，也就是说Redis使用一个线程来服务所有的客户端请求，同时Redis采用了非阻塞式IO，并精细地优化各种命令的算法时间复杂度，这些信息意味着： Redis是线程安全的（因为只有一个线程），其所有操作都是原子的，不会因并发产生数据异常 Redis的速度非常快（因为使用非阻塞式IO，且大部分命令的算法时间复杂度都是O(1)) 使用高耗时的Redis命令是很危险的，会占用唯一的一个线程的大量处理时间，导致所有的请求都被拖慢。（例如时间复杂度为O(N)的KEYS命令，严格禁止在生产环境中使用） Redis的数据结构和相关常用命令本节中将介绍Redis支持的主要数据结构，以及相关的常用Redis命令。本节只对Redis命令进行扼要的介绍，且只列出了较常用的命令。如果想要了解完整的Redis命令集，或了解某个命令的详细使用方法，请参考官方文档：https://redis.io/commands KeyRedis采用Key-Value型的基本数据结构，任何二进制序列都可以作为Redis的Key使用（例如普通的字符串或一张JPEG图片）关于Key的一些注意事项： 不要使用过长的Key。例如使用一个1024字节的key就不是一个好主意，不仅会消耗更多的内存，还会导致查找的效率降低 Key短到缺失了可读性也是不好的，例如”u1000flw”比起”user:1000:followers”来说，节省了寥寥的存储空间，却引发了可读性和可维护性上的麻烦 最好使用统一的规范来设计Key，比如”object-type:id:attr”，以这一规范设计出的Key可能是”user:1000”或”comment:1234:reply-to” Redis允许的最大Key长度是512MB（对Value的长度限制也是512MB） StringString是Redis的基础数据类型，Redis没有Int、Float、Boolean等数据类型的概念，所有的基本类型在Redis中都以String体现。与String相关的常用命令： SET：为一个key设置value，可以配合EX/PX参数指定key的有效期，通过NX/XX参数针对key是否存在的情况进行区别操作，时间复杂度O(1) GET：获取某个key对应的value，时间复杂度O(1) GETSET：为一个key设置value，并返回该key的原value，时间复杂度O(1) MSET：为多个key设置value，时间复杂度O(N) MSETNX：同MSET，如果指定的key中有任意一个已存在，则不进行任何操作，时间复杂度O(N) MGET：获取多个key对应的value，时间复杂度O(N) 上文提到过，Redis的基本数据类型只有String，但Redis可以把String作为整型或浮点型数字来使用，主要体现在INCR、DECR类的命令上： INCR：将key对应的value值自增1，并返回自增后的值。只对可以转换为整型的String数据起作用。时间复杂度O(1) NCRBY：将key对应的value值自增指定的整型数值，并返回自增后的值。只对可以转换为整型的String数据起作用。时间复杂度O(1) DECR/DECRBY：同INCR/INCRBY，自增改为自减。INCR/DECR系列命令要求操作的value类型为String，并可以转换为64位带符号的整型数字，否则会返回错误。 也就是说，进行INCR/DECR系列命令的value，必须在[-2^63 ~ 2^63 - 1]范围内。提到过，Redis采用单线程模型，天然是线程安全的，这使得INCR/DECR命令可以非常便利的实现高并发场景下的精确控制。 例1：库存控制在高并发场景下实现库存余量的精准校验，确保不出现超卖的情况。设置库存总量：1SET inv:remain "100" 库存扣减+余量校验：1DECR inv:remain 当DECR命令返回值大于等于0时，说明库存余量校验通过，如果返回小于0的值，则说明库存已耗尽。假设同时有300个并发请求进行库存扣减，Redis能够确保这300个请求分别得到99到-200的返回值，每个请求得到的返回值都是唯一的，绝对不会找出现两个请求得到一样的返回值的情况。 例2：自增序列生成实现类似于RDBMS的Sequence功能，生成一系列唯一的序列号设置序列起始值：1SET sequence "10000" 获取一个序列值：1INCR sequence 直接将返回值作为序列使用即可。 获取一批（如100个）序列值：1INCRBY sequence 100 假设返回值为N，那么[N - 99 ~ N]的数值都是可用的序列值。当多个客户端同时向Redis申请自增序列时，Redis能够确保每个客户端得到的序列值或序列范围都是全局唯一的，绝对不会出现不同客户端得到了重复的序列值的情况。 ListRedis的List是链表型的数据结构，可以使用LPUSH/RPUSH/LPOP/RPOP等命令在List的两端执行插入元素和弹出元素的操作。虽然List也支持在特定index上插入和读取元素的功能，但其时间复杂度较高（O(N)），应小心使用。与List相关的常用命令： LPUSH：向指定List的左侧（即头部）插入1个或多个元素，返回插入后的List长度。时间复杂度O(N)，N为插入元素的数量 RPUSH：同LPUSH，向指定List的右侧（即尾部）插入1或多个元素 LPOP：从指定List的左侧（即头部）移除一个元素并返回，时间复杂度O(1) RPOP：同LPOP，从指定List的右侧（即尾部）移除1个元素并返回 LPUSHX/RPUSHX：与LPUSH/RPUSH类似，区别在于，LPUSHX/RPUSHX操作的key如果不存在，则不会进行任何操作 LLEN：返回指定List的长度，时间复杂度O(1) LRANGE：返回指定List中指定范围的元素（双端包含，即LRANGE key 0 10会返回11个元素），时间复杂度O(N)。应尽可能控制一次获取的元素数量，一次获取过大范围的List元素会导致延迟，同时对长度不可预知的List，避免使用LRANGE key 0 -1这样的完整遍历操作。 应谨慎使用的List相关命令： LINDEX：返回指定List指定index上的元素，如果index越界，返回nil。index数值是回环的，即-1代表List最后一个位置，-2代表List倒数第二个位置。时间复杂度O(N) LSET：将指定List指定index上的元素设置为value，如果index越界则返回错误，时间复杂度O(N)，如果操作的是头/尾部的元素，则时间复杂度为O(1) LINSERT：向指定List中指定元素之前/之后插入一个新元素，并返回操作后的List长度。如果指定的元素不存在，返回-1。如果指定key不存在，不会进行任何操作，时间复杂度O(N) 由于Redis的List是链表结构的，上述的三个命令的算法效率较低，需要对List进行遍历，命令的耗时无法预估，在List长度大的情况下耗时会明显增加，应谨慎使用。 换句话说，Redis的List实际是设计来用于实现队列，而不是用于实现类似ArrayList这样的列表的。如果你不是想要实现一个双端出入的队列，那么请尽量不要使用Redis的List数据结构。 为了更好支持队列的特性，Redis还提供了一系列阻塞式的操作命令，如BLPOP/BRPOP等，能够实现类似于BlockingQueue的能力，即在List为空时，阻塞该连接，直到List中有对象可以出队时再返回。针对阻塞类的命令，此处不做详细探讨，请参考官方文档（https://redis.io/topics/data-types-intro） 中”Blocking operations on lists”一节。 HashHash即哈希表，Redis的Hash和传统的哈希表一样，是一种field-value型的数据结构，可以理解成将HashMap搬入Redis。Hash非常适合用于表现对象类型的数据，用Hash中的field对应对象的field即可。Hash的优点包括： 可以实现二元查找，如”查找ID为1000的用户的年龄” 比起将整个对象序列化后作为String存储的方法，Hash能够有效地减少网络传输的消耗 当使用Hash维护一个集合时，提供了比List效率高得多的随机访问命令 与Hash相关的常用命令： HSET：将key对应的Hash中的field设置为value。如果该Hash不存在，会自动创建一个。时间复杂度O(1) HGET：返回指定Hash中field字段的值，时间复杂度O(1) HMSET/HMGET：同HSET和HGET，可以批量操作同一个key下的多个field，时间复杂度：O(N)，N为一次操作的field数量 HSETNX：同HSET，但如field已经存在，HSETNX不会进行任何操作，时间复杂度O(1) HEXISTS：判断指定Hash中field是否存在，存在返回1，不存在返回0，时间复杂度O(1) HDEL：删除指定Hash中的field（1个或多个），时间复杂度：O(N)，N为操作的field数量 HINCRBY：同INCRBY命令，对指定Hash中的一个field进行INCRBY，时间复杂度O(1) 应谨慎使用的Hash相关命令： HGETALL：返回指定Hash中所有的field-value对。返回结果为数组，数组中field和value交替出现。时间复杂度O(N) HKEYS/HVALS：返回指定Hash中所有的field/value，时间复杂度O(N) 上述三个命令都会对Hash进行完整遍历，Hash中的field数量与命令的耗时线性相关，对于尺寸不可预知的Hash，应严格避免使用上面三个命令，而改为使用HSCAN命令进行游标式的遍历，具体请见 https://redis.io/commands/scan SetRedis Set是无序的，不可重复的String集合。与Set相关的常用命令： SADD：向指定Set中添加1个或多个member，如果指定Set不存在，会自动创建一个。时间复杂度O(N)，N为添加的member个数 SREM：从指定Set中移除1个或多个member，时间复杂度O(N)，N为移除的member个数 SRANDMEMBER：从指定Set中随机返回1个或多个member，时间复杂度O(N)，N为返回的member个数 SPOP：从指定Set中随机移除并返回count个member，时间复杂度O(N)，N为移除的member个数 SCARD：返回指定Set中的member个数，时间复杂度O(1) SISMEMBER：判断指定的value是否存在于指定Set中，时间复杂度O(1) SMOVE：将指定member从一个Set移至另一个Set 慎用的Set相关命令： SMEMBERS：返回指定Hash中所有的member，时间复杂度O(N) SUNION/SUNIONSTORE：计算多个Set的并集并返回/存储至另一个Set中，时间复杂度O(N)，N为参与计算的所有集合的总member数 SINTER/SINTERSTORE：计算多个Set的交集并返回/存储至另一个Set中，时间复杂度O(N)，N为参与计算的所有集合的总member数 SDIFF/SDIFFSTORE：计算1个Set与1或多个Set的差集并返回/存储至另一个Set中，时间复杂度O(N)，N为参与计算的所有集合的总member数 上述几个命令涉及的计算量大，应谨慎使用，特别是在参与计算的Set尺寸不可知的情况下，应严格避免使用。可以考虑通过SSCAN命令遍历获取相关Set的全部member（具体请见 https://redis.io/commands/scan ），如果需要做并集/交集/差集计算，可以在客户端进行，或在不服务实时查询请求的Slave上进行。 Sorted SetRedis Sorted Set是有序的、不可重复的String集合。Sorted Set中的每个元素都需要指派一个分数(score)，Sorted Set会根据score对元素进行升序排序。如果多个member拥有相同的score，则以字典序进行升序排序。 Sorted Set非常适合用于实现排名。 Sorted Set的主要命令： ZADD：向指定Sorted Set中添加1个或多个member，时间复杂度O(Mlog(N))，M为添加的member数量，N为Sorted Set中的member数量 ZREM：从指定Sorted Set中删除1个或多个member，时间复杂度O(Mlog(N))，M为删除的member数量，N为Sorted Set中的member数量 ZCOUNT：返回指定Sorted Set中指定score范围内的member数量，时间复杂度：O(log(N)) ZCARD：返回指定Sorted Set中的member数量，时间复杂度O(1) ZSCORE：返回指定Sorted Set中指定member的score，时间复杂度O(1) ZRANK/ZREVRANK：返回指定member在Sorted Set中的排名，ZRANK返回按升序排序的排名，ZREVRANK则返回按降序排序的排名。时间复杂度O(log(N)) ZINCRBY：同INCRBY，对指定Sorted Set中的指定member的score进行自增，时间复杂度O(log(N)) 慎用的Sorted Set相关命令： ZRANGE/ZREVRANGE：返回指定Sorted Set中指定排名范围内的所有member，ZRANGE为按score升序排序，ZREVRANGE为按score降序排序，时间复杂度O(log(N)+M)，M为本次返回的member数 ZRANGEBYSCORE/ZREVRANGEBYSCORE：返回指定Sorted Set中指定score范围内的所有member，返回结果以升序/降序排序，min和max可以指定为-inf和+inf，代表返回所有的member。时间复杂度O(log(N)+M) ZREMRANGEBYRANK/ZREMRANGEBYSCORE：移除Sorted Set中指定排名范围/指定score范围内的所有member。时间复杂度O(log(N)+M) 上述几个命令，应尽量避免传递[0 -1]或[-inf +inf]这样的参数，来对Sorted Set做一次性的完整遍历，特别是在Sorted Set的尺寸不可预知的情况下。可以通过ZSCAN命令来进行游标式的遍历（具体请见 https://redis.io/commands/scan ），或通过LIMIT参数来限制返回member的数量（适用于ZRANGEBYSCORE和ZREVRANGEBYSCORE命令），以实现游标式的遍历。 Bitmap和HyperLogLogRedis的这两种数据结构相较之前的并不常用，在本文中只做简要介绍，如想要详细了解这两种数据结构与其相关的命令，请参考官方文档https://redis.io/topics/data-types-intro 中的相关章节 Bitmap在Redis中不是一种实际的数据类型，而是一种将String作为Bitmap使用的方法。可以理解为将String转换为bit数组。使用Bitmap来存储true/false类型的简单数据极为节省空间。 HyperLogLogs是一种主要用于数量统计的数据结构，它和Set类似，维护一个不可重复的String集合，但是HyperLogLogs并不维护具体的member内容，只维护member的个数。也就是说，HyperLogLogs只能用于计算一个集合中不重复的元素数量，所以它比Set要节省很多内存空间。 其他常用命令 EXISTS：判断指定的key是否存在，返回1代表存在，0代表不存在，时间复杂度O(1) DEL：删除指定的key及其对应的value，时间复杂度O(N)，N为删除的key数量 EXPIRE/PEXPIRE：为一个key设置有效期，单位为秒或毫秒，时间复杂度O(1) TTL/PTTL：返回一个key剩余的有效时间，单位为秒或毫秒，时间复杂度O(1) RENAME/RENAMENX：将key重命名为newkey。使用RENAME时，如果newkey已经存在，其值会被覆盖；使用RENAMENX时，如果newkey已经存在，则不会进行任何操作，时间复杂度O(1) TYPE：返回指定key的类型，string, list, set, zset, hash。时间复杂度O(1) CONFIG GET：获得Redis某配置项的当前值，可以使用*通配符，时间复杂度O(1) CONFIG SET：为Redis某个配置项设置新值，时间复杂度O(1) CONFIG REWRITE：让Redis重新加载redis.conf中的配置 数据持久化Redis提供了将数据定期自动持久化至硬盘的能力，包括RDB和AOF两种方案，两种方案分别有其长处和短板，可以配合起来同时运行，确保数据的稳定性。 必须使用数据持久化吗？Redis的数据持久化机制是可以关闭的。如果你只把Redis作为缓存服务使用，Redis中存储的所有数据都不是该数据的主体而仅仅是同步过来的备份，那么可以关闭Redis的数据持久化机制。但通常来说，仍然建议至少开启RDB方式的数据持久化，因为： RDB方式的持久化几乎不损耗Redis本身的性能，在进行RDB持久化时，Redis主进程唯一需要做的事情就是fork出一个子进程，所有持久化工作都由子进程完成 Redis无论因为什么原因crash掉之后，重启时能够自动恢复到上一次RDB快照中记录的数据。这省去了手工从其他数据源（如DB）同步数据的过程，而且要比其他任何的数据恢复方式都要快 现在硬盘那么大，真的不缺那一点地方 RDB采用RDB持久方式，Redis会定期保存数据快照至一个rbd文件中，并在启动时自动加载rdb文件，恢复之前保存的数据。可以在配置文件中配置Redis进行快照保存的时机：1save [seconds] [changes] 意为在[seconds]秒内如果发生了[changes]次数据修改，则进行一次RDB快照保存，例如1save 60 100 会让Redis每60秒检查一次数据变更情况，如果发生了100次或以上的数据变更，则进行RDB快照保存。可以配置多条save指令，让Redis执行多级的快照保存策略。Redis默认开启RDB快照，默认的RDB策略如下：123save 900 1save 300 10save 60 10000 也可以通过BGSAVE命令手工触发RDB快照保存。 RDB的优点： 对性能影响最小。如前文所述，Redis在保存RDB快照时会fork出子进程进行，几乎不影响Redis处理客户端请求的效率。 每次快照会生成一个完整的数据快照文件，所以可以辅以其他手段保存多个时间点的快照（例如把每天0点的快照备份至其他存储媒介中），作为非常可靠的灾难恢复手段。 使用RDB文件进行数据恢复比使用AOF要快很多。 RDB的缺点： 快照是定期生成的，所以在Redis crash时或多或少会丢失一部分数据。 如果数据集非常大且CPU不够强（比如单核CPU），Redis在fork子进程时可能会消耗相对较长的时间（长至1秒），影响这期间的客户端请求。 AOF采用AOF持久方式时，Redis会把每一个写请求都记录在一个日志文件里。在Redis重启时，会把AOF文件中记录的所有写操作顺序执行一遍，确保数据恢复到最新。 AOF默认是关闭的，如要开启，进行如下配置：1appendonly yes AOF提供了三种fsync配置，always/everysec/no，通过配置项[appendfsync]指定： appendfsync no：不进行fsync，将flush文件的时机交给OS决定，速度最快 appendfsync always：每写入一条日志就进行一次fsync操作，数据安全性最高，但速度最慢 appendfsync everysec：折中的做法，交由后台线程每秒fsync一次 随着AOF不断地记录写操作日志，必定会出现一些无用的日志，例如某个时间点执行了命令SET key1 “abc”，在之后某个时间点又执行了SET key1 “bcd”，那么第一条命令很显然是没有用的。大量的无用日志会让AOF文件过大，也会让数据恢复的时间过长。所以Redis提供了AOF rewrite功能，可以重写AOF文件，只保留能够把数据恢复到最新状态的最小写操作集。AOF rewrite可以通过BGREWRITEAOF命令触发，也可以配置Redis定期自动进行：12auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 上面两行配置的含义是，Redis在每次AOF rewrite时，会记录完成rewrite后的AOF日志大小，当AOF日志大小在该基础上增长了100%后，自动进行AOF rewrite。同时如果增长的大小没有达到64mb，则不会进行rewrite。 AOF的优点： 最安全，在启用appendfsync always时，任何已写入的数据都不会丢失，使用在启用appendfsync everysec也至多只会丢失1秒的数据。 AOF文件在发生断电等问题时也不会损坏，即使出现了某条日志只写入了一半的情况，也可以使用redis-check-aof工具轻松修复。 AOF文件易读，可修改，在进行了某些错误的数据清除操作后，只要AOF文件没有rewrite，就可以把AOF文件备份出来，把错误的命令删除，然后恢复数据。 AOF的缺点： AOF文件通常比RDB文件更大 性能消耗比RDB高 数据恢复速度比RDB慢 内存管理与数据淘汰机制最大内存设置默认情况下，在32位OS中，Redis最大使用3GB的内存，在64位OS中则没有限制。 在使用Redis时，应该对数据占用的最大空间有一个基本准确的预估，并为Redis设定最大使用的内存。否则在64位OS中Redis会无限制地占用内存（当物理内存被占满后会使用swap空间），容易引发各种各样的问题。 通过如下配置控制Redis使用的最大内存：1maxmemory 100mb 在内存占用达到了maxmemory后，再向Redis写入数据时，Redis会： 根据配置的数据淘汰策略尝试淘汰数据，释放空间 如果没有数据可以淘汰，或者没有配置数据淘汰策略，那么Redis会对所有写请求返回错误，但读请求仍然可以正常执行 在为Redis设置maxmemory时，需要注意： 如果采用了Redis的主从同步，主节点向从节点同步数据时，会占用掉一部分内存空间，如果maxmemory过于接近主机的可用内存，导致数据同步时内存不足。所以设置的maxmemory不要过于接近主机可用的内存，留出一部分预留用作主从同步。 数据淘汰机制Redis提供了5种数据淘汰策略： volatile-lru：使用LRU算法进行数据淘汰（淘汰上次使用时间最早的，且使用次数最少的key），只淘汰设定了有效期的key allkeys-lru：使用LRU算法进行数据淘汰，所有的key都可以被淘汰 volatile-random：随机淘汰数据，只淘汰设定了有效期的key allkeys-random：随机淘汰数据，所有的key都可以被淘汰 volatile-ttl：淘汰剩余有效期最短的key 最好为Redis指定一种有效的数据淘汰策略以配合maxmemory设置，避免在内存使用满后发生写入失败的情况。 一般来说，推荐使用的策略是volatile-lru，并辨识Redis中保存的数据的重要性。对于那些重要的，绝对不能丢弃的数据（如配置类数据等），应不设置有效期，这样Redis就永远不会淘汰这些数据。对于那些相对不是那么重要的，并且能够热加载的数据（比如缓存最近登录的用户信息，当在Redis中找不到时，程序会去DB中读取），可以设置上有效期，这样在内存不够时Redis就会淘汰这部分数据。 配置方法：1maxmemory-policy volatile-lru #默认是noeviction，即不进行数据淘汰 PipeliningRedis提供许多批量操作的命令，如MSET/MGET/HMSET/HMGET等等，这些命令存在的意义是减少维护网络连接和传输数据所消耗的资源和时间。例如连续使用5次SET命令设置5个不同的key，比起使用一次MSET命令设置5个不同的key，效果是一样的，但前者会消耗更多的RTT(Round Trip Time)时长，永远应优先使用后者。 然而，如果客户端要连续执行的多次操作无法通过Redis命令组合在一起，例如：123SET a &quot;abc&quot;INCR bHSET c name &quot;hi&quot; 此时便可以使用Redis提供的pipelining功能来实现在一次交互中执行多条命令。使用pipelining时，只需要从客户端一次向Redis发送多条命令（以\r\n）分隔，Redis就会依次执行这些命令，并且把每个命令的返回按顺序组装在一起一次返回，比如：1234$ (printf "PING\r\nPING\r\nPING\r\n"; sleep 1) | nc localhost 6379+PONG+PONG+PONG 大部分的Redis客户端都对Pipelining提供支持，所以开发者通常并不需要自己手工拼装命令列表。 Pipelining的局限性Pipelining只能用于执行连续且无相关性的命令，当某个命令的生成需要依赖于前一个命令的返回时，就无法使用Pipelining了。 通过Scripting功能，可以规避这一局限性 事务与ScriptingPipelining能够让Redis在一次交互中处理多条命令，然而在一些场景下，我们可能需要在此基础上确保这一组命令是连续执行的。 比如获取当前累计的PV数并将其清01234&gt; GET vCount12384&gt; SET vCount 0OK 如果在GET和SET命令之间插进来一个INCR vCount，就会使客户端拿到的vCount不准确。 Redis的事务可以确保复数命令执行时的原子性。也就是说Redis能够保证：一个事务中的一组命令是绝对连续执行的，在这些命令执行完成之前，绝对不会有来自于其他连接的其他命令插进去执行。 通过MULTI和EXEC命令来把这两个命令加入一个事务中：123456789&gt; MULTIOK&gt; GET vCountQUEUED&gt; SET vCount 0QUEUED&gt; EXEC1) 123842) OK Redis在接收到MULTI命令后便会开启一个事务，这之后的所有读写命令都会保存在队列中但并不执行，直到接收到EXEC命令后，Redis会把队列中的所有命令连续顺序执行，并以数组形式返回每个命令的返回结果。 可以使用DISCARD命令放弃当前的事务，将保存的命令队列清空。 需要注意的是，Redis事务不支持回滚：如果一个事务中的命令出现了语法错误，大部分客户端驱动会返回错误，2.6.5版本以上的Redis也会在执行EXEC时检查队列中的命令是否存在语法错误，如果存在，则会自动放弃事务并返回错误。但如果一个事务中的命令有非语法类的错误（比如对String执行HSET操作），无论客户端驱动还是Redis都无法在真正执行这条命令之前发现，所以事务中的所有命令仍然会被依次执行。在这种情况下，会出现一个事务中部分命令成功部分命令失败的情况，然而与RDBMS不同，Redis不提供事务回滚的功能，所以只能通过其他方法进行数据的回滚。 通过事务实现CASRedis提供了WATCH命令与事务搭配使用，实现CAS乐观锁的机制。 假设要实现将某个商品的状态改为已售：12if(exec(HGET stock:1001 state) == &quot;in stock&quot;) exec(HSET stock:1001 state &quot;sold&quot;); 这一伪代码执行时，无法确保并发安全性，有可能多个客户端都获取到了”in stock”的状态，导致一个库存被售卖多次。 使用WATCH命令和事务可以解决这一问题：123456exec(WATCH stock:1001);if(exec(HGET stock:1001 state) == &quot;in stock&quot;) &#123; exec(MULTI); exec(HSET stock:1001 state &quot;sold&quot;); exec(EXEC);&#125; WATCH的机制是：在事务EXEC命令执行时，Redis会检查被WATCH的key，只有被WATCH的key从WATCH起始时至今没有发生过变更，EXEC才会被执行。如果WATCH的key在WATCH命令到EXEC命令之间发生过变化，则EXEC命令会返回失败。 Scripting通过EVAL与EVALSHA命令，可以让Redis执行LUA脚本。这就类似于RDBMS的存储过程一样，可以把客户端与Redis之间密集的读/写交互放在服务端进行，避免过多的数据交互，提升性能。 Scripting功能是作为事务功能的替代者诞生的，事务提供的所有能力Scripting都可以做到。Redis官方推荐使用LUA Script来代替事务，前者的效率和便利性都超过了事务。 关于Scripting的具体使用，本文不做详细介绍，请参考官方文档 https://redis.io/commands/eval Redis性能调优尽管Redis是一个非常快速的内存数据存储媒介，也并不代表Redis不会产生性能问题。前文中提到过，Redis采用单线程模型，所有的命令都是由一个线程串行执行的，所以当某个命令执行耗时较长时，会拖慢其后的所有命令，这使得Redis对每个任务的执行效率更加敏感。 针对Redis的性能优化，主要从下面几个层面入手： 最初的也是最重要的，确保没有让Redis执行耗时长的命令 使用pipelining将连续执行的命令组合执行 操作系统的Transparent huge pages功能必须关闭： 1echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled 如果在虚拟机中运行Redis，可能天然就有虚拟机环境带来的固有延迟。可以通过./redis-cli –intrinsic-latency 100命令查看固有延迟。同时如果对Redis的性能有较高要求的话，应尽可能在物理机上直接部署Redis。 检查数据持久化策略 考虑引入读写分离机制 长耗时命令Redis绝大多数读写命令的时间复杂度都在O(1)到O(N)之间，在文本和官方文档中均对每个命令的时间复杂度有说明。 通常来说，O(1)的命令是安全的，O(N)命令在使用时需要注意，如果N的数量级不可预知，则应避免使用。例如对一个field数未知的Hash数据执行HGETALL/HKEYS/HVALS命令，通常来说这些命令执行的很快，但如果这个Hash中的field数量极多，耗时就会成倍增长。又如使用SUNION对两个Set执行Union操作，或使用SORT对List/Set执行排序操作等时，都应该严加注意。 避免在使用这些O(N)命令时发生问题主要有几个办法： 不要把List当做列表使用，仅当做队列来使用 通过机制严格控制Hash、Set、Sorted Set的大小 可能的话，将排序、并集、交集等操作放在客户端执行 绝对禁止使用KEYS命令 避免一次性遍历集合类型的所有成员，而应使用SCAN类的命令进行分批的，游标式的遍历 Redis提供了SCAN命令，可以对Redis中存储的所有key进行游标式的遍历，避免使用KEYS命令带来的性能问题。同时还有SSCAN/HSCAN/ZSCAN等命令，分别用于对Set/Hash/Sorted Set中的元素进行游标式遍历。SCAN类命令的使用请参考官方文档：https://redis.io/commands/scan Redis提供了Slow Log功能，可以自动记录耗时较长的命令。相关的配置参数有两个：12slowlog-log-slower-than xxxms #执行时间慢于xxx毫秒的命令计入Slow Logslowlog-max-len xxx #Slow Log的长度，即最大纪录多少条Slow Log 使用SLOWLOG GET [number]命令，可以输出最近进入Slow Log的number条命令。使用SLOWLOG RESET命令，可以重置Slow Log 网络引发的延迟 尽可能使用长连接或连接池，避免频繁创建销毁连接 客户端进行的批量数据操作，应使用Pipeline特性在一次交互中完成。具体请参照本文的Pipelining章节 数据持久化引发的延迟Redis的数据持久化工作本身就会带来延迟，需要根据数据的安全级别和性能要求制定合理的持久化策略： AOF + fsync always的设置虽然能够绝对确保数据安全，但每个操作都会触发一次fsync，会对Redis的性能有比较明显的影响 AOF + fsync every second是比较好的折中方案，每秒fsync一次 AOF + fsync never会提供AOF持久化方案下的最优性能 使用RDB持久化通常会提供比使用AOF更高的性能，但需要注意RDB的策略配置 每一次RDB快照和AOF Rewrite都需要Redis主进程进行fork操作。fork操作本身可能会产生较高的耗时，与CPU和Redis占用的内存大小有关。根据具体的情况合理配置RDB快照和AOF Rewrite时机，避免过于频繁的fork带来的延迟 Redis在fork子进程时需要将内存分页表拷贝至子进程，以占用了24GB内存的Redis实例为例，共需要拷贝24GB / 4kB * 8 = 48MB的数据。在使用单Xeon 2.27Ghz的物理机上，这一fork操作耗时216ms。 可以通过INFO命令返回的latest_fork_usec字段查看上一次fork操作的耗时（微秒） Swap引发的延迟当Linux将Redis所用的内存分页移至swap空间时，将会阻塞Redis进程，导致Redis出现不正常的延迟。Swap通常在物理内存不足或一些进程在进行大量I/O操作时发生，应尽可能避免上述两种情况的出现。 /proc/&lt;pid&gt;/smaps文件中会保存进程的swap记录，通过查看这个文件，能够判断Redis的延迟是否由Swap产生。如果这个文件中记录了较大的Swap size，则说明延迟很有可能是Swap造成的。 数据淘汰引发的延迟当同一秒内有大量key过期时，也会引发Redis的延迟。在使用时应尽量将key的失效时间错开。 引入读写分离机制Redis的主从复制能力可以实现一主多从的多节点架构，在这一架构下，主节点接收所有写请求，并将数据同步给多个从节点。在这一基础上，我们可以让从节点提供对实时性要求不高的读请求服务，以减小主节点的压力。尤其是针对一些使用了长耗时命令的统计类任务，完全可以指定在一个或多个从节点上执行，避免这些长耗时命令影响其他请求的响应。 主从复制与集群分片主从复制Redis支持一主多从的主从复制架构。一个Master实例负责处理所有的写请求，Master将写操作同步至所有Slave。借助Redis的主从复制，可以实现读写分离和高可用： 实时性要求不是特别高的读请求，可以在Slave上完成，提升效率。特别是一些周期性执行的统计任务，这些任务可能需要执行一些长耗时的Redis命令，可以专门规划出1个或几个Slave用于服务这些统计任务 借助Redis Sentinel可以实现高可用，当Master crash后，Redis Sentinel能够自动将一个Slave晋升为Master，继续提供服务 启用主从复制非常简单，只需要配置多个Redis实例，在作为Slave的Redis实例中配置：1slaveof 192.168.1.1 6379 #指定Master的IP和端口 当Slave启动后，会从Master进行一次冷启动数据同步，由Master触发BGSAVE生成RDB文件推送给Slave进行导入，导入完成后Master再将增量数据通过Redis Protocol同步给Slave。之后主从之间的数据便一直以Redis Protocol进行同步 使用Sentinel做自动failoverRedis的主从复制功能本身只是做数据同步，并不提供监控和自动failover能力，要通过主从复制功能来实现Redis的高可用，还需要引入一个组件：Redis Sentinel Redis Sentinel是Redis官方开发的监控组件，可以监控Redis实例的状态，通过Master节点自动发现Slave节点，并在监测到Master节点失效时选举出一个新的Master，并向所有Redis实例推送新的主从配置。 Redis Sentinel需要至少部署3个实例才能形成选举关系。 关键配置：1234sentinel monitor mymaster 127.0.0.1 6379 2 #Master实例的IP、端口，以及选举需要的赞成票数sentinel down-after-milliseconds mymaster 60000 #多长时间没有响应视为Master失效sentinel failover-timeout mymaster 180000 #两次failover尝试间的间隔时长sentinel parallel-syncs mymaster 1 #如果有多个Slave，可以通过此配置指定同时从新Master进行数据同步的Slave数，避免所有Slave同时进行数据同步导致查询服务也不可用 另外需要注意的是，Redis Sentinel实现的自动failover不是在同一个IP和端口上完成的，也就是说自动failover产生的新Master提供服务的IP和端口与之前的Master是不一样的，所以要实现HA，还要求客户端必须支持Sentinel，能够与Sentinel交互获得新Master的信息才行。 集群分片为何要做集群分片： Redis中存储的数据量大，一台主机的物理内存已经无法容纳 Redis的写请求并发量大，一个Redis实例以无法承载 当上述两个问题出现时，就必须要对Redis进行分片了。Redis的分片方案有很多种，例如很多Redis的客户端都自行实现了分片功能，也有向Twemproxy这样的以代理方式实现的Redis分片方案。然而首选的方案还应该是Redis官方在3.0版本中推出的Redis Cluster分片方案。 本文不会对Redis Cluster的具体安装和部署细节进行介绍，重点介绍Redis Cluster带来的好处与弊端。 Redis Cluster的能力 能够自动将数据分散在多个节点上 当访问的key不在当前分片上时，能够自动将请求转发至正确的分片 当集群中部分节点失效时仍能提供服务 其中第三点是基于主从复制来实现的，Redis Cluster的每个数据分片都采用了主从复制的结构，原理和前文所述的主从复制完全一致，唯一的区别是省去了Redis Sentinel这一额外的组件，由Redis Cluster负责进行一个分片内部的节点监控和自动failover。 Redis Cluster分片原理Redis Cluster中共有16384个hash slot，Redis会计算每个key的CRC16，将结果与16384取模，来决定该key存储在哪一个hash slot中，同时需要指定Redis Cluster中每个数据分片负责的Slot数。Slot的分配在任何时间点都可以进行重新分配。 客户端在对key进行读写操作时，可以连接Cluster中的任意一个分片，如果操作的key不在此分片负责的Slot范围内，Redis Cluster会自动将请求重定向到正确的分片上。 hash tags在基础的分片原则上，Redis还支持hash tags功能，以hash tags要求的格式明明的key，将会确保进入同一个Slot中。例如：{uiv}user:1000和{uiv}user:1001拥有同样的hash tag {uiv}，会保存在同一个Slot中。 使用Redis Cluster时，pipelining、事务和LUA Script功能涉及的key必须在同一个数据分片上，否则将会返回错误。如要在Redis Cluster中使用上述功能，就必须通过hash tags来确保一个pipeline或一个事务中操作的所有key都位于同一个Slot中。 有一些客户端（如Redisson）实现了集群化的pipelining操作，可以自动将一个pipeline里的命令按key所在的分片进行分组，分别发到不同的分片上执行。但是Redis不支持跨分片的事务，事务和LUA Script还是必须遵循所有key在一个分片上的规则要求。 主从复制 vs 集群分片设计软件架构时，要如何在主从复制和集群分片两种部署方案中取舍呢？ 从各个方面看，Redis Cluster都是优于主从复制的方案 Redis Cluster能够解决单节点上数据量过大的问题 Redis Cluster能够解决单节点访问压力过大的问题 Redis Cluster包含了主从复制的能力那是不是代表Redis Cluster永远是优于主从复制的选择呢？ 并不是。 软件架构永远不是越复杂越好，复杂的架构在带来显著好处的同时，一定也会带来相应的弊端。采用Redis Cluster的弊端包括： 维护难度增加。在使用Redis Cluster时，需要维护的Redis实例数倍增，需要监控的主机数量也相应增加，数据备份/持久化的复杂度也会增加。同时在进行分片的增减操作时，还需要进行reshard操作，远比主从模式下增加一个Slave的复杂度要高。 客户端资源消耗增加。当客户端使用连接池时，需要为每一个数据分片维护一个连接池，客户端同时需要保持的连接数成倍增多，加大了客户端本身和操作系统资源的消耗。 性能优化难度增加。你可能需要在多个分片上查看Slow Log和Swap日志才能定位性能问题。 事务和LUA Script的使用成本增加。在Redis Cluster中使用事务和LUA Script特性有严格的限制条件，事务和Script中操作的key必须位于同一个分片上，这就使得在开发时必须对相应场景下涉及的key进行额外的规划和规范要求。如果应用的场景中大量涉及事务和Script的使用，如何在保证这两个功能的正常运作前提下把数据平均分到多个数据分片中就会成为难点。 所以说，在主从复制和集群分片两个方案中做出选择时，应该从应用软件的功能特性、数据和访问量级、未来发展规划等方面综合考虑，只在确实有必要引入数据分片时再使用Redis Cluster。下面是一些建议： 需要在Redis中存储的数据有多大？未来2年内可能发展为多大？这些数据是否都需要长期保存？是否可以使用LRU算法进行非热点数据的淘汰？综合考虑前面几个因素，评估出Redis需要使用的物理内存。 用于部署Redis的主机物理内存有多大？有多少可以分配给Redis使用？对比(1)中的内存需求评估，是否足够用？ Redis面临的并发写压力会有多大？在不使用pipelining时，Redis的写性能可以超过10万次/秒（更多的benchmark可以参考 https://redis.io/topics/benchmarks ） 在使用Redis时，是否会使用到pipelining和事务功能？使用的场景多不多？ 综合上面几点考虑，如果单台主机的可用物理内存完全足以支撑对Redis的容量需求，且Redis面临的并发写压力距离Benchmark值还尚有距离，建议采用主从复制的架构，可以省去很多不必要的麻烦。同时，如果应用中大量使用pipelining和事务，也建议尽可能选择主从复制架构，可以减少设计和开发时的复杂度。 Redis Java客户端的选择Redis的Java客户端很多，官方推荐的有三种：Jedis、Redisson和lettuce。 在这里对Jedis和Redisson进行对比介绍 Jedis： 轻量，简洁，便于集成和改造 支持连接池 支持pipelining、事务、LUA Scripting、Redis Sentinel、Redis Cluster 不支持读写分离，需要自己实现 文档差（真的很差，几乎没有……） Redisson： 基于Netty实现，采用非阻塞IO，性能高 支持异步请求 支持连接池 支持pipelining、LUA Scripting、Redis Sentinel、Redis Cluster 不支持事务，官方建议以LUA Scripting代替事务 支持在Redis Cluster架构下使用pipelining 支持读写分离，支持读负载均衡，在主从复制和Redis Cluster架构下都可以使用 内建Tomcat Session Manager，为Tomcat 6/7/8提供了会话共享功能 可以与Spring Session集成，实现基于Redis的会话共享 文档较丰富，有中文文档 对于Jedis和Redisson的选择，同样应遵循前述的原理，尽管Jedis比起Redisson有各种各样的不足，但也应该在需要使用Redisson的高级特性时再选用Redisson，避免造成不必要的程序复杂度提升。 Jedis：github：https://github.com/xetorthio/jedis文档：https://github.com/xetorthio/jedis/wiki Redisson：github：https://github.com/redisson/redisson文档：https://github.com/redisson/redisson/wiki from http://www.jianshu.com/p/2f14bc570563]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>redis cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile 指令 VOLUME 介绍]]></title>
    <url>%2F2017%2F04%2F06%2Fdocker-dockerfile-volume%2F</url>
    <content type="text"><![CDATA[在介绍VOLUME指令之前，我们来看下如下场景需求： 容器是基于镜像创建的，最后的容器文件系统包括镜像的只读层+可写层，容器中的进程操作的数据持久化都是保存在容器的可写层上。一旦容器删除后，这些数据就没了，除非我们人工备份下来（或者基于容器创建新的镜像）。能否可以让容器进程持久化的数据保存在主机上呢？这样即使容器删除了，数据还在。 当我们在开发一个web应用时，开发环境是在主机本地，但运行测试环境是放在docker容器上。这样的话，我在主机上修改文件（如html，js等）后，需要再同步到容器中。这显然比较麻烦。 多个容器运行一组相关联的服务，如果他们要共享一些数据怎么办？对于这些问题，我们当然能想到各种解决方案。而docker本身提供了一种机制，可以将主机上的某个目录与容器的某个目录（称为挂载点、或者叫卷）关联起来，容器上的挂载点下的内容就是主机的这个目录下的内容，这类似linux系统下mount的机制。 这样的话，我们修改主机上该目录的内容时，不需要同步容器，对容器来说是立即生效的。 挂载点可以让多个容器共享。 下面我们来介绍具体的机制。 通过docker run命令 运行命令：docker run --name test -it -v /home/xqh/myimage:/data ubuntu /bin/bash其中的 -v 标记 在容器中设置了一个挂载点 /data（就是容器中的一个目录），并将主机上的 /home/xqh/myimage 目录中的内容关联到 /data下。这样在容器中对/data目录下的操作，还是在主机上对/home/xqh/myimage的操作，都是完全实时同步的，因为这两个目录实际都是指向主机目录。 运行命令：docker run --name test1 -it -v /data ubuntu /bin/bash上面-v的标记只设置了容器的挂载点，并没有指定关联的主机目录。这时docker会自动绑定主机上的一个目录。通过docker inspect 命令可以查看到。 12345678910"Mounts": [ &#123; "Name": "0ab0aaf0d6ef391cb68b72bd8c43216a8f8ae9205f0ae941ef16ebe32dc9fc01", "Source": "/var/lib/docker/volumes/0ab0aaf0d6ef391cb68b72bd8c43216a8f8ae9205f0ae941ef16ebe32dc9fc01/_data", "Destination": "/data", "Driver": "local", "Mode": "", "RW": true &#125;], 上面 Mounts下的每条信息记录了容器上一个挂载点的信息，”Destination” 值是容器的挂载点，”Source”值是对应的主机目录。 可以看出这种方式对应的主机目录是自动创建的，其目的不是让在主机上修改，而是让多个容器共享。 通过dockerfile创建挂载点上面介绍的通过docker run命令的-v标识创建的挂载点只能对创建的容器有效。通过dockerfile的 VOLUME 指令可以在镜像中创建挂载点，这样只要通过该镜像创建的容器都有了挂载点。还有一个区别是，通过 VOLUME 指令创建的挂载点，无法指定主机上对应的目录，是自动生成的。1234#testFROM ubuntuMAINTAINER hello1VOLUME [&quot;/data1&quot;,&quot;/data2&quot;] 上面的dockfile文件通过VOLUME指令指定了两个挂载点 /data1 和 /data2.我们通过docker inspect 查看通过该dockerfile创建的镜像生成的容器，可以看到如下信息123456789101112131415161718&quot;Mounts&quot;: [ &#123; &quot;Name&quot;: &quot;d411f6b8f17f4418629d4e5a1ab69679dee369b39e13bb68bed77aa4a0d12d21&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/d411f6b8f17f4418629d4e5a1ab69679dee369b39e13bb68bed77aa4a0d12d21/_data&quot;, &quot;Destination&quot;: &quot;/data1&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true &#125;, &#123; &quot;Name&quot;: &quot;6d3badcf47c4ac5955deda6f6ae56f4aaf1037a871275f46220c14ebd762fc36&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/6d3badcf47c4ac5955deda6f6ae56f4aaf1037a871275f46220c14ebd762fc36/_data&quot;, &quot;Destination&quot;: &quot;/data2&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true &#125; ], 可以看到两个挂载点的信息。 容器共享卷（挂载点）1docker run --name test1 -it myimage /bin/bash 上面命令中的 myimage是用前面的dockerfile文件构建的镜像。 这样容器test1就有了 /data1 和 /data2两个挂载点。下面我们创建另一个容器可以和test1共享 /data1 和 /data2卷 ，这是在 docker run中使用 –volumes-from标记，如：可以是来源不同镜像，如：1docker run --name test2 -it --volumes-from test1 ubuntu /bin/bash 也可以是同一镜像，如：1docker run --name test3 -it --volumes-from test1 myimage /bin/bash 上面的三个容器 test1 , test2 , test3 均有 /data1 和 /data2 两个目录，且目录中内容是共享的，任何一个容器修改了内容，别的容器都能获取到。 最佳实践：数据容器如果多个容器需要共享数据（如持久化数据库、配置文件或者数据文件等），可以考虑创建一个特定的数据容器，该容器有1个或多个卷。其它容器通过–volumes-from 来共享这个数据容器的卷。因为容器的卷本质上对应主机上的目录，所以这个数据容器也不需要启动。1docker run --name dbdata myimage echo &quot;data container&quot; from http://www.cnblogs.com/51kata/p/5266626.html]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一张图理解docker的命令]]></title>
    <url>%2F2017%2F03%2F06%2Fdocker-blog%2F</url>
    <content type="text"><![CDATA[docker run –rm加--rm 参数用来运行一次性的容器最好，用完就会把容器删掉，免得手动清理没用的容器的,相当于容器结束时执行docker rm -v显然，--rm选项不能与-d同时使用，即只能自动清理foreground容器，不能自动清理detached容器注意，--rm选项也会清理容器的匿名data volumes。 Dockerfile里面的VOLUME和命令选项-v以下引用https://segmentfault.com/q/1010000004107293VOLUME并非只是声明，它会把指定路径重新加载一遍，我通过inspect容器也发现了这一点。这是在Dockerfile指定了VOLUME，并没有指定-v，查看容器的Mounts信息：12345678910"Mounts": [ &#123; "Name": "b3e2dcacd3f9f40b43ccd5773d45ca74f0f49b02d3da17749cb378ff9f59bb67", "Source": "/var/lib/docker/volumes/b3e2dcacd3f9f40b43ccd5773d45ca74f0f49b02d3da17749cb378ff9f59bb67/_data", "Destination": "/etc", "Driver": "local", "Mode": "", "RW": true &#125; ], 这是在上一个的基础上，指定了-v，查看容器的Mounts信息：12345678"Mounts": [ &#123; "Source": "/etc", "Destination": "/etc", "Mode": "", "RW": true &#125; ], 然后你去/var/lib/docker/volumes/b3e2dcacd3f9f40b43ccd5773d45ca74f0f49b02d3da17749cb378ff9f59bb67/_data目录下看一下，大致就清楚了。你可以把VOLUME理解为，从镜像中复制指定卷的文件夹到本地/var/lib/docker/volumes/xxxxxxxxx/文件夹，然后把本地的该文件夹挂载到容器里面去。本质上还是相当于一个本地文件夹挂载而已。 继续补充，因为VOLUME实际上就是在本地新建了一个文件夹挂载了，那么实际上容器内部的文件夹有三种情况： 没有指定VOLUME也没有指定-v，这种是普通文件夹。 指定了VOLUME没有指定-v，这种文件夹可以在不同容器之间共享，但是无法在本地修改。 指定了-v的文件夹，这种文件夹可以在不同容器之间共享，且可以在本地修改。 那就列举一种需要在不同容器之间共享且不需要在本地修改的情况。 首先，我们先了解容器中获取动态数据的方式： 本地提供，挂载到容器 远程提供，从远程下载 生成提供，在容器内部生成 后面两种命令都不需要在本地修改，但是他们生成的动态数据却可能需要共享。下载命令，比如git clone直接从git服务器拉取代码，不需要挂载本地文件夹。生成命令，比如jekyll（静态网站生成器），你可能挂载一个代码文件夹，然后build目录里生成的静态网页文件需要提供给Apache服务器，那么你需要指定build目录为VOLUME。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SELECT * ...... FOR UPDATE 锁机制]]></title>
    <url>%2F2016%2F12%2F19%2Fmysql-select-for-update%2F</url>
    <content type="text"><![CDATA[由于InnoDB预设是Row-Level Lock，InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！ 举个例子:假设有个表单products ，里面有id跟name二个栏位，id是主键。例1: (明确指定主键，并且有此笔资料，row lock)12SELECT * FROM products WHERE id='3' FOR UPDATE;SELECT * FROM products WHERE id='3' and type=1 FOR UPDATE; 例2: (明确指定主键，若查无此笔资料，无lock)1SELECT * FROM products WHERE id='-1' FOR UPDATE; 例3: (无主键，table lock)1SELECT * FROM products WHERE name='Mouse' FOR UPDATE; 例4: (主键不明确，table lock)1SELECT * FROM products WHERE id&lt;&gt;'3' FOR UPDATE; 例5: (主键不明确，table lock)1SELECT * FROM products WHERE id LIKE '3' FOR UPDATE; 注1: FOR UPDATE仅适用于InnoDB，且必须在交易区块(BEGIN/COMMIT)中才能生效。注2: 要测试锁定的状况，可以利用mysql的Command Mode ，开二个视窗来做测试。 在MySql 5.0中测试确实是这样的另外：MyAsim 只支持表级锁，InnerDB支持行级锁添加了(行级锁/表级锁)锁的数据不能被其它事务再锁定，也不被其它事务修改（修改、删除）是表级锁时，不管是否查询到记录，都会锁定表 此外，如果A与B都对表id进行查询但查询不到记录，则A与B在查询上不会进行row锁，但A与B都会获取排它锁，此时A再插入一条记录的话则会因为B已经有锁而处于等待中，此时B再插入一条同样的数据则会抛出Deadlock found when trying to get lock; try restarting transaction然后释放锁，此时A就获得了锁而插入成功 上面介绍过SELECT … FOR UPDATE 的用法，不过锁定(Lock)的数据是判别就得要注意一下了。由于InnoDB 预设是Row-Level Lock，所以只有「明确」的指定主键，MySQL 才会执行Row lock (只锁住被选取的数据) ，否则MySQL 将会执行Table Lock (将整个数据表单给锁住)。 转载 http://www.cnblogs.com/chenwenbiao/archive/2012/06/06/2537508.html]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL事务和例子]]></title>
    <url>%2F2016%2F09%2F23%2Fmysql-transaction%2F</url>
    <content type="text"><![CDATA[事务（Transaction）及其ACID属性事务是由一组SQL语句组成的逻辑处理单元，事务具有以下4个属性，通常简称为事务的ACID属性： 原子性（Atomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。 一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。 隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的，反之亦然。 持久性（Durable）：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 并发事务处理带来的问题相对于串行处理来说，并发事务处理能大大增加数据库资源的利用率，提高数据库系统的事务吞吐量，从而可以支持更多的用户。但并发事务处理也会带来一些问题，主要包括以下几种情况。 更新丢失（Lost Update）：当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题－－最后的更新覆盖了由其他事务所做的更新。例如，两个编辑人员制作了同一文档的电子副本。每个编辑人员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。最后保存其更改副本的编辑人员覆盖另一个编辑人员所做的更改。如果在一个编辑人员完成并提交事务之前，另一个编辑人员不能访问同一文件，则可避免此问题。 脏读（Dirty Reads）：一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象地叫做”脏读”。 不可重复读（Non-Repeatable Reads）：一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读”。 幻读（Phantom Reads）：一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。 事务隔离级别在上面讲到的并发事务处理带来的问题中，“更新丢失”通常是应该完全避免的。但防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决，因此，防止更新丢失应该是应用的责任。“脏读”、“不可重复读”和“幻读”，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。数据库实现事务隔离的方式，基本上可分为以下两种： 在读取数据前，对其加锁，阻止其他事务对数据进行修改。 不用加任何锁，通过一定机制生成一个数据请求时间点的一致性数据快照（Snapshot)，并用这个快照来提供一定级别（语句级或事务级）的一致性读取。从用户的角度来看，好像是数据库可以提供同一数据的多个版本，因此，这种技术叫做数据多版本并发控制（MultiVersion Concurrency Control，简称MVCC或MCC），也经常称为多版本数据库。 数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上 “串行化”进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对“不可重复读”和“幻读”并不敏感，可能更关心数据并发访问的能力。为了解决“隔离”与“并发”的矛盾，ISO/ANSI SQL92定义了4个事务隔离级别，每个级别的隔离程度不同，允许出现的副作用也不同，应用可以根据自己的业务逻辑要求，通过选择不同的隔离级别来平衡 “隔离”与“并发”的矛盾。下表很好地概括了这4个隔离级别的特性： 通过例子理解事务的四种隔离级别首先，我们使用 test 数据库，新建 tx 表，表里面有id和num两个字段，并且打开两个客户端来操作同一个数据库。 第1级别：Read Uncommitted(读取未提交内容)(1)所有事务都可以看到其他未提交事务的执行结果(2)本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少(3)该级别引发的问题是——脏读(Dirty Read)：读取到了未提交的数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#首先，修改隔离级别set tx_isolation='READ-UNCOMMITTED';select @@tx_isolation;+------------------+| @@tx_isolation |+------------------+| READ-UNCOMMITTED |+------------------+#事务A：启动一个事务start transaction;select * from tx;+------+------+| id | num |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 |+------+------+#事务B：也启动一个事务(那么两个事务交叉了) 在事务B中执行更新语句，且不提交start transaction;update tx set num=10 where id=1;select * from tx;+------+------+| id | num |+------+------+| 1 | 10 || 2 | 2 || 3 | 3 |+------+------+#事务A：那么这时候事务A能看到这个更新了的数据吗?select * from tx;+------+------+| id | num |+------+------+| 1 | 10 | ---&gt;可以看到！说明我们读到了事务B还没有提交的数据| 2 | 2 || 3 | 3 |+------+------+#事务B：事务B回滚,仍然未提交rollback;select * from tx;+------+------+| id | num |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 |+------+------+#事务A：在事务A里面看到的也是B没有提交的数据select * from tx;+------+------+| id | num |+------+------+| 1 | 1 | ---&gt;脏读意味着我在这个事务中(A中)，事务B虽然没有提交，但它任何一条数据变化，我都可以看到！| 2 | 2 || 3 | 3 |+------+------+ 第2级别：Read Committed(读取提交内容)(1)这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）(2)它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变(3)这种隔离级别出现的问题是——不可重复读(Nonrepeatable Read)：不可重复读意味着我们在同一个事务中执行完全相同的select语句时可能看到不一样的结果。导致这种情况的原因可能有：(1)有一个交叉的事务有新的commit，导致了数据的改变;(2)一个数据库被多个实例操作时,同一事务的其他实例在该实例处理其间可能会有新的commit12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#首先修改隔离级别set tx_isolation='read-committed';select @@tx_isolation;+----------------+| @@tx_isolation |+----------------+| READ-COMMITTED |+----------------+#事务A：启动一个事务start transaction;select * from tx;+------+------+| id | num |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 |+------+------+#事务B：也启动一个事务(那么两个事务交叉了) 在这事务中更新数据，且未提交start transaction;update tx set num=10 where id=1;select * from tx;+------+------+| id | num |+------+------+| 1 | 10 || 2 | 2 || 3 | 3 |+------+------+#事务A：这个时候我们在事务A中能看到数据的变化吗?select * from tx; ---------------&gt;+------+------+ || id | num | |+------+------+ || 1 | 1 |---&gt;并不能看到！ || 2 | 2 | || 3 | 3 | |+------+------+ |——&gt;相同的select语句，结果却不一样 |#事务B：如果提交了事务B呢? |commit; | |#事务A: |select * from tx; ---------------&gt;+------+------+| id | num |+------+------+| 1 | 10 |---&gt;因为事务B已经提交了，所以在A中我们看到了数据变化| 2 | 2 || 3 | 3 |+------+------+ 第3级别：Repeatable Read(可重读)(1)这是MySQL的默认事务隔离级别(2)它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行(3)此级别可能出现的问题——幻读(Phantom Read)：当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行(4)InnoDB和Falcon存储引擎通过多版本并发控制(MVCC，Multiversion Concurrency Control)机制解决了该问题123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#首先，更改隔离级别set tx_isolation='repeatable-read';select @@tx_isolation;+-----------------+| @@tx_isolation |+-----------------+| REPEATABLE-READ |+-----------------+#事务A：启动一个事务start transaction;select * from tx;+------+------+| id | num |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 |+------+------+#事务B：开启一个新事务(那么这两个事务交叉了) 在事务B中更新数据，并提交start transaction;update tx set num=10 where id=1;select * from tx;+------+------+| id | num |+------+------+| 1 | 10 || 2 | 2 || 3 | 3 |+------+------+commit;#事务A：这时候即使事务B已经提交了,但A能不能看到数据变化？select * from tx;+------+------+| id | num |+------+------+| 1 | 1 | ---&gt;还是看不到的！(这个级别2不一样，也说明级别3解决了不可重复读问题)| 2 | 2 || 3 | 3 |+------+------+#事务A：只有当事务A也提交了，它才能够看到数据变化commit;select * from tx;+------+------+| id | num |+------+------+| 1 | 10 || 2 | 2 || 3 | 3 |+------+------+ 第4级别：Serializable(可串行化)(1)这是最高的隔离级别(2)它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之,它是在每个读的数据行上加上共享锁。(3)在这个级别，可能导致大量的超时现象和锁竞争123456789101112131415161718#首先修改隔离界别set tx_isolation='serializable';select @@tx_isolation;+----------------+| @@tx_isolation |+----------------+| SERIALIZABLE |+----------------+#事务A：开启一个新事务start transaction;#事务B：在A没有commit之前，这个交叉事务是不能更改数据的start transaction;insert tx values('4','4');ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionupdate tx set num=10 where id=1;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL分区和分表]]></title>
    <url>%2F2016%2F09%2F21%2Fmysql-partition%2F</url>
    <content type="text"><![CDATA[概念 为什么要分表和分区？日常开发中我们经常会遇到大表的情况，所谓的大表是指存储了百万级乃至千万级条记录的表。这样的表过于庞大，导致数据库在查询和插入的时候耗时太长，性能低下，如果涉及联合查询的情况，性能会更加糟糕。分表和表分区的目的就是减少数据库的负担，提高数据库的效率，通常点来讲就是提高表的增删改查效率。 什么是分表？分表是将一个大表按照一定的规则分解成多张具有独立存储空间的实体表，我们可以称为子表，每个表都对应三个文件，MYD数据文件，.MYI索引文件，.frm表结构文件。这些子表可以分布在同一块磁盘上，也可以在不同的机器上。app读写的时候根据事先定义好的规则得到对应的子表名，然后去操作它。 什么是分区？分区和分表相似，都是按照规则分解表。不同在于分表将大表分解为若干个独立的实体表，而分区是将数据分段划分在多个位置存放，可以是同一块磁盘也可以在不同的机器。分区后，表面上还是一张表，但数据散列到多个位置了。app读写的时候操作的还是大表名字，db自动去组织分区的数据。 mysql分表和分区有什么联系呢？（1）都能提高mysql的性高，在高并发状态下都有一个良好的表现。（2）分表和分区不矛盾，可以相互配合的，对于那些大访问量，并且表数据比较多的表，我们可以采取分表和分区结合的方式（如果merge这种分表方式，不能和分区配合的话，可以用其他的分表试），访问量不大，但是表数据很多的表，我们可以采取分区的方式等。（3）分表技术是比较麻烦的，需要手动去创建子表，app服务端读写时候需要计算子表名。采用merge好一些，但也要创建子表和配置子表间的union关系。（4）表分区相对于分表，操作方便，不需要创建子表。 分区分区的类型： Range：把连续区间按范围划分例： 1234567891011create table user( id int(11), money int(11) unsigned not null, date datetime)partition by range(YEAR(date))( partition p2014 values less than (2015), partition p2015 values less than (2016), partition p2016 values less than (2017), partition p2017 values less than maxvalue); List：把离散值分成集合，按集合划分，适合有固定取值列的表例： 12345678create table user( a int(11), b int(11))partition by list(b)( partition p0 values in (1,3,5,7,9), partition p1 values in (2,4,6,8,0)); Hash：随机分配，分区数固定例： 123456create table user( a int(11), b datetime)partition by hash(YEAR(b))partitions 4; Key：类似Hash，区别是只支持1列或多列,且mysql提供自身的Hash函数例： 123456create table user( a int(11), b datetime)partition by key(b)partitions 4; 分区管理 新增分区 12ALTER TABLE sale_dataADD PARTITION (PARTITION p201710 VALUES LESS THAN (201711)); 删除分区 12--当删除了一个分区，也同时删除了该分区中所有的数据。ALTER TABLE sale_data DROP PARTITION p201710; 分区的合并下面的SQL，将p201701 - p201709 合并为3个分区p2017Q1 - p2017Q3 123456789ALTER TABLE sale_dataREORGANIZE PARTITION p201701,p201702,p201703,p201704,p201705,p201706,p201707,p201708,p201709 INTO( PARTITION p2017Q1 VALUES LESS THAN (201704), PARTITION p2017Q2 VALUES LESS THAN (201707), PARTITION p2017Q3 VALUES LESS THAN (201710)); 分区应该注意的事项： 做分区时，要么不定义主键，要么把分区字段加入到主键中。 分区字段不能为NULL，要不然怎么确定分区范围呢，所以尽量NOT NULL 分表垂直分表把原来有很多列的表拆分成多个表，原则是：（1）把常用、不常用的字段分开放（2）把大字段独立存放在一个表中 水平分表为了解决单表数据量过大的问题，每个水平拆分表的结构完全一致。 按时间结构如果业务系统对时效性较高，比如新闻发布系统的文章表，可以把数据库设计成时间结构，按时间分有几种结构：（a）平板式表类似：123article_201701article_201702article_201703 用年来分还是用月可自定，但用日期的话表就太多了，也没这必要。一般建议是按月分就可以。这种分法，其难处在于，假设我要列20条数据，结果这三张表里都有2条，那么业务上很有可能要求读三次表。如果时间长了，有几十张表，而每张表是0条，那不就是要读完整个系统的表才行么?另外这个结构，要作分页是比较难实现的。主键：在这个系统中，主键是13位带毫秒的时间戳，不要用自动编号，否则难以通过主键定位到表，也可以在查询时带上时间，但比较烦琐。（b）归档式表类似：12article_oldarticle_new 为了解决平板式的缺点，可以采用时间归档式设计，可以看到这个系统只有两张表。一张是旧文章表，一张是新文章表，新文章表放2个月的信息，每天定期把2个月中的最早一天的文章归入旧表中。这样一方面可以解决性能问题，因为一般新闻发布系统读取的都是新的内容，旧的内容读取少;第二可以委婉地解决功能问题，比如平板式所说的问题，在归档式中最多也只需要读2张表就完成了。归档式的缺点在于旧表容量还是相对比较大，如果业务允许，可对旧表中的超旧内容进行再归档或直接清理掉。 按版块结构如果按照文章的所属版块进行拆表，比如新闻、体育版块拆表，一方面可以使每个表数据量分离，另一方面是各版块之间相互影响可降到最低。假如新闻版块的数据表损坏或需要维护，并不会影响到体育版块的正常工作，从而降低了风险。版块结构同时常用于bbs这样的系统。板块结构也有几种分法：（a）对应式对于版块数量不多，而且较为固定的形式，就直接对应就好。比如新闻版块，可以分出新闻的目录表，新闻的文章表等。1234news_categorynews_articlesports_categorysports_article 可看到每一个版块都对应着一组相同的表结构，好处就是一目了然。在功能上，因为版块之间还是有一些隔阂，所以需要联合查询的需求不多，开发上比时间结构的方式要轻松。主键：依旧要考虑的，在这个系统中，主键是版块+时间戳，单纯的时间戳或自动编号也能用，查询时要记得带上版块用于定位表。（b）冷热式对应式的缺点是，如果版块数量很大而且不确定，那要分出的表数量就太多了。举个例子：百度贴吧，如果按一个词条一个表设计，那得有多少张表呢?用这样的方式吧。1234tieba_汽车tieba_飞机tieba_火箭tieba_unite 这个表汽车、火箭表是属于热门表，定义为新建的版块放在unite表里面，待到其超过一万张主贴的时候才开对应表结构。因为在贴吧这种系统中，冷门版块肯定比热门版块多得多，这些冷门版块通常只有几张帖子，为它们开表也太浪费了;同时热门版块数量和访问量等，又比冷门版块多得多，非常有特点。unite表还可以扩展成哈希表，利用词条的md5编码，可以分成n张表，我算了一下，md5前一位可分36张表，两位即是1296张表，足够了。12tieba_unite_abtieba_unite_ac 按哈希结构哈希结构通常用于博客之类的基于用户的场合，在博客这样的系统里有几个特点，1是用户数量非常多，2是每个用户发的文章数量都较少，3是用户发文章不定期，4是每个用户发得不多，但总量仍非常之大。基于这些特点，用以上所说的任何一种分表方式都不合适，一没有固定的时效不宜用时间拆，二用户很多，而且还偏偏都是冷门，所以也不宜用版块(用户)拆。哈希结构在上面有所提及，既然按每个用户不好直接拆，那就把一群用户归进一个表好了。123blog_aablog_abblog_ac 如上所说，md5取前两位哈希可以达到1296张表，如果觉得不够，那就再加一位，总数可达46656张表，还不够?表的数量太多，要创建这些表也是挺麻烦的，可以考虑在程序里往数据库insert之前，多执行一句判断表存在与否并创建表的语句，很实用，消耗也并不很大。主键：依旧要考虑的，在这个系统中，主键是用户ID+时间戳，单纯的时间戳或自动编号也能用，但查询时要记得带上用户名用于定位表。 merge存储引擎分表使用场景Merge表有点类似于视图。使用Merge存储引擎实现MySQL分表，这种方法比较适合那些没有事先考虑分表，随着数据的增多，已经出现了数据查询慢的情况。 这个时候如果要把已有的大数据量表分开比较痛苦，最痛苦的事就是改代码。所以使用Merge存储引擎实现MySQL分表可以避免改代码。 Merge引擎下每一张表只有一个MRG文件。MRG里面存放着分表的关系，以及插入数据的方式。它就像是一个外壳，或者是连接池，数据存放在分表里面。 对于增删改查，直接操作总表即可。 建表 用户1表 123456CREATE TABLE `user1` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(50) DEFAULT NULL, `sex` int(1) NOT NULL DEFAULT '0', PRIMARY KEY (`id`)) ENGINE=MyISAM DEFAULT CHARSET=utf8; 用户2表 1create table user2 like user1; 主表 123456CREATE TABLE `alluser` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(50) DEFAULT NULL, `sex` int(1) NOT NULL DEFAULT '0', KEY `id` (`id`)) ENGINE=MRG_MyISAM DEFAULT CHARSET=utf8 INSERT_METHOD=LAST UNION=(`user1`,`user2`); 1) ENGINE = MERGE 和 ENGINE = MRG_MyISAM是一样的意思，都是代表使用的存储引擎是 Merge。2) INSERT_METHOD，表示插入方式，取值可以是：0 和 1，0代表不允许插入，1代表可以插入；3) FIRST插入到UNION中的第一个表，LAST插入到UNION中的最后一个表。 操作 先在user1表中增加一条数据，然后再在user2表中增加一条数据，查看 alluser中的数据。123insert into user1(name,sex) values ('张三',1);insert into user2(name,sex) values ('李四',2);select * from alluser; 发现是刚刚插入的数据如下：这就出现了一个id重复，这就造成了当删除和修改的时候异常，解决办法是给 alluser的id赋唯一值。我们解决方法是，重新建立一张表tb_ids(id int)，用来专门存一个id的，并插入一条初始数据，同时删除掉user1和user2中的数据。1234create table tb_ids(id int);insert into tb_ids values(1);delete from user1;delete from user2; 然后在user1和user2表中建立一个触发器，触发器的功能是 当在user1或者user2表中增加一条记录时，取出tb_ids中的id值，赋给user1和user2的id，然后将tb_ids的id值加1，触发器内容如下（将user1改为user2）： 123456789DELIMITER $$ CREATE TRIGGER tr_seq BEFORE INSERT on user1 FOR EACH ROW BEGIN select id into @testid from tb_ids limit 1; update tb_ids set id = @testid + 1; set new.id = @testid; END$$ DELIMITER; 在user1和user2表中分别增加一条数据， 12insert into user1(name,sex) values('王五',1);insert into user2(name,sex) values('赵六',2); 查询user1和user2中的数据： 查询总表alluser中的数据，发现id没有重复的：]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL索引类型]]></title>
    <url>%2F2016%2F09%2F20%2Fmysql-index-type%2F</url>
    <content type="text"><![CDATA[简介MySQL目前主要有以下几种索引类型： 普通索引 唯一索引 主键索引 组合索引 全文索引 语句12CREATE TABLE table_name[col_name data type][unique|fulltext][index|key][index_name](col_name[length])[asc|desc] unique|fulltext为可选参数，分别表示唯一索引、全文索引 index和key为同义词，两者作用相同，用来指定创建索引 col_name为需要创建索引的字段列，该列必须从数据表中该定义的多个列中选择 index_name指定索引的名称，为可选参数，如果不指定，默认col_name为索引值 length为可选参数，表示索引的长度，只有字符串类型的字段才能指定索引长度 asc或desc指定升序或降序的索引值存储 索引类型普通索引是最基本的索引，它没有任何限制。它有以下几种创建方式： 直接创建索引 1CREATE INDEX index_name ON table(column(length)) 修改表结构的方式添加索引 1ALTER TABLE table_name ADD INDEX index_name ON (column(length)) 创建表的时候同时创建索引 12345678CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER NOT NULL , `content` text CHARACTER NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), INDEX index_name (title(length))) 删除索引 1DROP INDEX index_name ON table 唯一索引与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。它有以下几种创建方式： 创建唯一索引 1CREATE UNIQUE INDEX indexName ON table(column(length)) 修改表结构 1ALTER TABLE table_name ADD UNIQUE indexName ON (column(length)) 创建表的时候直接指定 1234567CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER NOT NULL , `content` text CHARACTER NULL , `time` int(10) NULL DEFAULT NULL , UNIQUE indexName (title(length))); 主键索引是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。一般是在建表的时候同时创建主键索引：12345CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) NOT NULL , PRIMARY KEY (`id`)); 组合索引指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀集合1ALTER TABLE `table` ADD INDEX name_city_age (name,city,age); 全文索引主要用来查找文本中的关键字，而不是直接与索引中的值相比较。fulltext索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的where语句的参数匹配。fulltext索引配合match against操作使用，而不是一般的where语句加like。它可以在create table，alter table ，create index使用，不过目前只有char、varchar，text 列上可以创建全文索引。值得一提的是，在数据量较大时候，先将数据放入一个没有全文索引的表中，然后再用CREATE index创建fulltext索引，要比先为一张表建立fulltext然后再将数据写入的速度快很多。 创建表的时候添加全文索引 12345678CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER NOT NULL , `content` text CHARACTER NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), FULLTEXT (content)); 修改表结构添加全文索引 1ALTER TABLE article ADD FULLTEXT index_content(content) 直接创建索引 1CREATE FULLTEXT INDEX index_content ON article(content) 缺点 虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行insert、update和delete。因为更新表时，不仅要保存数据，还要保存一下索引文件。 建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会增长很快。索引只是提高效率的一个因素，如果有大数据量的表，就需要花时间研究建立最优秀的索引，或优化查询语句。 注意事项使用索引时，有以下一些技巧和注意事项： 索引不会包含有null值的列 只要列中包含有null值都将不会被包含在索引中，复合索引中只要有一列含有null值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为null。 使用短索引 对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个char(255)的列，如果在前10个或20个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。 索引列排序 查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。 like语句操作 一般情况下不推荐使用like操作，如果非使用不可，如何使用也是一个问题。like “%aaa%” 不会使用索引而like “aaa%”可以使用索引。 不要在列上进行运算 这将导致索引失效而进行全表扫描，例如 1SELECT * FROM table_name WHERE YEAR(column_name)&lt;2017; 不使用not in和&lt;&gt;操作]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL索引方法]]></title>
    <url>%2F2016%2F09%2F20%2Fmysql-index-method%2F</url>
    <content type="text"><![CDATA[MySQL目前主要有以下几种索引方法：B-Tree，Hash，R-Tree。 B-TreeB-Tree是最常见的索引类型，所有值（被索引的列）都是排过序的，每个叶节点到跟节点距离相等。所以B-Tree适合用来查找某一范围内的数据，而且可以直接支持数据排序（ORDER BY）B-Tree在MyISAM里的形式和Innodb稍有不同：MyISAM表数据文件和索引文件是分离的，索引文件仅保存数据记录的磁盘地址InnoDB表数据文件本身就是主索引，叶节点data域保存了完整的数据记录 Hash索引 仅支持”=”,”IN”和”&lt;=&gt;”精确查询，不能使用范围查询： 由于Hash索引比较的是进行Hash运算之后的Hash值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的Hash算法处理之后的Hash 不支持排序： 由于Hash索引中存放的是经过Hash计算之后的Hash值，而且Hash值的大小关系并不一定和Hash运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运算 在任何时候都不能避免表扫描： 由于Hash索引比较的是进行Hash运算之后的Hash值，所以即使取满足某个Hash键值的数据的记录条数，也无法从Hash索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果 检索效率高，索引的检索可以一次定位，不像B-Tree索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以Hash索引的查询效率要远高于B-Tree索引 只有Memory引擎支持显式的Hash索引，但是它的Hash是nonunique的，冲突太多时也会影响查找性能。Memory引擎默认的索引类型即是Hash索引，虽然它也支持B-Tree索引 R-Tree索引R-Tree在MySQL很少使用，仅支持geometry数据类型，支持该类型的存储引擎只有MyISAM、BDb、InnoDb、NDb、Archive几种。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开启mysql慢查询]]></title>
    <url>%2F2016%2F09%2F20%2Fmysql-slow-query%2F</url>
    <content type="text"><![CDATA[简介开启慢查询日志，可以让MySQL记录下查询超过指定时间的语句，通过定位分析性能的瓶颈，才能更好的优化数据库系统的性能。 参数说明slow_query_log 慢查询开启状态slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录）long_query_time 查询超过多少秒才记录 设置步骤 查看慢查询相关参数 1234567891011121314mysql&gt; show variables like 'slow_query%';+---------------------------+----------------------------------+| Variable_name | Value |+---------------------------+----------------------------------+| slow_query_log | OFF || slow_query_log_file | /mysql/data/localhost-slow.log |+---------------------------+----------------------------------+mysql&gt; show variables like 'long_query_time';+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 10.000000 |+-----------------+-----------+ 设置方法 方法一：全局变量设置 将 slow_query_log 全局变量设置为“ON”状态 1mysql&gt; set global slow_query_log='ON'; 设置慢查询日志存放的位置 1mysql&gt; set global slow_query_log_file='/usr/local/mysql/data/slow.log'; 查询超过1秒就记录 1mysql&gt; set global long_query_time=1; 方法二：配置文件设置 修改配置文件my.cnf，在[mysqld]下的下方加入 1234[mysqld]slow_query_log = ONslow_query_log_file = /usr/local/mysql/data/slow.loglong_query_time = 1 重启MySQL服务 1service mysqld restart 查看设置后的参数 1234567891011121314mysql&gt; show variables like 'slow_query%';+---------------------+--------------------------------+| Variable_name | Value |+---------------------+--------------------------------+| slow_query_log | ON || slow_query_log_file | /usr/local/mysql/data/slow.log |+---------------------+--------------------------------+mysql&gt; show variables like 'long_query_time';+-----------------+----------+| Variable_name | Value |+-----------------+----------+| long_query_time | 1.000000 |+-----------------+----------+ 测试 执行一条慢查询SQL语句 1mysql&gt; select sleep(2); 查看是否生成慢查询日志 1ls /usr/local/mysql/data/slow.log 如果日志存在，MySQL开启慢查询设置成功！]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL各存储引擎]]></title>
    <url>%2F2016%2F09%2F20%2Fmysql-storage-engine%2F</url>
    <content type="text"><![CDATA[MySQL中的数据用各种不同的技术存储在文件（或者内存）中。这些技术中的每一种技术都使用不同的存储机制、索引技巧、锁定水平并且最终提供广泛的不同的功能和能力。通过选择不同的技术，你能够获得额外的速度或者功能，从而改善你的应用的整体功能。这些不同的技术以及配套的相关功能在MySQL中被称作存储引擎(也称作表类型)。MySQL默认配置了许多不同的存储引擎，可以预先设置或者在MySQL服务器中启用。你可以选择适用于服务器、数据库和表格的存储引擎，以便在选择如何存储你的信息、如何检索这些信息以及你需要你的数据结合什么性能和功能的时候为你提供最大的灵活性。使用以下命令可以查看MySQL支持的引擎：1mysql&gt; show engines; MyISAM存储引擎默认的存储引擎，提供高速存储和检索，以及全文搜索能力。不支持事务。表级锁。不能在表损坏后恢复数据。每个表会生成三个文件(文件名就是表名)：.frm 表结构；.MYD 数据；.MYI 索引。适合在以下几种情况下使用： 做很多count的计算 查询非常频繁 InnoDB存储引擎具有提交、回滚和崩溃恢复能力的事务安全（ACID兼容）存储引擎。基于聚簇索引建立，聚簇索引对主键查询有很高的性能。不过它的二级索引(secondary index，非主键索引)中必须包含主键列，所以如果主键列很大的话，其他的所有索引都会很大。因此表上的索引较多的话，主键应当尽可能的小。支持事务和外键。行级锁。适合在以下几种情况下使用： 更新和查询都相当的频繁，多重并发 要求事务，或者可靠性要求比较高 外键约束，MySQL支持外键的存储引擎只有InnoDB一般来说，如果需要事务支持，并且有较高的并发读取频率，InnoDB是不错的选择。 MEMORY（HEAP）引擎数据保存在内存中，拥有极高的插入、更新和查询效率。但是不稳定，重启以后数据都会丢失。不支持事务。支持表级锁，因此并发写入的性能较低。支持长度不变的数据类型，不支持BLOB或TEXT长度可变的数据类型。VARCHAR是一种长度可变的类型，但因为它在MySQL内部当做长度固定不变的CHAR类型，所以可以使用。每个表会生成一个.frm文件，该文件只存储表的结构。支持HASH索引和B-Tree索引擎默认使用HASH索引。B-Tree索引的优于HASH索引的是，可以使用部分查询和通配查询，也可以使用&lt;、&gt;和&gt;=等操作符方便数据挖掘。HASH索引进行“相等比较”非常快，但是对“范围比较”的速度就慢多了，因此HASH索引值适合使用在=和&lt;&gt;的操作符中，不适合在&lt;或&gt;操作符中，也同样不适合用在order by子句中。在内存中存放数据，所以会造成内存的使用，可以通过参数max_heap_table_size控制MEMORY表的大小。 ARCHIVE引擎拥有很好的压缩机制，它使用zlib压缩库，在记录被请求时会实时压缩。支持最基本的插入和查询两种功能。在MySQL 5.5开始支持索引。不支持事务。支持行级锁和专用的缓存区，所以可以实现高并发的插入。适合存储大量日志、历史数据。 BLACKHOLE引擎接受但不存储数据，但是如果MySQL启用了二进制日志，SQL语句被写入日志（并被复制到从服务器）。用于做日志记录或同步归档的中继存储。但这种应用方式会碰到很多问题，因此并不推荐。支持事务，而且支持mvcc的行级锁。 CSV引擎每个表会生成一个.CSV文件，将CSV类型的文件当做表进行处理。把数据以逗号分隔的格式存储在文本文件中，这种文件是一种普通文本文件，每个数据行占用一个文本行。不支持索引，即使用该种类型的表没有主键列，也不允许表中的字段为null。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单理解Java GC与幽灵引用]]></title>
    <url>%2F2016%2F09%2F11%2Fjava-gc-reference%2F</url>
    <content type="text"><![CDATA[Java中一共有4种类型的引用:StrongReference、SoftReference、WeakReference以及PhantomReference (幽灵引用), 这 4 种类型的引用与Java GC有着密切的关系, 让我们逐一来看它们的定义和使用场景。 Strong ReferenceStrongReference 是 Java 的默认引用实现,它会尽可能长时间的存活于 JVM 内， 当没有任何对象指向它时Java GC 执行后将会被回收12345678910111213141516171819@Test public void strongReference() &#123; Object referent = new Object(); /** * 通过赋值创建 StrongReference */ Object strongReference = referent; assertSame(referent, strongReference); referent = null; System.gc(); /** * StrongReference 在 GC 后不会被回收 */ assertNotNull(strongReference); &#125; WeakReference &amp; WeakHashMapWeakReference， 顾名思义,是一个弱引用,当所引用的对象在 JVM 内不再有强引用时, Java GC 后 weak reference 将会被自动回收123456789101112131415@Test public void weakReference() &#123; Object referent = new Object(); WeakReference&lt;Object&gt; weakRerference = new WeakReference&lt;Object&gt;(referent); assertSame(referent, weakRerference.get()); referent = null; System.gc(); /** * 一旦没有指向 referent 的强引用, weak reference 在 GC 后会被自动回收 */ assertNull(weakRerference.get()); &#125; WeakHashMap 使用 WeakReference 作为 key， 一旦没有指向 key 的强引用, WeakHashMap 在Java GC 后将自动删除相关的 entry12345678910111213141516171819202122@Test public void weakHashMap() throws InterruptedException &#123; Map&lt;Object, Object&gt; weakHashMap = new WeakHashMap&lt;Object, Object&gt;(); Object key = new Object(); Object value = new Object(); weakHashMap.put(key, value); assertTrue(weakHashMap.containsValue(value)); key = null; System.gc(); /** * 等待无效 entries 进入 ReferenceQueue 以便下一次调用 getTable 时被清理 */ Thread.sleep(1000); /** * 一旦没有指向 key 的强引用, WeakHashMap 在 GC 后将自动删除相关的 entry */ assertFalse(weakHashMap.containsValue(value)); &#125; SoftReferenceSoftReference 于 WeakReference 的特性基本一致， 最大的区别在于 SoftReference 会尽可能长的保留引用直到 JVM 内存不足时才会被回收(虚拟机保证), 这一特性使得 SoftReference 非常适合缓存应用123456789101112131415@Test public void softReference() &#123; Object referent = new Object(); SoftReference&lt;Object&gt; softRerference = new SoftReference&lt;Object&gt;(referent); assertNotNull(softRerference.get()); referent = null; System.gc(); /** *soft references 只有在 jvm OutOfMemory 之前才会被回收, 所以它非常适合缓存应用 */ assertNotNull(softRerference.get()); &#125; Phantom Reference作为本文主角， Phantom Reference(幽灵引用) 与 WeakReference 和 SoftReference 有很大的不同,因为它的 get() 方法永远返回 null, 这也正是它名字的由来12345678910@Test public void phantomReferenceAlwaysNull() &#123; Object referent = new Object(); PhantomReference&lt;Object&gt; phantomReference = new PhantomReference&lt;Object&gt;(referent, new ReferenceQueue&lt;Object&gt;()); /** * phantom reference 的 get 方法永远返回 null */ assertNull(phantomReference.get()); &#125; 诸位可能要问, 一个永远返回 null 的 reference 要来何用,请注意构造 PhantomReference 时的第二个参数 ReferenceQueue(事实上 WeakReference &amp; SoftReference 也可以有这个参数)，PhantomReference 唯一的用处就是跟踪 referent何时被 enqueue 到 ReferenceQueue 中. RererenceQueue当一个 WeakReference 开始返回 null 时， 它所指向的对象已经准备被回收， 这时可以做一些合适的清理工作. 将一个 ReferenceQueue 传给一个 Reference 的构造函数， 当对象被回收时， 虚拟机会自动将这个对象插入到 ReferenceQueue 中， WeakHashMap 就是利用 ReferenceQueue 来清除 key 已经没有强引用的 entries.1234567891011121314151617@Test public void referenceQueue() throws InterruptedException &#123; Object referent = new Object(); ReferenceQueue&lt;Object&gt; referenceQueue = new ReferenceQueue&lt;Object&gt;(); WeakReference&lt;Object&gt; weakReference = new WeakReference&lt;Object&gt;(referent, referenceQueue); assertFalse(weakReference.isEnqueued()); Reference&lt;? extends Object&gt; polled = referenceQueue.poll(); assertNull(polled); referent = null; System.gc(); assertTrue(weakReference.isEnqueued()); Reference&lt;? extends Object&gt; removed = referenceQueue.remove(); assertNotNull(removed); &#125; Phantom Reference vs Weak ReferencePhantomReference有两个好处， 其一， 它可以让我们准确地知道对象何时被从内存中删除， 这个特性可以被用于一些特殊的需求中(例如 Distributed GC，XWork 和 google-guice 中也使用 PhantomReference 做了一些清理性工作). 其二， 它可以避免 finalization 带来的一些根本性问题, 上文提到 PhantomReference 的唯一作用就是跟踪 referent 何时被 enqueue 到 ReferenceQueue 中,但是 WeakReference 也有对应的功能, 两者的区别到底在哪呢 ?这就要说到 Object 的 finalize 方法, 此方法将在 gc 执行前被调用, 如果某个对象重载了 finalize 方法并故意在方法内创建本身的强引用,这将导致这一轮的 GC 无法回收这个对象并有可能引起任意次 GC， 最后的结果就是明明 JVM 内有很多 Garbage 却 OutOfMemory， 使用 PhantomReference 就可以避免这个问题， 因为 PhantomReference 是在 finalize 方法执行后回收的，也就意味着此时已经不可能拿到原来的引用,也就不会出现上述问题,当然这是一个很极端的例子, 一般不会出现. 对比Soft vs Weak vs Phantom References Type Purpose Use When GCed Implementing Class Strong Reference An ordinary reference. Keeps objects alive as long as they are referenced. normal reference. Any object not pointed to can be reclaimed. default Soft Reference Keeps objects alive provided there’s enough memory. to keep objects alive even after clients have removed their references (memory-sensitive caches), in case clients start asking for them again by key. After a first gc pass, the JVM decides it still needs to reclaim more space. java.lang.ref.SoftReference Weak Reference Keeps objects alive only while they’re in use (reachable) by clients. Containers that automatically delete objects no longer in use. After gc determines the object is only weakly reachable java.lang.ref.WeakReference java.util.WeakHashMap Phantom Reference Lets you clean up after finalization but before the space is reclaimed (replaces or augments the use offinalize()) Special clean up processing After finalization. java.lang.ref.PhantomReference Java GC小结一般的应用程序不会涉及到 Reference 编程， 但是了解这些知识会对理解Java GC 的工作原理以及性能调优有一定帮助, 在实现一些基础性设施比如缓存时也可能会用到， 希望本文能有所帮助.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis的事务和watch]]></title>
    <url>%2F2016%2F09%2F03%2Fredis-transaction-watch%2F</url>
    <content type="text"><![CDATA[redis的事务严格意义来讲,redis的事务和我们理解的传统数据库(如mysql)的事务是不一样的。 redis中的事务定义Redis中的事务（transaction）是一组命令的集合。 事务同命令一样都是Redis的最小执行单位，一个事务中的命令要么都执行，要么都不执行。事务的原理是先将属于一个事务的命令发送给Redis，然后再让Redis依次执行这些命令。 Redis保证一个事务中的所有命令要么都执行，要么都不执行。如果在发送EXEC命令前客户端断线了，则Redis会清空事务队列，事务中的所有命令都不会执行。而一旦客户端发送了EXEC命令，所有的命令就都会被执行，即使此后客户端断线也没关系，因为Redis中已经记录了所有要执行的命令。 除此之外，Redis的事务还能保证一个事务内的命令依次执行而不被其他命令插入。试想客户端A需要执行几条命令，同时客户端B发送了一条命令，如果不使用事务，则客户端B的命令可能会插入到客户端A的几条命令中执行。如果不希望发生这种情况，也可以使用事务。 事务的应用 事务的应用非常普遍，如银行转账过程中A给B汇款，首先系统从A的账户中将钱划走，然后向B的账户增加相应的金额。这两个步骤必须属于同一个事务，要么全执行，要么全不执行。否则只执行第一步，钱就凭空消失了，这显然让人无法接受。 和传统的事务不同 和传统的mysql事务不同的事，即使我们的加钱操作失败,我们也无法在这一组命令中让整个状态回滚到操作之前 事务的错误处理如果一个事务中的某个命令执行出错，Redis会怎样处理呢？要回答这个问题，首先需要知道什么原因会导致命令执行出错。 语法错误语法错误指命令不存在或者命令参数的个数不对。比如：12345678910redis＞MULTIOKredis＞SET key valueQUEUEDredis＞SET key(error)ERR wrong number of arguments for 'set' commandredis＞ errorCOMMAND key(error) ERR unknown command 'errorCOMMAND'redis＞ EXEC(error) EXECABORT Transaction discarded because of previous errors. 跟在MULTI命令后执行了3个命令：一个是正确的命令，成功地加入事务队列；其余两个命令都有语法错误。而只要有一个命令有语法错误，执行EXEC命令后Redis就会直接返回错误，连语法正确的命令也不会执行。 这里需要注意一点：Redis 2.6.5之前的版本会忽略有语法错误的命令，然后执行事务中其他语法正确的命令。就此例而言，SET key value会被执行，EXEC命令会返回一个结果：1) OK。 运行错误运行错误指在命令执行时出现的错误，比如使用散列类型的命令操作集合类型的键，这种错误在实际执行之前Redis是无法发现的，所以在事务里这样的命令是会被Redis接受并执行的。如果事务里的一条命令出现了运行错误，事务里其他的命令依然会继续执行（包括出错命令之后的命令），示例如下：1234567891011121314redis＞MULTIOKredis＞SET key 1QUEUEDredis＞SADD key 2QUEUEDredis＞SET key 3QUEUEDredis＞EXEC1) OK2) (error) ERR Operation against a key holding the wrong kind of value3) OKredis＞GET key"3" 可见虽然SADD key 2出现了错误，但是SET key 3依然执行了。 Redis的事务没有关系数据库事务提供的回滚（rollback）功能。为此开发者必须在事务执行出错后自己收拾剩下的摊子（将数据库复原回事务执行前的状态等,这里我们一般采取日志记录然后业务补偿的方式来处理，但是一般情况下，在redis做的操作不应该有这种强一致性要求的需求，我们认为这种需求为不合理的设计）。 Watch命令大家可能知道redis提供了基于incr命令来操作一个整数型数值的原子递增，那么我们假设如果redis没有这个incr命令，我们该怎么实现这个incr的操作呢？ 那么我们下面的正主watch就要上场了。 如何使用watch命令正常情况下我们想要对一个整形数值做修改是这么做的(伪代码实现)：123val = GET mykeyval = val + 1SET mykey $val 但是上述的代码会出现一个问题,因为上面吧正常的一个incr(原子递增操作)分为了两部分,那么在多线程(分布式)环境中，这个操作就有可能不再具有原子性了。 研究过java的juc包的人应该都知道cas，那么redis也提供了这样的一个机制，就是利用watch命令来实现的。 watch命令描述 WATCH命令可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行。监控一直持续到EXEC命令（事务中的命令是在EXEC之后才执行的，所以在MULTI命令后可以修改WATCH监控的键值） 利用watch实现incr具体做法如下:123456WATCH mykeyval = GET mykeyval = val + 1MULTISET mykey $valEXEC 和此前代码不同的是，新代码在获取mykey的值之前先通过WATCH命令监控了该键，此后又将set命令包围在事务中，这样就可以有效的保证每个连接在执行EXEC之前，如果当前连接获取的mykey的值被其它连接的客户端修改，那么当前连接的EXEC命令将执行失败。这样调用者在判断返回值后就可以获悉val是否被重新设置成功。 注意点由于WATCH命令的作用只是当被监控的键值被修改后阻止之后一个事务的执行，而不能保证其他客户端不修改这一键值，所以在一般的情况下我们需要在EXEC执行失败后重新执行整个函数。 执行EXEC命令后会取消对所有键的监控，如果不想执行事务中的命令也可以使用UNWATCH命令来取消监控。 实现一个hsetNX函数我们实现的hsetNX这个功能是：仅当字段存在时才赋值。 为了避免竞态条件我们使用watch和事务来完成这一功能（伪代码）：123456789WATCH key isFieldExists = HEXISTS key, field if isFieldExists is 1 MULTI HSET key, field, value EXEC else UNWATCH return isFieldExists 在代码中会判断要赋值的字段是否存在，如果字段不存在的话就不执行事务中的命令，但需要使用UNWATCH命令来保证下一个事务的执行不会受到影响。 原文地址 http://www.jianshu.com/p/361cb9cd13d5]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出MappedByteBuffer]]></title>
    <url>%2F2016%2F08%2F23%2Fjava-mapped-byte-buffer%2F</url>
    <content type="text"><![CDATA[前言java io操作中通常采用BufferedReader，BufferedInputStream等带缓冲的IO类处理大文件，不过java nio中引入了一种基于MappedByteBuffer操作大文件的方式，其读写性能极高，本文会介绍其性能如此高的内部实现原理。 内存管理在深入MappedByteBuffer之前，先看看计算机内存管理的几个术语： MMC：CPU的内存管理单元。 物理内存：即内存条的内存空间。 虚拟内存：计算机系统内存管理的一种技术。它使得应用程序认为它拥有连续的可用的内存（一个连续完整的地址空间），而实际上，它通常是被分隔成多个物理内存碎片，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。 页面文件：操作系统反映构建并使用虚拟内存的硬盘空间大小而创建的文件，在windows下，即pagefile.sys文件，其存在意味着物理内存被占满后，将暂时不用的数据移动到硬盘上。 缺页中断：当程序试图访问已映射在虚拟地址空间中但未被加载至物理内存的一个分页时，由MMC发出的中断。如果操作系统判断此次访问是有效的，则尝试将相关的页从虚拟内存文件中载入物理内存。 为什么会有虚拟内存和物理内存的区别？如果正在运行的一个进程，它所需的内存是有可能大于内存条容量之和的，如内存条是256M，程序却要创建一个2G的数据区，那么所有数据不可能都加载到内存（物理内存），必然有数据要放到其他介质中（比如硬盘），待进程需要访问那部分数据时，再调度进入物理内存。 什么是虚拟内存地址和物理内存地址？假设你的计算机是32位，那么它的地址总线是32位的，也就是它可以寻址0~0xFFFFFFFF（4G）的地址空间，但如果你的计算机只有256M的物理内存0x~0x0FFFFFFF（256M），同时你的进程产生了一个不在这256M地址空间中的地址，那么计算机该如何处理呢？回答这个问题前，先说明计算机的内存分页机制。 计算机会对虚拟内存地址空间（32位为4G）进行分页产生页（page），对物理内存地址空间（假设256M）进行分页产生页帧（page frame），页和页帧的大小一样，所以虚拟内存页的个数势必要大于物理内存页帧的个数。在计算机上有一个页表（page table），就是映射虚拟内存页到物理内存页的，更确切的说是页号到页帧号的映射，而且是一对一的映射。问题来了，虚拟内存页的个数 &gt; 物理内存页帧的个数，岂不是有些虚拟内存页的地址永远没有对应的物理内存地址空间？不是的，操作系统是这样处理的。操作系统有个页面失效（page fault）功能。操作系统找到一个最少使用的页帧，使之失效，并把它写入磁盘，随后把需要访问的页放到页帧中，并修改页表中的映射，保证了所有的页都会被调度。 现在来看看什么是虚拟内存地址和物理内存地址： 虚拟内存地址：由页号（与页表中的页号关联）和偏移量（页的小大，即这个页能存多少数据）组成。 举个例子，有一个虚拟地址它的页号是4，偏移量是20，那么他的寻址过程是这样的：首先到页表中找到页号4对应的页帧号（比如为8），如果页不在内存中，则用失效机制调入页，接着把页帧号和偏移量传给MMC组成一个物理上真正存在的地址，最后就是访问物理内存的数据了。 MappedByteBuffer是什么从继承结构上看，MappedByteBuffer继承自ByteBuffer，内部维护了一个逻辑地址address。 示例通过MappedByteBuffer读取文件1234567891011121314151617181920212223public class MappedByteBufferTest &#123; public static void main(String[] args) &#123; File file = new File("D://data.txt"); long len = file.length(); byte[] ds = new byte[(int) len]; try &#123; MappedByteBuffer mappedByteBuffer = new RandomAccessFile(file, "r") .getChannel() .map(FileChannel.MapMode.READ_ONLY, 0, len); for (int offset = 0; offset &lt; len; offset++) &#123; byte b = mappedByteBuffer.get(); ds[offset] = b; &#125; Scanner scan = new Scanner(new ByteArrayInputStream(ds)).useDelimiter(" "); while (scan.hasNext()) &#123; System.out.print(scan.next() + " "); &#125; &#125; catch (IOException e) &#123;&#125; &#125;&#125; map过程FileChannel提供了map方法把文件映射到虚拟内存，通常情况可以映射整个文件，如果文件比较大，可以进行分段映射。 FileChannel中的几个变量： MapMode mode：内存映像文件访问的方式，共三种： MapMode.READ_ONLY：只读，试图修改得到的缓冲区将导致抛出异常。 MapMode.READ_WRITE：读/写，对得到的缓冲区的更改最终将写入文件；但该更改对映射到同一文件的其他程序不一定是可见的。 MapMode.PRIVATE：私用，可读可写,但是修改的内容不会写入文件，只是buffer自身的改变，这种能力称之为”copy on write”。 position：文件映射时的起始位置。 allocationGranularity：Memory allocation size for mapping buffers，通过native函数initIDs初始化。 接下去通过分析源码，了解一下map过程的内部实现。 通过RandomAccessFile获取FileChannel。 12345678public final FileChannel getChannel() &#123;synchronized (this) &#123; if (channel == null) &#123; channel = FileChannelImpl.open(fd, path, true, rw, this); &#125; return channel;&#125;&#125; 上述实现可以看出，由于synchronized ，只有一个线程能够初始化FileChannel。 通过FileChannel.map方法，把文件映射到虚拟内存，并返回逻辑地址address，实现如下： 123456789101112131415161718192021222324252627282930313233343536**只保留了核心代码**public MappedByteBuffer map(MapMode mode, long position, long size) throws IOException &#123; int pagePosition = (int)(position % allocationGranularity); long mapPosition = position - pagePosition; long mapSize = size + pagePosition; try &#123; addr = map0(imode, mapPosition, mapSize); &#125; catch (OutOfMemoryError x) &#123; System.gc(); try &#123; Thread.sleep(100); &#125; catch (InterruptedException y) &#123; Thread.currentThread().interrupt(); &#125; try &#123; addr = map0(imode, mapPosition, mapSize); &#125; catch (OutOfMemoryError y) &#123; // After a second OOME, fail throw new IOException("Map failed", y); &#125; &#125; int isize = (int)size; Unmapper um = new Unmapper(addr, mapSize, isize, mfd); if ((!writable) || (imode == MAP_RO)) &#123; return Util.newMappedByteBufferR(isize, addr + pagePosition, mfd, um); &#125; else &#123; return Util.newMappedByteBuffer(isize, addr + pagePosition, mfd, um); &#125;&#125; 上述代码可以看出，最终map通过native函数map0完成文件的映射工作。 如果第一次文件映射导致OOM，则手动触发垃圾回收，休眠100ms后再次尝试映射，如果失败，则抛出异常。 通过newMappedByteBuffer方法初始化MappedByteBuffer实例，不过其最终返回的是DirectByteBuffer的实例，实现如下： 12345678910111213141516171819202122232425static MappedByteBuffer newMappedByteBuffer(int size, long addr, FileDescriptor fd, Runnable unmapper) &#123;MappedByteBuffer dbb;if (directByteBufferConstructor == null)initDBBConstructor();dbb = (MappedByteBuffer)directByteBufferConstructor.newInstance(new Object[] &#123; new Integer(size), new Long(addr), fd, unmapper &#125;return dbb;&#125;// 访问权限private static void initDBBConstructor() &#123;AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123;public Void run() &#123; Class&lt;?&gt; cl = Class.forName("java.nio.DirectByteBuffer"); Constructor&lt;?&gt; ctor = cl.getDeclaredConstructor( new Class&lt;?&gt;[] &#123; int.class, long.class, FileDescriptor.class, Runnable.class &#125;); ctor.setAccessible(true); directByteBufferConstructor = ctor;&#125;&#125;);&#125; 由于FileChannelImpl和DirectByteBuffer不在同一个包中，所以有权限访问问题，通过AccessController类获取DirectByteBuffer的构造器进行实例化。 DirectByteBuffer是MappedByteBuffer的一个子类，其实现了对内存的直接操作。 get过程MappedByteBuffer的get方法最终通过DirectByteBuffer.get方法实现的。123456789public byte get() &#123; return ((unsafe.getByte(ix(nextGetIndex()))));&#125;public byte get(int i) &#123; return ((unsafe.getByte(ix(checkIndex(i)))));&#125;private long ix(int i) &#123; return address + (i &lt;&lt; 0);&#125; map0()函数返回一个地址address，这样就无需调用read或write方法对文件进行读写，通过address就能够操作文件。底层采用unsafe.getByte方法，通过（address + 偏移量）获取指定内存的数据。 第一次访问address所指向的内存区域，导致缺页中断，中断响应函数会在交换区中查找相对应的页面，如果找不到（也就是该文件从来没有被读入内存的情况），则从硬盘上将文件指定页读取到物理内存中（非jvm堆内存）。 如果在拷贝数据时，发现物理内存不够用，则会通过虚拟内存机制（swap）将暂时不用的物理页面交换到硬盘的虚拟内存中。 性能分析从代码层面上看，从硬盘上将文件读入内存，都要经过文件系统进行数据拷贝，并且数据拷贝操作是由文件系统和硬件驱动实现的，理论上来说，拷贝数据的效率是一样的。但是通过内存映射的方法访问硬盘上的文件，效率要比read和write系统调用高，这是为什么？ read()是系统调用，首先将文件从硬盘拷贝到内核空间的一个缓冲区，再将这些数据拷贝到用户空间，实际上进行了两次数据拷贝； map()也是系统调用，但没有进行数据拷贝，当缺页中断发生时，直接将文件从硬盘拷贝到用户空间，只进行了一次数据拷贝。 所以，采用内存映射的读写效率要比传统的read/write性能高。 总结 MappedByteBuffer使用虚拟内存，因此分配(map)的内存大小不受JVM的-Xmx参数限制，但是也是有大小限制的。 如果当文件超出1.5G限制时，可以通过position参数重新map文件后面的内容。 MappedByteBuffer在处理大文件时的确性能很高，但也存在一些问题，如内存占用、文件关闭不确定，被其打开的文件只有在垃圾回收的才会被关闭，而且这个时间点是不确定的。javadoc中也提到：A mapped byte buffer and the file mapping that it represents remain* valid until the buffer itself is garbage-collected. 原文：http://www.jianshu.com/p/f90866dcbffc]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程(四) Callable、Future和FutureTask浅析]]></title>
    <url>%2F2016%2F08%2F22%2Fjava-thread-4%2F</url>
    <content type="text"><![CDATA[通过前面几篇的学习，我们知道创建线程的方式有两种，一种是实现Runnable接口，另一种是继承Thread，但是这两种方式都有个缺点，那就是在任务执行完成之后无法获取返回结果，那如果我们想要获取返回结果该如何实现呢？还记上一篇Executor框架结构中提到的Callable接口和Future接口吗？，是的，从JAVA SE 5.0开始引入了Callable和Future，通过它们构建的线程，在任务执行完成后就可以获取执行结果，今天我们就来聊聊线程创建的第三种方式，那就是实现Callable接口。 Callable接口我们先回顾一下java.lang.Runnable接口，就声明了run(),其返回值为void，当然就无法获取结果了。123public interface Runnable &#123; public abstract void run(); &#125; 而Callable的接口定义如下123public interface Callable&lt;V&gt; &#123; V call() throws Exception; &#125; 该接口声明了一个名称为call()的方法，同时这个方法可以有返回值V，也可以抛出异常。嗯，对该接口我们先了解这么多就行，下面我们来说明如何使用，前篇文章我们说过，无论是Runnable接口的实现类还是Callable接口的实现类，都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行，ThreadPoolExecutor或ScheduledThreadPoolExecutor都实现了ExcutorService接口，而因此Callable需要和Executor框架中的ExcutorService结合使用，我们先看看ExecutorService提供的方法：123&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); Future&lt;?&gt; submit(Runnable task); 第一个方法：submit提交一个实现Callable接口的任务，并且返回封装了异步计算结果的Future。第二个方法：submit提交一个实现Runnable接口的任务，并且指定了在调用Future的get方法时返回的result对象。第三个方法：submit提交一个实现Runnable接口的任务，并且返回封装了异步计算结果的Future。因此我们只要创建好我们的线程对象（实现Callable接口或者Runnable接口），然后通过上面3个方法提交给线程池去执行即可。还有点要注意的是，除了我们自己实现Callable对象外，我们还可以使用工厂类Executors来把一个Runnable对象包装成Callable对象。Executors工厂类提供的方法如下：12public static Callable&lt;Object&gt; callable(Runnable task) public static &lt;T&gt; Callable&lt;T&gt; callable(Runnable task, T result) Future接口Future接口是用来获取异步计算结果的，说白了就是对具体的Runnable或者Callable对象任务执行的结果进行获取(get()),取消(cancel()),判断是否完成等操作。我们看看Future接口的源码：1234567public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; &#125; 方法解析：V get() ：获取异步执行的结果，如果没有结果可用，此方法会阻塞直到异步计算完成。V get(Long timeout , TimeUnit unit) ：获取异步执行结果，如果没有结果可用，此方法会阻塞，但是会有时间限制，如果阻塞时间超过设定的timeout时间，该方法将抛出异常。boolean isDone() ：如果任务执行结束，无论是正常结束或是中途取消还是发生异常，都返回true。boolean isCanceller() ：如果任务完成前被取消，则返回true。boolean cancel(boolean mayInterruptRunning) ：如果任务还没开始，执行cancel(…)方法将返回false；如果任务已经启动，执行cancel(true)方法将以中断执行此任务线程的方式来试图停止任务，如果停止成功，返回true；当任务已经启动，执行cancel(false)方法将不会对正在执行的任务线程产生影响(让线程正常执行到完成)，此时返回false；当任务已经完成，执行cancel(…)方法将返回false。mayInterruptRunning参数表示是否中断执行中的线程。通过方法分析我们也知道实际上Future提供了3种功能：（1）能够中断执行中的任务（2）判断任务是否执行完成（3）获取任务执行完成后额结果。但是我们必须明白Future只是一个接口，我们无法直接创建对象，因此就需要其实现类FutureTask登场啦。 FutureTask类我们先来看看FutureTask的实现1public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123;...&#125; FutureTask类实现了RunnableFuture接口，我们看一下RunnableFuture接口的实现：123public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; void run(); &#125; 分析：FutureTask除了实现了Future接口外还实现了Runnable接口，因此FutureTask也可以直接提交给Executor执行。 当然也可以调用线程直接执行（FutureTask.run()）。接下来我们根据FutureTask.run()的执行时机来分析其所处的3种状态： 未启动，FutureTask.run()方法还没有被执行之前，FutureTask处于未启动状态，当创建一个FutureTask，而且没有执行FutureTask.run()方法前，这个FutureTask也处于未启动状态。 已启动，FutureTask.run()被执行的过程中，FutureTask处于已启动状态。 已完成，FutureTask.run()方法执行完正常结束，或者被取消或者抛出异常而结束，FutureTask都处于完成状态。 下面我们再来看看FutureTask的方法执行示意图（方法和Future接口基本是一样的，这里就不过多描述了）分析： 当FutureTask处于未启动或已启动状态时，如果此时我们执行FutureTask.get()方法将导致调用线程阻塞；当FutureTask处于已完成状态时，执行FutureTask.get()方法将导致调用线程立即返回结果或者抛出异常。 当FutureTask处于未启动状态时，执行FutureTask.cancel()方法将导致此任务永远不会执行。当FutureTask处于已启动状态时，执行cancel(true)方法将以中断执行此任务线程的方式来试图停止任务，如果任务取消成功，cancel(…)返回true；但如果执行cancel(false)方法将不会对正在执行的任务线程产生影响(让线程正常执行到完成)，此时cancel(…)返回false。 当任务已经完成，执行cancel(…)方法将返回false。 最后我们给出FutureTask的两种构造函数：1234public FutureTask(Callable&lt;V&gt; callable) &#123; &#125; public FutureTask(Runnable runnable, V result) &#123; &#125; Callable/Future/FutureTask的使用通过上面的介绍，我们对Callable，Future，FutureTask都有了比较清晰的了解了，那么它们到底有什么用呢？我们前面说过通过这样的方式去创建线程的话，最大的好处就是能够返回结果，加入有这样的场景，我们现在需要计算一个数据，而这个数据的计算比较耗时，而我们后面的程序也要用到这个数据结果，那么这个时Callable岂不是最好的选择？我们可以开设一个线程去执行计算，而主线程继续做其他事，而后面需要使用到这个数据时，我们再使用Future获取不就可以了吗？下面我们就来编写一个这样的实例 使用Callable+Future获取执行结果Callable实现类如下：12345678910111213141516171819package com.sky.code.thread;import java.util.concurrent.Callable;public class CallableDemo implements Callable&lt;Integer&gt; &#123; private int sum; @Override public Integer call() throws Exception &#123; System.out.println("Callable子线程开始计算啦！"); Thread.sleep(2000); for(int i=0 ;i&lt;5000;i++)&#123; sum=sum+i; &#125; System.out.println("Callable子线程计算结束！"); return sum; &#125;&#125; Callable执行测试类如下：1234567891011121314151617181920212223242526272829303132333435package com.sky.code.thread;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Future;public class CallableTest &#123; public static void main(String[] args) &#123; //创建线程池 ExecutorService es = Executors.newSingleThreadExecutor(); //创建Callable对象任务 CallableDemo calTask=new CallableDemo(); //提交任务并获取执行结果 Future&lt;Integer&gt; future =es.submit(calTask); //关闭线程池 es.shutdown(); try &#123; Thread.sleep(2000); System.out.println("主线程在执行其他任务"); if(future.get()!=null)&#123; //输出获取到的结果 System.out.println("future.get()--&gt;"+future.get()); &#125;else&#123; //输出获取到的结果 System.out.println("future.get()未获取到结果"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println("主线程在执行完成"); &#125;&#125; 执行结果：12345Callable子线程开始计算啦！主线程在执行其他任务Callable子线程计算结束！future.get()--&gt;12497500主线程在执行完成 使用Callable+FutureTask获取执行结果1234567891011121314151617181920212223242526272829303132333435363738package com.sky.code.thread;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.FutureTask;public class CallableTest1 &#123; public static void main(String[] args) &#123; //创建线程池 ExecutorService es = Executors.newSingleThreadExecutor(); //创建Callable对象任务 CallableDemo calTask=new CallableDemo(); //创建FutureTask FutureTask&lt;Integer&gt; futureTask=new FutureTask&lt;&gt;(calTask); //执行任务 es.submit(futureTask); //关闭线程池 es.shutdown(); try &#123; Thread.sleep(2000); System.out.println("主线程在执行其他任务"); if(futureTask.get()!=null)&#123; //输出获取到的结果 System.out.println("futureTask.get()--&gt;"+futureTask.get()); &#125;else&#123; //输出获取到的结果 System.out.println("futureTask.get()未获取到结果"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println("主线程在执行完成"); &#125;&#125; 执行结果：12345Callable子线程开始计算啦！主线程在执行其他任务Callable子线程计算结束！futureTask.get()--&gt;12497500主线程在执行完成 使用thread+FutureTask获取执行结果上面两个例子都是用线程池，所以以下用一个线程来举个例子：123456789101112131415161718192021222324252627282930package com.sky.code.thread;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;public class FutureTest &#123; public static void main(String[] args)&#123; FutureTask&lt;String&gt; f=new FutureTask&lt;&gt;(() -&gt; &#123; int i=0; for (int j=0;j&lt;100000;j++)&#123; i++; &#125; Thread.sleep(5000); return i+""; &#125;); Thread thread=new Thread(f); thread.start(); System.out.println("主线程在执行其他任务"); try &#123; System.out.println("future task result : "+f.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 结果12主线程在执行其他任务future task result : 100000 参考 http://blog.csdn.net/javazejian/article/details/50896505代码 https://github.com/ejunjsh/java-code/tree/master/src/main/java/com/sky/code/thread]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程(三) 线程池]]></title>
    <url>%2F2016%2F08%2F21%2Fjava-thread-3%2F</url>
    <content type="text"><![CDATA[记录，汇总 Executor框架浅析首先我们得明白一个 问题，为什么需要线程池？在java中，使用线程来执行异步任务时，线程的创建和销毁需要一定的开销，如果我们为每一个任务创建一个新的线程来执行的话，那么这些线程的创建与销毁将消耗大量的计算资源。同时为每一个任务创建一个新线程来执行，这样的方式可能会使处于高负荷状态的应用最终崩溃。所以线程池的出现为解决这个问题带来曙光。我们将在线程池中创建若干条线程，当有任务需要执行时就从该线程池中获取一条线程来执行任务，如果一时间任务过多，超出线程池的线程数量，那么后面的线程任务就进入一个等待队列进行等待，直到线程池有线程处于空闲时才从等待队列获取要执行的任务进行处理，以此循环…..这样就大大减少了线程创建和销毁的开销，也会缓解我们的应用处于超负荷时的情况。 Executor框架的两级调度模型在java线程启动时会创建一个本地操作系统线程，当该java线程终止时，这个操作系统线程也会被回收。而每一个java线程都会被一对一映射为本地操作系统的线程，操作系统会调度所有的线程并将它们分别给可用的CPU。而所谓的映射方式是这样实现的，在上层，java多线程程序通过把应用分为若干个任务，然后使用用户级的调度器（Executor框架）将这些任务映射为固定数量的线程；在底层，操作系统内核将这些线程映射到硬件处理器上。这样种两级调度模型如下图所示：从图中我们可以看出，应用程序通过Executor框架控制上层的调度，而下层的调度由操作系统内核控制，下层的调度不受应用程序的控制。 Executor框架的结构Executor框架的结构主要包括3个部分 任务：包括被执行任务需要实现的接口：Runnable接口或Callable接口 任务的执行：包括任务执行机制的核心接口Executor，以及继承自Executor的EexcutorService接口。Exrcutor有两个关键类实现了ExecutorService接口（ThreadPoolExecutor和ScheduledThreadPoolExecutor）。 异步计算的结果：包括接口Future和实现Future接口的FutureTask类下面我们通过一个UML图来认识一下这些类间的关系： Extecutor是一个接口，它是Executor框架的基础，它将任务的提交与任务的执行分离开来。ThreadPoolExecutor是线程池的核心实现类，用来执行被提交的任务。ScheduledThreadPoolExecutor是一个实现类，可以在给定的延迟后运行命令，或者定期执行命令。ScheduledThreadPoolExecutor比Timer更灵活，功能更强大。Future接口和实现Future接口的FutureTask类，代表异步计算的结果。Runnable接口和Callable接口的实现类，都可以被ThreadPoolExecutor或者ScheduledThreadPoolExecutor执行。区别就是Runnable无法返回执行结果，而Callable可以返回执行结果。下面我们通过一张图来理解它们间的执行关系：分析说明：主线程首先创建实现Runnable或Callable接口的任务对象，工具类Executors可以把一个Runnable对象封装为一个Callable对象,使用如下两种方式：Executors.callable(Runnable task)或者Executors.callable(Runnable task,Object resule)。然后可以把Runnable对象直接提交给ExecutorService执行，方法为ExecutorService.execute(Runnable command)；或者也可以把Runnable对象或者Callable对象提交给ExecutorService执行，方法为ExecutorService.submit(Runnable task)或ExecutorService.submit(Callable task)。这里需要注意的是如果执行ExecutorService.submit(…),ExecutorService将返回一个实现Future接口的对象（其实就是FutureTask）。当然由于FutureTask实现了Runnable接口，我们也可以直接创建FutureTask，然后提交给ExecutorService执行。到此Executor框架的主要体系结构我们都介绍完了，我们对此有了大概了解后，下面我们就重点聊聊两个主要的线程池实现类。ThreadPoolExecutor浅析ThreadPoolExecutor是线程的真正实现，通常使用工厂类Executors来创建，但它的构造方法提供了一系列参数来配置线程池，下面我们就先介绍ThreadPoolExecutor的构造方法中各个参数的含义。123456789public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, defaultHandler); &#125; corePoolSize：线程池的核心线程数，默认情况下，核心线程数会一直在线程池中存活，即使它们处理闲置状态。如果将ThreadPoolExecutor的allowCoreThreadTimeOut属性设置为true，那么闲置的核心线程在等待新任务到来时会执行超时策略，这个时间间隔由keepAliveTime所指定，当等待时间超过keepAliveTime所指定的时长后，核心线程就会被终止。 maximumPoolSize：线程池所能容纳的最大线程数量，当活动线程数到达这个数值后，后续的新任务将会被阻塞。 keepAliveTime：非核心线程闲置时的超时时长，超过这个时长，非核心线程就会被回收。当ThreadPoolExecutor的allowCoreThreadTimeOut属性设置为true时，keepAliveTime同样会作用于核心线程。 unit：用于指定keepAliveTime参数的时间单位，这是一个枚举，常用的有TimeUnit.MILLISECONDS(毫秒)，TimeUnit.SECONDS(秒)以及TimeUnit.MINUTES(分钟)等。 workQueue：线程池中的任务队列，通过线程池的execute方法提交Runnable对象会存储在这个队列中。 threadFactory：线程工厂，为线程池提供创建新线程的功能。ThreadFactory是一个接口，它只有一个方法：Thread newThread（Runnable r）。 除了上面的参数外还有个不常用的参数，RejectExecutionHandler，这个参数表示当ThreadPoolExecutor已经关闭或者ThreadPoolExecutor已经饱和时（达到了最大线程池大小而且工作队列已经满），execute方法将会调用Handler的rejectExecution方法来通知调用者，默认情况 下是抛出一个RejectExecutionException异常。了解完相关构造函数的参数，我们再来看看ThreadPoolExecutor执行任务时的大致规则： 如果线程池的数量还未达到核心线程的数量，那么会直接启动一个核心线程来执行任务 如果线程池中的线程数量已经达到或者超出核心线程的数量，那么任务会被插入到任务队列中排队等待执行。 如果在步骤（2）中无法将任务插入到任务队列中，这往往是由于任务队列已满，这个时候如果线程数量未达到线程池规定的最大值，那么会立刻启动一个非核心线程来执行任务。 如果在步骤（3）中线程数量已经达到线程池规定的最大值，那么就会拒绝执行此任务，ThreadPoolExecutor会调用RejectExecutionHandler的rejectExecution方法来通知调用者。 到此ThreadPoolExecutor的详细配置了解完了，ThreadPoolExecutor的执行规则也了解完了，那么接下来我们就来介绍3种常见的线程池，它们都直接或者间接地通过配置ThreadPoolExecutor来实现自己的功能特性，这个3种线程池分别是FixedThreadPool，CachedThreadPool，ScheduledThreadPool以及SingleThreadExecutor。 FixedThreadPoolFixedThreadPool模式会使用一个优先固定数目的线程来处理若干数目的任务。规定数目的线程处理所有任务，一旦有线程处理完了任务就会被用来处理新的任务(如果有的话)。FixedThreadPool模式下最多的线程数目是一定的。创建FixedThreadPool对象代码如下：1ExecutorService fixedThreadPool=Executors.newFixedThreadPool(5); 我们来看看FixedThreadPool创建方法源码：12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); &#125; FixedThreadPool的corePoolSize和maximumPoolSize参数都被设置为nThreads。当线程池中的线程数量大于corePoolSize时，keepAliveTime为非核心空闲线程等待新任务的最长时间，超过这个时间后非核心线程将被终止，这里keepAliveTime设置为0L，就说明非核心线程会立即被终止。事实上这里也没有非核心线程创建，因为核心线程数和最大线程数都一样的。下面我们来看看FixedThreadPool的execute()方法的运行流程：分析： 如果当前运行线程数少corePoolSize，则创建一个新的线程来执行任务。 如果当前线程池的运行线程数等于corePoolSize，那么后面提交的任务将加入LinkedBlockingQueue。 线程在执行完图中的1后，会在循环中反复从LinkedBlockingQueue获取任务来执行。 这里还有点要说明的是FixedThreadPool使用的是无界队列LinkedBlockingQueue作为线程池的工作队列（队列容量为Integer.MAX_VALUE）。使用该队列作为工作队列会对线程池产生如下影响 当前线程池中的线程数量达到corePoolSize后，新的任务将在无界队列中等待。 由于我们使用的是无界队列，所以参数maximumPoolSize和keepAliveTime无效。 由于使用无界队列，运行中的FixedThreadPool不会拒绝任务（当然此时是未执行shutdown和shutdownNow方法），所以不会去调用RejectExecutionHandler的rejectExecution方法抛出异常。 下面我们给出案例，该案例来自java编程思想一书：123456789101112131415161718192021public class LiftOff implements Runnable&#123; protected int countDown = 10; //Default private static int taskCount = 0; private final int id = taskCount++; public LiftOff() &#123;&#125; public LiftOff(int countDown) &#123; this.countDown = countDown; &#125; public String status() &#123; return "#" + id + "(" + (countDown &gt; 0 ? countDown : "LiftOff!") + ") "; &#125; @Override public void run() &#123; while(countDown-- &gt; 0) &#123; System.out.print(status()); Thread.yield(); &#125; &#125; &#125; 声明一个Runnable对象，使用FixedThreadPool执行任务如下：12345678910public class FixedThreadPool &#123; public static void main(String[] args) &#123; //三个线程来执行五个任务 ExecutorService exec = Executors.newFixedThreadPool(3); for(int i = 0; i &lt; 5; i++) &#123; exec.execute(new LiftOff()); &#125; exec.shutdown(); &#125; &#125; CachedThreadPoolCachedThreadPool首先会按照需要创建足够多的线程来执行任务(Task)。随着程序执行的过程，有的线程执行完了任务，可以被重新循环使用时，才不再创建新的线程来执行任务。创建方式：1ExecutorService cachedThreadPool=Executors.newCachedThreadPool(); 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); &#125; 从该静态方法，我们可以看到CachedThreadPool的corePoolSize被设置为0，而maximumPoolSize被设置Integer.MAX_VALUE，即maximumPoolSize是无界的，而keepAliveTime被设置为60L，单位为妙。也就是空闲线程等待时间最长为60秒，超过该时间将会被终止。而且在这里CachedThreadPool使用的是没有容量的SynchronousQueue作为线程池的工作队列，但其maximumPoolSize是无界的，也就是意味着如果主线程提交任务的速度高于maximumPoolSize中线程处理任务的速度时CachedThreadPool将会不断的创建新的线程，在极端情况下，CachedThreadPool会因为创建过多线程而耗尽CPU和内存资源。CachedThreadPool的execute()方法的运行流程：分析： 首先执行SynchronousQueue.offer(Runnable task)，添加一个任务。如果当前CachedThreadPool中有空闲线程正在执行SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS),其中NANOSECONDS是毫微秒即十亿分之一秒（就是微秒/1000），那么主线程执行offer操作与空闲线程执行poll操作配对成功，主线程把任务交给空闲线程执行，execute()方法执行完成，否则进入第（2）步。 当CachedThreadPool初始线程数为空时，或者当前没有空闲线程，将没有线程去执行SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)。这样的情况下，步骤（1）将会失败，此时CachedThreadPool会创建一个新的线程来执行任务，execute()方法执行完成。 在步骤（2）中创建的新线程将任务执行完成后，会执行SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)，这个poll操作会让空闲线程最多在SynchronousQueue中等待60秒，如果60秒内主线程提交了一个新任务，那么这个空闲线程将会执行主线程提交的新任务，否则，这个空闲线程将被终止。由于空闲60秒的空闲线程会被终止，因此长时间保持空闲的 CachedThreadPool是不会使用任何资源的。 根据前面的分析我们知道SynchronousQueue是一个没有容量的阻塞队列（其实个人认为是相对应时间而已的没有容量，因为时间到空闲线程就会被移除）。每个插入操作必须等到一个线程与之对应。CachedThreadPool使用SynchronousQueue，把主线程的任务传递给空闲线程执行。流程如下：CachedThreadPool使用的案例代码如下：123456789public class CachedThreadPool &#123; public static void main(String[] args) &#123; ExecutorService exec = Executors.newCachedThreadPool(); for(int i = 0; i &lt; 10; i++) &#123; exec.execute(new LiftOff()); &#125; exec.shutdown(); &#125; &#125; SingleThreadExecutorSingleThreadExecutor模式只会创建一个线程。它和FixedThreadPool比较类似，不过线程数是一个。如果多个任务被提交给SingleThreadExecutor的话，那么这些任务会被保存在一个队列中，并且会按照任务提交的顺序，一个先执行完成再执行另外一个线程。SingleThreadExecutor模式可以保证只有一个任务会被执行。这种特点可以被用来处理共享资源的问题而不需要考虑同步的问题。创建方式：1ExecutorService singleThreadExecutor=Executors.newSingleThreadExecutor(); 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); &#125; 从静态方法可以看出SingleThreadExecutor的corePoolSize和maximumPoolSize被设置为1，其他参数则与FixedThreadPool相同。SingleThreadExecutor使用的工作队列也是无界队列LinkedBlockingQueue。由于SingleThreadExecutor采用无界队列的对线程池的影响与FixedThreadPool一样，这里就不过多描述了。同样的我们先来看看其运行流程：分析： 如果当前线程数少于corePoolSize即线程池中没有线程运行，则创建一个新的线程来执行任务。 在线程池的线程数量等于corePoolSize时，将任务加入到LinkedBlockingQueue。 线程执行完成（1）中的任务后，会在一个无限循环中反复从LinkedBlockingQueue获取任务来执行。 SingleThreadExecutor使用的案例代码如下：12345678public class SingleThreadExecutor &#123; public static void main(String[] args) &#123; ExecutorService exec = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 2; i++) &#123; exec.execute(new LiftOff()); &#125; &#125; &#125; 各自的适用场景 FixedThreadPool：适用于为了满足资源管理需求，而需要限制当前线程的数量的应用场景，它适用于负载比较重的服务器。 SingleThreadExecutor：适用于需要保证执行顺序地执行各个任务；并且在任意时间点，不会有多个线程是活动的场景。 CachedThreadPool：大小无界的线程池，适用于执行很多的短期异步任务的小程序，或者负载较轻的服务器。 ScheduledThreadPoolExecutor浅析ScheduledThreadPoolExecutor执行机制分析ScheduledThreadPoolExecutor继承自ThreadPoolExecutor。它主要用来在给定的延迟之后执行任务，或者定期执行任务。ScheduledThreadPoolExecutor的功能与Timer类似，但比Timer更强大，更灵活，Timer对应的是单个后台线程，而ScheduledThreadPoolExecutor可以在构造函数中指定多个对应的后台线程数。接下来我们先来了解一下ScheduledThreadPoolExecutor的运行机制：分析：DelayQueue是一个无界队列，所以ThreadPoolExecutor的maximumPoolSize在ScheduledThreadPoolExecutor中无意义。ScheduledThreadPoolExecutor的执行主要分为以下两个部分 当调用ScheduledThreadPoolExecutor的scheduleAtFixedRate()方法或者scheduleWithFixedDelay()方法时，会向ScheduledThreadPoolExecutor的DelayQueue添加一个实现了RunnableScheduledFuture接口的ScheduleFutureTask。 线程池中的线程从DelayQueue中获取ScheduleFutureTask，然后执行任务。如何创建ScheduledThreadPoolExecutor？ScheduledThreadPoolExecutor通常使用工厂类Executors来创建，Executors可以创建两种类型的ScheduledThreadPoolExecutor，如下： ScheduledThreadPoolExecutor：可以执行并行任务也就是多条线程同时执行。 SingleThreadScheduledExecutor：可以执行单条线程。 创建ScheduledThreadPoolExecutor的方法构造如下：12public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize, ThreadFactory threadFactory) 创建SingleThreadScheduledExecutor的方法构造如下：12public static ScheduledExecutorService newSingleThreadScheduledExecutor() public static ScheduledExecutorService newSingleThreadScheduledExecutor(ThreadFactory threadFactory) 创建实例对象代码如下：12ScheduledExecutorService scheduledThreadPoolExecutor=Executors.newScheduledThreadPool(5); ScheduledExecutorService singleThreadScheduledExecutor=Executors.newSingleThreadScheduledExecutor(); ScheduledThreadPoolExecutor和SingleThreadScheduledExecutor的适用场景ScheduledThreadPoolExecutor：适用于多个后台线程执行周期性任务，同时为了满足资源管理的需求而需要限制后台线程数量的应用场景。SingleThreadScheduledExecutor：适用于需要单个后台线程执行周期任务，同时需要保证任务顺序执行的应用场景。 ScheduledThreadPoolExecutor使用案例我们创建一个Runnable的对象，然后使用ScheduledThreadPoolExecutor的Scheduled()来执行延迟任务，输出执行时间即可:我们先来介绍一下该类延迟执行的方法：1public ScheduledFuture&lt;?&gt; schedule(Runnable command,long delay, TimeUnit unit); 参数解析：command：就是一个实现Runnable接口的类delay：延迟多久后执行。unit：用于指定keepAliveTime参数的时间单位，这是一个枚举，常用的有TimeUnit.MILLISECONDS(毫秒)，TimeUnit.SECONDS(秒)以及TimeUnit.MINUTES(分钟)等。这里要注意这个方法会返回ScheduledFuture实例，可以用于获取线程状态信息和延迟时间。12345678910111213141516171819202122232425262728293031323334353637package com.sky.code.thread;import java.text.SimpleDateFormat;import java.util.Date;public class WorkerThread implements Runnable&#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()+" Start. Time = "+getNowDate()); threadSleep(); System.out.println(Thread.currentThread().getName()+" End. Time = "+getNowDate()); &#125; /** * 睡3秒 */ public void threadSleep()&#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; /** * 获取现在时间 * * @return 返回时间类型 yyyy-MM-dd HH:mm:ss */ public static String getNowDate() &#123; Date currentTime = new Date(); SimpleDateFormat formatter; formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); String ctime = formatter.format(currentTime); return ctime; &#125;&#125; 执行类如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.sky.code.thread;import java.text.SimpleDateFormat;import java.util.Date;import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;public class ScheduledThreadPoolTest &#123; public static void main(String[] args) &#123; ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); try &#123; //schedule to run after sometime System.out.println("Current Time = "+getNowDate()); for(int i=0; i&lt;3; i++)&#123; Thread.sleep(1000); WorkerThread worker = new WorkerThread(); //延迟10秒后执行 scheduledThreadPool.schedule(worker, 10, TimeUnit.SECONDS); &#125; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; scheduledThreadPool.shutdown(); while(!scheduledThreadPool.isTerminated())&#123; //wait for all tasks to finish &#125; System.out.println("Finished all threads"); &#125; /** * 获取现在时间 * * @return 返回时间类型 yyyy-MM-dd HH:mm:ss */ public static String getNowDate() &#123; Date currentTime = new Date(); SimpleDateFormat formatter; formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); String ctime = formatter.format(currentTime); return ctime; &#125;&#125; 运行输入执行结果：线程任务确实在10秒延迟后才开始执行。这就是schedule()方法的使用。下面我们再介绍2个可用于周期性执行任务的方法。1public ScheduledFuture&lt;?&gt; scheduleAtFixedRate(Runnable command,long initialDelay,long period,TimeUnit unit) scheduleAtFixedRate方法的作用是预定在初始的延迟结束后，周期性地执行给定的任务，周期长度为period，其中initialDelay为初始延迟。1public ScheduledFuture&lt;?&gt; scheduleWithFixedDelay(Runnable command,long initialDelay,long delay,TimeUnit unit); scheduleWithFixedDelay方法的作用是预定在初始的延迟结束后周期性地执行给定任务，在一次调用完成和下一次调用开始之间有长度为delay的延迟，其中initialDelay为初始延迟。下面给出实现案例代码参考：1234567891011121314151617181920212223242526package com.sky.code.thread;import java.util.Date;import java.util.concurrent.ScheduledThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class ScheduledTask &#123; public ScheduledThreadPoolExecutor se = new ScheduledThreadPoolExecutor(5); public static void main(String[] args) &#123; new ScheduledTask(); &#125; public void fixedPeriodSchedule() &#123; // 设定可以循环执行的runnable,初始延迟为0，这里设置的任务的间隔为5秒 for(int i=0;i&lt;5;i++)&#123; se.scheduleAtFixedRate(new FixedSchedule(), 0, 5, TimeUnit.SECONDS); &#125; &#125; public ScheduledTask() &#123; fixedPeriodSchedule(); &#125; class FixedSchedule implements Runnable &#123; public void run() &#123; System.out.println("当前线程："+Thread.currentThread().getName()+" 当前时间："+new Date(System.currentTimeMillis())); &#125; &#125;&#125; 运行结果123456789101112131415当前线程：pool-1-thread-5 当前时间：Tue Aug 08 09:43:18 CST 2017 当前线程：pool-1-thread-4 当前时间：Tue Aug 08 09:43:18 CST 2017 当前线程：pool-1-thread-3 当前时间：Tue Aug 08 09:43:18 CST 2017 当前线程：pool-1-thread-1 当前时间：Tue Aug 08 09:43:18 CST 2017 当前线程：pool-1-thread-2 当前时间：Tue Aug 08 09:43:18 CST 2017 当前线程：pool-1-thread-1 当前时间：Tue Aug 08 09:43:23 CST 2017 当前线程：pool-1-thread-4 当前时间：Tue Aug 08 09:43:23 CST 2017 当前线程：pool-1-thread-3 当前时间：Tue Aug 08 09:43:23 CST 2017 当前线程：pool-1-thread-5 当前时间：Tue Aug 08 09:43:23 CST 2017 当前线程：pool-1-thread-2 当前时间：Tue Aug 08 09:43:23 CST 2017 当前线程：pool-1-thread-1 当前时间：Tue Aug 08 09:43:28 CST 2017 当前线程：pool-1-thread-4 当前时间：Tue Aug 08 09:43:28 CST 2017 当前线程：pool-1-thread-5 当前时间：Tue Aug 08 09:43:28 CST 2017 当前线程：pool-1-thread-3 当前时间：Tue Aug 08 09:43:28 CST 2017 当前线程：pool-1-thread-1 当前时间：Tue Aug 08 09:43:28 CST 2017 至于scheduleWithFixedDelay方法，大家就把代码稍微修改一下执行试试就行，这里就不重复了。而SingleThreadScheduledExecutor的使用的方法基本是类似，只不过是单线程罢了，这里也不再描述了。好了，今天就到这吧。 参考 http://blog.csdn.net/javazejian/article/details/50890554代码 https://github.com/ejunjsh/java-code/tree/master/src/main/java/com/sky/code/thread]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程(二) 同步和锁]]></title>
    <url>%2F2016%2F08%2F20%2Fjava-thread-2%2F</url>
    <content type="text"><![CDATA[记录，汇总 线程同步问题的产生什么是线程同步问题，我们先来看一段卖票系统的代码，然后再分析这个问题：1234567891011121314151617181920212223242526package com.sky.code.thread;public class TicketSeller implements Runnable &#123; private int num = 100; public void run() &#123; while(true) &#123; if(num&gt;0) &#123; try&#123; Thread.sleep(10); &#125;catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //输出卖票信息 System.out.println(Thread.currentThread().getName()+".....sale...."+num--); &#125; else &#123; break; &#125; &#125; &#125;&#125; 上面是卖票线程类，下来再来看看执行类：1234567891011121314151617181920package com.sky.code.thread;public class TickeDemo &#123; public static void main(String[] args) &#123; TicketSeller t = new TicketSeller();//创建一个线程任务对象。 //创建4个线程同时卖票 Thread t1 = new Thread(t); Thread t2 = new Thread(t); Thread t3 = new Thread(t); Thread t4 = new Thread(t); //启动线程 t1.start(); t2.start(); t3.start(); t4.start(); &#125;&#125; 运行程序结果如下（仅截取部分数据）：从运行结果，我们就可以看出我们3个售票窗口同时卖出了96号票，这显然是不合逻辑的，其实这个问题就是我们前面所说的线程同步问题。不同的线程都对同一个数据进了操作这就容易导致数据错乱的问题，也就是线程不同步。那么这个问题该怎么解决呢？在给出解决思路之前我们先来分析一下这个问题是怎么产生的？我们声明一个线程类TicketSeller，在这个类中我们又声明了一个成员变量num也就是票的数量，然后我们通过run方法不断的去获取票数并输出，最后我们在外部类TicketDemo中创建了四个线程同时操作这个数据，运行后就出现我们刚才所说的线程同步问题，从这里我们可以看出产生线程同步(线程安全)问题的条件有两个：1.多个线程在操作共享的数据（num），2.操作共享数据的线程代码有多条（4条线程）；既然原因知道了，那该怎么解决？ 解决思路：将多条操作共享数据的线程代码封装起来，当有线程在执行这些代码的时候，其他线程时不可以参与运算的。必须要当前线程把这些代码都执行完毕后，其他线程才可以参与运算。 好了，思路知道了，我们就用java代码的方式来解决这个问题。 解决线程同步的两种典型方案在java中有两种机制可以防止线程安全的发生，Java语言提供了一个synchronized关键字来解决这问题，同时在Java SE5.0引入了Lock锁对象的相关类，接下来我们分别介绍这两种方法 通过锁（Lock）对象的方式解决线程安全问题在给出解决代码前我们先来介绍一个知识点：Lock，锁对象。在java中锁是用来控制多个线程访问共享资源的方式，一般来说，一个锁能够防止多个线程同时访问共享资源（但有的锁可以允许多个线程并发访问共享资源，比如读写锁，后面我们会分析）。在Lock接口出现之前，java程序是靠synchronized关键字（后面分析）实现锁功能的，而JAVA SE5.0之后并发包中新增了Lock接口用来实现锁的功能，它提供了与synchronized关键字类似的同步功能，只是在使用时需要显式地获取和释放锁，缺点就是缺少像synchronized那样隐式获取释放锁的便捷性，但是却拥有了锁获取与释放的可操作性，可中断的获取锁以及超时获取锁等多种synchronized关键字所不具备的同步特性。接下来我们就来介绍Lock接口的主要API方便我们学习 方法 相关描述内容 void lock() 获取锁，调用该方法当前线程会获取锁，当获取锁后。从该方法返回 void lockInterruptibly() throws InterruptedException 可中断获取锁和lock()方法不同的是该方法会响应中断，即在获取锁中可以中断当前线程。例如某个线程在等待一个锁的控制权的这段时间需要中断。 boolean tryLock() 尝试非阻塞获取锁，调用该方法后立即返回，如果能够获取锁则返回true，否则返回false。 boolean tryLock(long time,TimeUnit unit) throws InterruptedException 超时获取锁，当前线程在以下3种情况返回：1.当前线程在超时时间内获取了锁2.当前线程在超时时间被中断3.当前线程超时时间结束，返回false void unlock() 释放锁 Condition newCondition() 条件对象，获取等待通知组件。该组件和当前的锁绑定，当前线程只有获取了锁，才能调用该组件的await()方法，而调用后，当前线程将释放锁。 这里先介绍一下API，接下来我们将结合Lock接口的实现子类ReentrantLock来讲解下他的几个方法。 ReentrantLock（重入锁)重入锁，顾名思义就是支持重新进入的锁，它表示该锁能够支持一个线程对资源的重复加锁，也就是说在调用lock()方法时，已经获取到锁的线程，能够再次调用lock()方法获取锁而不被阻塞，同时还支持获取锁的公平性和非公平性。这里的公平是在绝对时间上，先对锁进行获取的请求一定先被满足，那么这个锁是公平锁，反之，是不公平的(但是如果不是需要，建议不要用公平锁，因为会造成一些资源的没必要等待，浪费性能)。那么该如何使用呢？看范例代码：1.同步执行的代码跟synchronized类似功能：123456789ReentrantLock lock = new ReentrantLock(); //参数默认false，不公平锁 ReentrantLock lock = new ReentrantLock(true); //公平锁 lock.lock(); //如果被其它资源锁定，会在此等待锁释放，达到暂停的效果 try &#123; //操作 &#125; finally &#123; lock.unlock(); //释放锁 &#125; 2.防止重复执行代码：12345678ReentrantLock lock = new ReentrantLock(); if (lock.tryLock()) &#123; //如果已经被lock，则立即返回false不会等待，达到忽略操作的效果 try &#123; //操作 &#125; finally &#123; lock.unlock(); &#125; &#125; 3.尝试等待执行的代码：12345678910111213ReentrantLock lock = new ReentrantLock(true); //公平锁 try &#123; if (lock.tryLock(5, TimeUnit.SECONDS)) &#123; //如果已经被lock，尝试等待5s，看是否可以获得锁，如果5s后仍然无法获得锁则返回false继续执行 try &#123; //操作 &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); //当前线程被中断时(interrupt)，会抛InterruptedException &#125; 这里有点需要特别注意的，把解锁操作放在finally代码块内这个十分重要。如果在临界区的代码抛出异常，锁必须被释放。否则，其他线程将永远阻塞。好了，ReentrantLock我们就简单介绍到这里，接下来我们通过ReentrantLock来解决前面卖票线程的线程同步（安全）问题，代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041package com.sky.code.thread;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class TicketSellerWithLock implements Runnable &#123; //创建锁对象 private Lock ticketLock = new ReentrantLock(); //创建锁对象(公平锁) //private Lock ticketLock = new ReentrantLock(true); private int num = 100; public void run() &#123; while(true) &#123; ticketLock.lock();//获取锁 if(num&gt;0) &#123; try&#123; Thread.sleep(10); //输出卖票信息 System.out.println(Thread.currentThread().getName()+".....sale...."+num--); &#125;catch (InterruptedException e) &#123; Thread.currentThread().interrupt();//继续中断异常 &#125;finally &#123; ticketLock.unlock();//释放锁 &#125; &#125; else &#123; ticketLock.unlock();//释放锁 break; &#125; &#125; &#125;&#125; TicketDemo类无需变化，线程安全问题就此解决。但是还是要说一下公平锁的问题，上面例子，不开公平锁的结果如下：开公平锁的结果如下：你会发现不开公平锁，cpu钟爱用第一个线程做事情，而开了公平锁后，基本是各个线程交替执行。上面提到公平锁是会消耗性能的，如果CPU调度的时候选择的不是公平调度的那个线程，CPU会放弃本次调度，干别的事情，如果老是调度不到的话，是浪费CPU调度的。 通过synchronied关键字的方式解决线程安全问题在Java中内置了语言级的同步原语－synchronized，这个可以大大简化了Java中多线程同步的使用。从JAVA SE1.0开始，java中的每一个对象都有一个内部锁，如果一个方法使用synchronized关键字进行声明，那么这个对象将保护整个方法，也就是说调用该方法线程必须获得内部的对象锁。123public synchronized void method&#123; //method body &#125; 等价于123456789private Lock ticketLock = new ReentrantLock(); public void method&#123; ticketLock.lock(); try&#123; //....... &#125;finally&#123; ticketLock.unlock(); &#125; &#125; 从这里可以看出使用synchronized关键字来编写代码要简洁得多了。当然，要理解这一代码，我们必须知道每个对象有一个内部锁，并且该锁有一个内部条件。由锁来管理那些试图进入synchronized方法的线程，由条件来管那些调用wait的线程(wait()/notifyAll/notify())。同时我们必须明白一旦有一个线程通过synchronied方法获取到内部锁，该类的所有synchronied方法或者代码块都无法被其他线程访问直到当前线程释放了内部锁。刚才上面说的是同步方法，synchronized还有一种同步代码块的实现方式：1234Object obj = new Object(); synchronized(obj)&#123; //需要同步的代码 &#125; 其中obj是对象锁，可以是任意对象。那么我们就通过其中的一个方法来解决售票系统的线程同步问题：1234567891011121314151617181920class Ticket implements Runnable &#123; private int num = 100; Object obj = new Object(); public void run() &#123; while(true) &#123; synchronized(obj) &#123; if(num&gt;0) &#123; try&#123;Thread.sleep(10);&#125;catch (InterruptedException e)&#123;&#125; System.out.println(Thread.currentThread().getName()+".....sale...."+num--); &#125; &#125; &#125; &#125; &#125; 嗯，同步代码块解决，运行结果也正常。到此同步问题也就解决了，当然代码同步也是要牺牲效率为前提的：同步的好处：解决了线程的安全问题。同步的弊端：相对降低了效率，因为同步外的线程的都会判断同步锁。同步的前提：同步中必须有多个线程并使用同一个锁。 线程间的通信机制线程开始运行，拥有自己的栈空间，但是如果每个运行中的线程，如果仅仅是孤立地运行，那么没有一点儿价值，或者是价值很小，如果多线程能够相互配合完成工作的话，这将带来巨大的价值，这也就是线程间的通信啦。在java中多线程间的通信使用的是等待/通知机制来实现的。 synchronied关键字等待/通知机制是指一个线程A调用了对象O的wait()方法进入等待状态，而另一个线程B调用了对象O的notify()或者notifyAll()方法，线程A收到通知后从对象O的wait()方法返回，进而执行后续操作。上述的两个线程通过对象O来完成交互，而对象上的wait()和notify()/notifyAll()的关系就如同开关信号一样，用来完成等待方和通知方之间的交互工作。等待/通知机制主要是用到的函数方法是notify()/notifyAll(),wait()/wait(long),wait(long,int),这些方法在上一篇文章都有说明过，这里就不重复了。当然这是针对synchronied关键字修饰的函数或代码块，因为要使用notify()/notifyAll(),wait()/wait(long),wait(long,int)这些方法的前提是对调用对象加锁，也就是说只能在同步函数或者同步代码块中使用。 条件对象的等待/通知机制所谓的条件对象也就是配合前面我们分析的Lock锁对象，通过锁对象的条件对象来实现等待/通知机制。那么条件对象是怎么创建的呢？12//创建条件对象 Condition conditionObj=ticketLock.newCondition(); 就这样我们创建了一个条件对象。注意这里返回的对象是与该锁（ticketLock）相关的条件对象。下面是条件对象的API： 方法 函数方法对应的描述 void await() 将该线程放到条件等待池中（对应wait()方法） void signalAll() 解除该条件等待池中所有线程的阻塞状态（对应notifyAll()方法） void signal() 从该条件的等待池中随机地选择一个线程，解除其阻塞状态（对应notify()方法） 上述方法的过程分析：一个线程A调用了条件对象的await()方法进入等待状态，而另一个线程B调用了条件对象的signal()或者signalAll()方法，线程A收到通知后从条件对象的await()方法返回，进而执行后续操作。上述的两个线程通过条件对象来完成交互，而对象上的await()和signal()/signalAll()的关系就如同开关信号一样，用来完成等待方和通知方之间的交互工作。当然这样的操作都是必须基于条件对象的锁的，当前线程只有获取了锁，才能调用该条件对象的await()方法，而调用后，当前线程将释放锁。 这里有点要特别注意的是，上述两种等待/通知机制中，无论是调用了signal()/signalAll()方法还是调用了notify()/notifyAll()方法并不会立即激活一个等待线程。它们仅仅都只是解除等待线程的阻塞状态，以便这些线程可以在当前线程解锁或者退出同步方法后，通过争夺CPU执行权实现对对象的访问。到此，线程通信机制的概念分析完，我们下面通过生产者消费者模式来实现等待/通知机制。 生产者消费者模式单生产者单消费者模式顾名思义，就是一个线程消费，一个线程生产。我们先来看看等待/通知机制下的生产者消费者模式：我们假设这样一个场景，我们是卖北京烤鸭店铺，我们现在只有一条生产线也只有一条消费线，也就是说只能生产线程生产完了，再通知消费线程才能去卖，如果消费线程没烤鸭了，就必须通知生产线程去生产，此时消费线程进入等待状态。在这样的场景下，我们不仅要保证共享数据（烤鸭数量）的线程安全，而且还要保证烤鸭数量在消费之前必须有烤鸭。下面我们通过java代码来实现：北京烤鸭生产资源类KaoYaResource：1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.sky.code.thread;public class KaoYaResource &#123; private String name; private int count = 1;//烤鸭的初始数量 private boolean flag = false;//判断是否有需要线程等待的标志 /** * 生产烤鸭 */ public synchronized void product(String name)&#123; if(flag)&#123; //此时有烤鸭，等待 try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace() ; &#125; &#125; this.name=name+count;//设置烤鸭的名称 count++; System.out.println(Thread.currentThread().getName()+"...生产者..."+this.name); flag=true;//有烤鸭后改变标志 notifyAll();//通知消费线程可以消费了 &#125; /** * 消费烤鸭 */ public synchronized void consume()&#123; if(!flag)&#123;//如果没有烤鸭就等待 try&#123; this.wait(); &#125;catch(InterruptedException e)&#123; &#125; &#125; System.out.println(Thread.currentThread().getName()+"...消费者........"+this.name);//消费烤鸭1 flag = false; notifyAll();//通知生产者生产烤鸭 &#125;&#125; 在这个类中我们有两个synchronized的同步方法，一个是生产烤鸭的，一个是消费烤鸭的，之所以需要同步是因为我们操作了共享数据count，同时为了保证生产烤鸭后才能消费也就是生产一只烤鸭后才能消费一只烤鸭，我们使用了等待/通知机制，wait()和notify()。当第一次运行生产现场时调用生产的方法，此时有一只烤鸭，即flag=false，无需等待，因此我们设置可消费的烤鸭名称然后改变flag=true，同时通知消费线程可以消费烤鸭了，即使此时生产线程再次抢到执行权，因为flag=true，所以生产线程会进入等待阻塞状态，消费线程被唤醒后就进入消费方法，消费完成后，又改变标志flag=false，通知生产线程可以生产烤鸭了………以此循环。生产消费执行类Single_Producer_Consumer.java:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.sky.code.thread;public class Single_Producer_Consumer &#123; public static void main(String[] args) &#123; KaoYaResource r = new KaoYaResource(); Producer pro = new Producer(r); Consumer con = new Consumer(r); //生产者线程 Thread t0 = new Thread(pro); //消费者线程 Thread t2 = new Thread(con); //启动线程 t0.start(); t2.start(); &#125;&#125;class Producer implements Runnable&#123; private KaoYaResource r; Producer(KaoYaResource r) &#123; this.r = r; &#125; public void run() &#123; while(true) &#123; r.product("北京烤鸭"); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;class Consumer implements Runnable&#123; private KaoYaResource r; Consumer(KaoYaResource r) &#123; this.r = r; &#125; public void run() &#123; while(true) &#123; r.consume(); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 在这个类中我们创建两个线程，一个是消费者线程，一个是生产者线程，我们分别开启这两个线程用于不断的生产消费，运行结果如下：12345678910111213141516171819hread-0...生产者...北京烤鸭1Thread-1...消费者........北京烤鸭1Thread-0...生产者...北京烤鸭2Thread-1...消费者........北京烤鸭2Thread-0...生产者...北京烤鸭3Thread-1...消费者........北京烤鸭3Thread-0...生产者...北京烤鸭4Thread-1...消费者........北京烤鸭4Thread-0...生产者...北京烤鸭5Thread-1...消费者........北京烤鸭5Thread-0...生产者...北京烤鸭6Thread-1...消费者........北京烤鸭6Thread-0...生产者...北京烤鸭7Thread-1...消费者........北京烤鸭7Thread-0...生产者...北京烤鸭8Thread-1...消费者........北京烤鸭8Thread-0...生产者...北京烤鸭9Thread-1...消费者........北京烤鸭9..... 很显然的情况就是生产一只烤鸭然后就消费一只烤鸭。运行情况完全正常，嗯，这就是单生产者单消费者模式。上面使用的是synchronized关键字的方式实现的，那么接下来我们使用对象锁的方式实现：KaoYaResourceByLock.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.sky.code.thread;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class KaoYaResourceByLock &#123; private String name; private int count = 1;//烤鸭的初始数量 private boolean flag = false;//判断是否有需要线程等待的标志 //创建一个锁对象 private Lock resourceLock=new ReentrantLock(); //创建条件对象 private Condition condition= resourceLock.newCondition(); /** * 生产烤鸭 */ public void product(String name)&#123; resourceLock.lock();//先获取锁 try&#123; if(flag)&#123; try &#123; condition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; this.name=name+count;//设置烤鸭的名称 count++; System.out.println(Thread.currentThread().getName()+"...生产者..."+this.name); flag=true;//有烤鸭后改变标志 condition.signalAll();//通知消费线程可以消费了 &#125;finally&#123; resourceLock.unlock(); &#125; &#125; /** * 消费烤鸭 */ public void consume()&#123; resourceLock.lock(); try&#123; if(!flag)&#123;//如果没有烤鸭就等待 try&#123; condition.await(); &#125;catch(InterruptedException e)&#123; &#125; &#125; System.out.println(Thread.currentThread().getName()+"...消费者........"+this.name);//消费烤鸭1 flag = false; condition.signalAll();//通知生产者生产烤鸭 &#125;finally&#123; resourceLock.unlock(); &#125; &#125;&#125; 代码变化不大，我们通过对象锁的方式去实现，首先要创建一个对象锁，我们这里使用的重入锁ReestrantLock类，然后通过手动设置lock()和unlock()的方式去获取锁以及释放锁。为了实现等待/通知机制，我们还必须通过锁对象去创建一个条件对象Condition，然后通过锁对象的await()和signalAll()方法去实现等待以及通知操作。Single_Producer_Consumer.java代码替换一下资源类即可,运行结果一样。 多生产者多消费者模式分析完了单生产者单消费者模式，我们再来聊聊多生产者多消费者模式，也就是多条生产线程配合多条消费线程。既然这样的话我们先把上面的代码Single_Producer_Consumer.java类修改成新类，大部分代码不变，仅新增2条线程去跑，一条t1的生产 共享资源类KaoYaResource不作更改，代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.sky.code.thread;public class Mutil_Producer_Consumer &#123; public static void main(String[] args) &#123; KaoYaResource r = new KaoYaResource(); Mutil_Producer pro = new Mutil_Producer(r); Mutil_Consumer con = new Mutil_Consumer(r); //生产者线程 Thread t0 = new Thread(pro); Thread t1 = new Thread(pro); //消费者线程 Thread t2 = new Thread(con); Thread t3 = new Thread(con); //启动线程 t0.start(); t1.start(); t2.start(); t3.start(); &#125;&#125;class Mutil_Producer implements Runnable&#123; private KaoYaResource r; Mutil_Producer(KaoYaResource r) &#123; this.r = r; &#125; public void run() &#123; while(true) &#123; r.product("北京烤鸭"); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;class Mutil_Consumer implements Runnable&#123; private KaoYaResource r; Mutil_Consumer(KaoYaResource r) &#123; this.r = r; &#125; public void run() &#123; while(true) &#123; r.consume(); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 就多了两条线程，我们运行代码看看，结果如下：123456789Thread-0...生产者...北京烤鸭63Thread-2...消费者........北京烤鸭63Thread-3...消费者........北京烤鸭63 #消费两次了............Thread-0...生产者...北京烤鸭67 #没有被消费Thread-1...生产者...北京烤鸭68 Thread-2...消费者........北京烤鸭68Thread-0...生产者...北京烤鸭69 不对呀，我们才生产一只烤鸭，怎么就被消费了2次啊，有的烤鸭生产了也没有被消费啊？难道共享数据源没有进行线程同步？回顾下KaoYaResource.java共享数据count的获取方法都进行synchronized关键字同步了呀！那怎么还会出现数据混乱的现象啊？分析：确实，我们对共享数据也采用了同步措施，而且也应用了等待/通知机制，但是这样的措施只在单生产者单消费者的情况下才能正确应用，但从运行结果来看，我们之前的单生产者单消费者安全处理措施就不太适合多生产者多消费者的情况了。那么问题出在哪里？可以明确的告诉大家，肯定是在资源共享类，下面我们就来分析问题是如何出现，又该如何解决？直接上图 解决后的资源代码如下只将if改为了while：1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.sky.code.thread;public class KaoYaResourceByMulti &#123; private String name; private int count = 1;//烤鸭的初始数量 private boolean flag = false;//判断是否有需要线程等待的标志 /** * 生产烤鸭 */ public synchronized void product(String name)&#123; while (flag)&#123; //此时有烤鸭，等待 try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace() ; &#125; &#125; this.name=name+count;//设置烤鸭的名称 count++; System.out.println(Thread.currentThread().getName()+"...生产者..."+this.name); flag=true;//有烤鸭后改变标志 notifyAll();//通知消费线程可以消费了 &#125; /** * 消费烤鸭 */ public synchronized void consume()&#123; while (!flag)&#123;//如果没有烤鸭就等待 try&#123; this.wait(); &#125;catch(InterruptedException e)&#123; &#125; &#125; System.out.println(Thread.currentThread().getName()+"...消费者........"+this.name);//消费烤鸭1 flag = false; notifyAll();//通知生产者生产烤鸭 &#125;&#125; 运行结果跟单线程那个一致，就不贴了。到此，多消费者多生产者模式也完成，不过上面用的是synchronied关键字实现的，而锁对象的解决方法也一样将之前单消费者单生产者的资源类中的if判断改为while判断即可代码就不贴了哈。不过下面我们将介绍一种更有效的锁对象解决方法，我们准备使用两组条件对象（Condition也称为监视器）来实现等待/通知机制，也就是说通过已有的锁获取两组监视器，一组监视生产者，一组监视消费者。有了前面的分析这里我们直接上代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.sky.code.thread;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class ResourceBy2Condition &#123; private String name; private int count = 1; private boolean flag = false; //创建一个锁对象。 Lock lock = new ReentrantLock(); //通过已有的锁获取两组监视器，一组监视生产者，一组监视消费者。 Condition producer_con = lock.newCondition(); Condition consumer_con = lock.newCondition(); /** * 生产 * @param name */ public void product(String name) &#123; lock.lock(); try &#123; while(flag)&#123; try&#123;producer_con.await();&#125;catch(InterruptedException e)&#123;&#125; &#125; this.name = name + count; count++; System.out.println(Thread.currentThread().getName()+"...生产者5.0..."+this.name); flag = true;// notifyAll(); // con.signalAll(); consumer_con.signal();//直接唤醒消费线程 &#125; finally &#123; lock.unlock(); &#125; &#125; /** * 消费 */ public void consume() &#123; lock.lock(); try &#123; while(!flag)&#123; try&#123;consumer_con.await();&#125;catch(InterruptedException e)&#123;&#125; &#125; System.out.println(Thread.currentThread().getName()+"...消费者.5.0......."+this.name);//消费烤鸭1 flag = false;// notifyAll(); // con.signalAll(); producer_con.signal();//直接唤醒生产线程 &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 从代码中可以看到，我们创建了producer_con 和consumer_con两个条件对象，分别用于监听生产者线程和消费者线程，在product()方法中，我们获取到锁后，如果此时flag为true的话，也就是此时还有烤鸭未被消费，因此生产线程需要等待，所以我们调用生产线程的监控器producer_con的await()的方法进入阻塞等待池；但如果此时的flag为false的话，就说明烤鸭已经消费完，需要生产线程去生产烤鸭，那么生产线程将进行烤鸭生产并通过消费线程的监控器consumer_con的signal()方法去通知消费线程对烤鸭进行消费。consume()方法也是同样的道理，这里就不过多分析了。我们可以发现这种方法比我们之前的synchronized同步方法或者是单监视器的锁对象都来得高效和方便些，之前都是使用notifyAll()和signalAll()方法去唤醒池中的线程，然后让池中的线程又进入 竞争队列去抢占CPU资源，这样不仅唤醒了无关的线程而且又让全部线程进入了竞争队列中，而我们最后使用两种监听器分别监听生产者线程和消费者线程，这样的方式恰好解决前面两种方式的问题所在，我们每次唤醒都只是生产者线程或者是消费者线程而不会让两者同时唤醒，这样不就能更高效得去执行程序了吗？好了，到此多生产者多消费者模式也分析完毕。 线程死锁现在我们再来讨论一下线程死锁问题，从上面的分析，我们知道锁是个非常有用的工具，运用的场景非常多，因为它使用起来非常简单，而且易于理解。但同时它也会带来一些不必要的麻烦，那就是可能会引起死锁，一旦产生死锁，就会造成系统功能不可用。我们先通过一个例子来分析，这个例子会引起死锁，使得线程t1和线程t2互相等待对方释放锁。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.sky.code.thread;public class DeadLockDemo &#123; private static String A="A"; private static String B="B"; public static void main(String[] args) &#123; DeadLockDemo deadLock=new DeadLockDemo(); deadLock.deadLock(); &#125; private void deadLock()&#123; Thread t1=new Thread(new Runnable()&#123; @SuppressWarnings("static-access") @Override public void run() &#123; synchronized (A) &#123; try &#123; Thread.currentThread().sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (B) &#123; System.out.println("1"); &#125; &#125; &#125; &#125;); Thread t2 =new Thread(new Runnable() &#123; @Override public void run() &#123; synchronized (B) &#123; synchronized (A) &#123; System.out.println("2"); &#125; &#125; &#125; &#125;); //启动线程 t1.start(); t2.start(); &#125;&#125; 上面代码运行基本没有输出，一直卡着。同步嵌套是产生死锁的常见情景，从上面的代码中我们可以看出，当t1线程拿到锁A后，睡眠2秒，此时线程t2刚好拿到了B锁，接着要获取A锁，但是此时A锁正好被t1线程持有，因此只能等待t1线程释放锁A，但遗憾的是在t1线程内又要求获取到B锁，而B锁此时又被t2线程持有，到此结果就是t1线程拿到了锁A同时在等待t2线程释放锁B，而t2线程获取到了锁B也同时在等待t1线程释放锁A，彼此等待也就造成了线程死锁问题。虽然我们现实中一般不会向上面那么写出那样的代码，但是有些更为复杂的场景中，我们可能会遇到这样的问题，比如t1拿了锁之后，因为一些异常情况没有释放锁（死循环），也可能t1拿到一个数据库锁，释放锁的时候抛出了异常，没有释放等等，所以我们应该在写代码的时候多考虑死锁的情况，这样才能有效预防死锁程序的出现。下面我们介绍一下避免死锁的几个常见方法： 避免一个线程同时获取多个锁。 避免在一个资源内占用多个 资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用tryLock(timeout)来代替使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 避免同步嵌套的发生 Thread.join()如果一个线程A执行了thread.join()语句，其含义是：当前线程A等待thread线程终止之后才能从thread.join()返回。线程Thread除了提供join()方法之外，还提供了join(long millis)和join(long millis,int nanos)两个具备超时特性的方法。这两个超时的方法表示，如果线程在给定的超时时间里没有终止，那么将会从该超时方法中返回。下面给出一个例子，创建10个线程，编号0~9，每个线程调用前一个线程的join()方法，也就是线程0结束了，线程1才能从join()方法中返回，而0需要等待main线程结束。12345678910111213141516171819202122232425262728293031package com.sky.code.thread;public class JoinDemo &#123; public static void main(String[] args) &#123; Thread previous = Thread.currentThread(); for(int i=0;i&lt;10;i++)&#123; //每个线程拥有前一个线程的引用。需要等待前一个线程终止，才能从等待中返回 Thread thread=new Thread(new Domino(previous),String.valueOf(i)); thread.start(); previous=thread; &#125; System.out.println(Thread.currentThread().getName()+" 线程结束"); &#125;&#125;class Domino implements Runnable&#123; private Thread thread; public Domino(Thread thread)&#123; this.thread=thread; &#125; @Override public void run() &#123; try &#123; thread.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+" 线程结束"); &#125;&#125; 运行结果：1234567891011main 线程结束0 线程结束1 线程结束2 线程结束3 线程结束4 线程结束5 线程结束6 线程结束7 线程结束8 线程结束9 线程结束 结束 参考 http://blog.csdn.net/javazejian/article/details/50878665所有代码在 https://github.com/ejunjsh/java-code/tree/master/src/main/java/com/sky/code/thread]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程(一)基础]]></title>
    <url>%2F2016%2F08%2F08%2Fjava-thread-1%2F</url>
    <content type="text"><![CDATA[对多线程复习下，汇总一下 什么是线程以及多线程与进程的区别在现代操作在运行一个程序时，会为其创建一个进程。例如启动一个QQ程序，操作系统就会为其创建一个进程。而操作系统中调度的最小单位元是线程，也叫轻量级进程，在一个进程里可以创建多个线程，这些线程都拥有各自的计数器，堆栈和局部变量等属性，并且能够访问共享的内存变量。处理器在这些线程上高速切换，让使用者感觉到这些线程在同时执行。因此我们可以这样理解：进程：正在运行的程序，是系统进行资源分配和调用的独立单位。每一个进程都有它自己的内存空间和系统资源。线程：是进程中的单个顺序控制流，是一条执行路径一个进程如果只有一条执行路径，则称为单线程程序。一个进程如果有多条执行路径，则称为多线程程序。 多线程的创建与启动创建多线程有两种方法，一种是继承Thread类重写run方法，另一种是实现Runnable接口重写run方法。下面我们分别给出代码示例，继承Thread类重写run方法：123456789package com.sky.code.thread;public class NewThread extends Thread &#123; @Override public void run()&#123; System.out.println("I'm a thread that extends Thread!"); &#125;&#125; 实现Runnable接口重写run方法:12345678910package com.sky.code.thread;public class NewRunnable implements Runnable&#123; @Override public void run() &#123; System.out.println("I'm a thread that implements Runnable !"); &#125;&#125; 怎么启动线程？12345678910111213package com.sky.code.thread;public class StartThread &#123; public static void main(String[] args)&#123; NewThread t1=new NewThread(); t1.start(); NewRunnable r=new NewRunnable(); Thread t2=new Thread(r); t2.start(); &#125;&#125; 运行结果：12I&apos;m a thread that extends Thread!I&apos;m a thread that implements Runnable ! 代码相当简单，不过多解释。这里有点需要注意的是调用start()方法后并不是是立即的执行多线程的代码，而是使该线程变为可运行态，什么时候运行多线程代码是由操作系统决定的。 中断线程和守护线程以及线程优先级什么是中断线程？我们先来看看中断线程是什么？(该解释来自java核心技术一书，我对其进行稍微简化)，当线程的run()方法执行方法体中的最后一条语句后，并经由执行return语句返回时，或者出现在方法中没有捕获的异常时线程将终止。在java早期版本中有一个stop方法，其他线程可以调用它终止线程，但是这个方法现在已经被弃用了，因为这个方法会造成一些线程不安全的问题。我们可以把中断理解为一个标识位的属性，它表示一个运行中的线程是否被其他线程进行了中断操作，而中断就好比其他线程对该线程打可个招呼，其他线程通过调用该线程的interrupt方法对其进行中断操作，当一个线程调用interrupt方法时，线程的中断状态（标识位）将被置位（改变），这是每个线程都具有的boolean标志，每个线程都应该不时的检查这个标志，来判断线程是否被中断。而要判断线程是否被中断，我们可以使用如下代码1Thread.currentThread().isInterrupted() 123while(!Thread.currentThread().isInterrupted())&#123; do something &#125; 但是如果此时线程处于阻塞状态（sleep或者wait），就无法检查中断状态，此时会抛出InterruptedException异常。如果每次迭代之后都调用sleep方法（或者其他可中断的方法），isInterrupted检测就没必要也没用处了，如果在中断状态被置位时调用sleep方法，它不会休眠反而会清除这一休眠状态并抛出InterruptedException。所以如果在循环中调用sleep,不要去检测中断状态，只需捕获InterruptedException。代码范例如下：1234567891011public void run()&#123; while(more work to do )&#123; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; //thread was interrupted during sleep e.printStackTrace(); &#125;finally&#123; //clean up , if required &#125; &#125; 不妥的处理方式：12345678void myTask()&#123; ... try&#123; sleep(50) &#125;catch(InterruptedException e)&#123; ... &#125; &#125; 正确的处理方式：123void myTask()throw InterruptedException&#123; sleep(50) &#125; 或者12345678void myTask()&#123; ... try&#123; sleep(50) &#125;catch(InterruptedException e)&#123; Thread.currentThread().interrupt(); &#125; &#125; 最后关于中断线程，我们这里给出中断线程的一些主要方法：void interrupt()：向线程发送中断请求，线程的中断状态将会被设置为true，如果当前线程被一个sleep调用阻塞，那么将会抛出interrupedException异常。static boolean interrupted()：测试当前线程（当前正在执行命令的这个线程）是否被中断。注意这是个静态方法，调用这个方法会产生一个副作用那就是它会将当前线程的中断状态重置为false。boolean isInterrupted()：判断线程是否被中断，这个方法的调用不会产生副作用即不改变线程的当前中断状态。static Thread currentThread() : 返回代表当前执行线程的Thread对象。 这里要注意下，为啥上面的代码，在catch之后还要在中断一次，因为catch会把当前线程的中断标志重置为false，这里不重新中断一次，上层代码就不知道中断了，程序就不知道有中断的发生，下面代码可以说明这个123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.sky.code.thread;/** * Created by ejunjsh on 8/10/2017. */public class TestInterrupt &#123; public static void main(String[] args) &#123; Thread t= new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e)&#123; //catch 异常之后，输出是false System.out.println("1.current interrupted flag is " +Thread.currentThread().isInterrupted()); Thread.currentThread().interrupt(); System.out.println("2.current interrupted flag is " +Thread.currentThread().isInterrupted()); &#125; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e)&#123; System.out.println("3.current interrupted flag is " +Thread.currentThread().isInterrupted()); Thread.currentThread().interrupt(); System.out.println("4.current interrupted flag is " +Thread.currentThread().isInterrupted()); &#125; &#125; &#125;); t.start(); //开始中断 t.interrupt(); try &#123; t.join(); &#125; catch (InterruptedException e) &#123; &#125; System.out.println("5.current state is " +t.getState()); &#125;&#125; 输出结果:123451.current interrupted flag is false2.current interrupted flag is true3.current interrupted flag is false4.current interrupted flag is true5.current state is TERMINATED 很明显，开始中断后，catch的标志位被重置了。 什么是守护线程？首先我们可以通过t.setDaemon(true)的方法将线程转化为守护线程。而守护线程的唯一作用就是为其他线程提供服务。计时线程就是一个典型的例子，它定时地发送“计时器滴答”信号告诉其他线程去执行某项任务。当只剩下守护线程时，虚拟机就退出了，因为如果只剩下守护线程，程序就没有必要执行了。另外JVM的垃圾回收、内存管理等线程都是守护线程。还有就是在做数据库应用时候，使用的数据库连接池，连接池本身也包含着很多后台线程，监控连接个数、超时时间、状态等等。最后还有一点需要特别注意的是在java虚拟机退出时Daemon线程中的finally代码块并不一定会执行哦，代码示例：12345678910111213141516171819202122232425package com.sky.code.thread;public class Deamon &#123; public static void main(String[] args) &#123; Thread deamon = new Thread(new DaemonRunner(),"DaemonRunner"); //设置为守护线程 deamon.setDaemon(true); deamon.start();//启动线程 &#125; static class DaemonRunner implements Runnable&#123; @Override public void run() &#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally&#123; System.out.println("这里的代码在java虚拟机退出时并不一定会执行哦！"); &#125; &#125; &#125;&#125; 因此在构建Daemon线程时，不能依靠finally代码块中的内容来确保执行关闭或清理资源的逻辑。 什么是线程优先级在现代操作系统中基本采用时分的形式调度运行的线程，操作系统会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了就会发生线程调度，并等待着下一次分配。线程分配到的时间片多少也决定了线程使用处理器资源的多少，而线程优先级就是决定线程需要多或者少分配一些处理器资源的线程属性。在java线程中，通过一个整型的成员变量Priority来控制线程优先级，每一个线程有一个优先级，默认情况下，一个线程继承它父类的优先级。可以用setPriority方法提高或降低任何一个线程优先级。可以将优先级设置在MIN_PRIORITY（在Thread类定义为1）与MAX_PRIORITY（在Thread类定义为10）之间的任何值。线程的默认优先级为NORM_PRIORITY（在Thread类定义为5）。尽量不要依赖优先级，如果确实要用，应该避免初学者常犯的一个错误。如果有几个高优先级的线程没有进入非活动状态，低优先级线程可能永远也不能执行。每当调度器决定运行一个新线程时，首先会在具有高优先级的线程中进行选择，尽管这样会使低优先级的线程可能永远不会被执行到。因此我们在设置优先级时，针对频繁阻塞（休眠或者I/O操作）的线程需要设置较高的优先级，而偏重计算（需要较多CPU时间或者运算）的线程则设置较低的优先级，这样才能确保处理器不会被长久独占。当然还有要注意就是在不同的JVM以及操作系统上线程的规划存在差异，有些操作系统甚至会忽略对线程优先级的设定，如mac os系统或者Ubuntu系统…….. 线程的状态转化关系1.新建状态（New）：新创建了一个线程对象。2.就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的start()方法。该状态的线程位于可运行线程池中，变得可运行，等待获取CPU的使用权。3.运行状态（Running）：就绪状态的线程获取了CPU，执行程序代码。4.阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种： 等待阻塞（WAITING）：运行的线程执行wait()方法，JVM会把该线程放入等待池中。 同步阻塞（Blocked）：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。 超时阻塞（TIME_WAITING）：运行的线程执行sleep(long)或join(long)方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。 5.死亡状态（Dead）：线程执行完了或者因异常退出了run()方法，该线程结束生命周期。图中的方法解析如下：Thread.sleep()：在指定时间内让当前正在执行的线程暂停执行，但不会释放”锁标志”。不推荐使用。Thread.sleep(long)：使当前线程进入阻塞状态，在指定时间内不会执行。Object.wait()和Object.wait(long)：在其他线程调用对象的notify或notifyAll方法前，导致当前线程等待。线程会释放掉它所占有的”锁标志”，从而使别的线程有机会抢占该锁。 当前线程必须拥有当前对象锁。如果当前线程不是此锁的拥有者，会抛出IllegalMonitorStateException异常。 唤醒当前对象锁的等待线程使用notify或notifyAll方法，也必须拥有相同的对象锁，否则也会抛出IllegalMonitorStateException异常，waite()和notify()必须在synchronized函数或synchronized中进行调用。如果在non-synchronized函数或non-synchronized中进行调用,虽然能编译通过，但在运行时会发生IllegalMonitorStateException的异常。Object.notifyAll()：则从对象等待池中唤醒所有等待等待线程Object.notify()：则从对象等待池中唤醒其中一个线程。Thread.yield()方法 暂停当前正在执行的线程对象，yield()只是使当前线程重新回到可执行状态，所以执行yield()的线程有可能在进入到可执行状态后马上又被执行，yield()只能使同优先级或更高优先级的线程有执行的机会。Thread.Join()：把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行的线程。比如在线程B中调用了线程A的Join()方法，直到线程A执行完毕后，才会继续执行线程B。好了。本篇线程基础知识介绍到此结束。 参考 http://blog.csdn.net/javazejian/article/details/50878598所有代码在 https://github.com/ejunjsh/java-code/tree/master/src/main/java/com/sky/code/thread]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过反编译深入理解Java String及intern]]></title>
    <url>%2F2016%2F07%2F29%2Fjava-string-intern%2F</url>
    <content type="text"><![CDATA[字符串问题字符串在我们平时的编码工作中其实用的非常多，并且用起来也比较简单，所以很少有人对其做特别深入的研究。倒是面试或者笔试的时候，往往会涉及比较深入和难度大一点的问题。我在招聘的时候也偶尔会问应聘者相关的问题，倒不是说一定要回答的特别正确和深入，通常问这些问题的目的有两个，第一是考察对 JAVA 基础知识的了解程度，第二是考察应聘者对技术的态度。 我们看看以下程序会输出什么结果？如果你能正确的回答每一道题，并且清楚其原因，那本文对你就没什么太大的意义。如果回答不正确或者不是很清楚其原理，那就仔细看看以下的分析，本文应该能帮助你清楚的理解每段程序的结果及输出该结果的深层次原因。 代码段一：1234567891011package com.paddx.test.string;public class StringTest &#123; public static void main(String[] args) &#123; String str1 = "string"; String str2 = new String("string"); String str3 = str2.intern(); System.out.println(str1==str2);//#1 System.out.println(str1==str3);//#2 &#125;&#125; 代码段二：12345678910111213141516171819 package com.paddx.test.string; public class StringTest01 &#123; public static void main(String[] args) &#123; String baseStr = "baseStr"; final String baseFinalStr = "baseStr"; String str1 = "baseStr01"; String str2 = "baseStr"+"01"; String str3 = baseStr + "01"; String str4 = baseFinalStr+"01"; String str5 = new String("baseStr01").intern(); System.out.println(str1 == str2);//#3 System.out.println(str1 == str3);//#4 System.out.println(str1 == str4);//#5 System.out.println(str1 == str5);//#6 &#125;&#125; 代码段三（1）：12345678910package com.paddx.test.string;&lt;br&gt; public class InternTest &#123; public static void main(String[] args) &#123; String str2 = new String("str")+new String("01"); str2.intern(); String str1 = "str01"; System.out.println(str2==str1);//#7 &#125;&#125; 代码段三（2）：12345678910 package com.paddx.test.string; public class InternTest01 &#123; public static void main(String[] args) &#123; String str1 = "str01"; String str2 = new String("str")+new String("01"); str2.intern(); System.out.println(str2 == str1);//#8 &#125;&#125; 为了方便描述，我对上述代码的输出结果由#1~#8进行了编码，下文中蓝色字体部分即为结果。 字符串深入分析代码段一分析字符串不属于基本类型，但是可以像基本类型一样，直接通过字面量赋值，当然也可以通过new来生成一个字符串对象。不过通过字面量赋值的方式和new的方式生成字符串有本质的区别： 通过字面量赋值创建字符串时，会优先在常量池中查找是否已经存在相同的字符串，倘若已经存在，栈中的引用直接指向该字符串；倘若不存在，则在常量池中生成一个字符串，再将栈中的引用指向该字符串。而通过new的方式创建字符串时，就直接在堆中生成一个字符串的对象（备注，JDK 7 以后，HotSpot 已将常量池从永久代转移到了堆中。详细信息可参考《JDK8内存模型-消失的PermGen》一文），栈中的引用指向该对象。对于堆中的字符串对象，可以通过 intern() 方法来将字符串添加的常量池中，并返回指向该常量的引用。现在我们应该能很清楚代码段一的结果了： 结果 #1：因为str1指向的是字符串中的常量，str2是在堆中生成的对象，所以str1==str2返回false。结果 #2：str2调用intern方法，会将str2中值（“string”）复制到常量池中，但是常量池中已经存在该字符串（即str1指向的字符串），所以直接返回该字符串的引用，因此str1==str2返回true。 以下运行代码段一的代码的结果： 代码段二分析对于代码段二的结果，还是通过反编译StringTest01.class文件比较容易理解：常量池内容（部分）：执行指令（部分，第二列#+序数对应常量池中的项）：在解释上述执行过程之前，先了解两条指令： ldc：Push item from run-time constant pool，从常量池中加载指定项的引用到栈。 astore_：Store reference into local variable，将引用赋值给第n个局部变量。 现在我们开始解释代码段二的执行过程： 0: ldc #2：加载常量池中的第二项（”baseStr”）到栈中。 2: astore_1 ：将1中的引用赋值给第一个局部变量，即String baseStr = “baseStr”； 3: ldc #2：加载常量池中的第二项（”baseStr”）到栈中。 5: astore_2 ：将3中的引用赋值给第二个局部变量，即 final String baseFinalStr=”baseStr”； 6: ldc #3：加载常量池中的第三项（”baseStr01”）到栈中。 8: astore_3 ：将6中的引用赋值给第三个局部变量，即String str1=”baseStr01”; 9: ldc #3：加载常量池中的第三项（”baseStr01”）到栈中。 11: astore 4：将9中的引用赋值给第四个局部变量：即String str2=”baseStr01”； 结果#3：str1==str2 肯定会返回true，因为str1和str2都指向常量池中的同一引用地址。所以其实在JAVA 1.6之后，常量字符串的“+”操作，编译阶段直接会合成为一个字符串。13: new #4：生成StringBuilder的实例。16: dup ：复制13生成对象的引用并压入栈中。17: invokespecial #5：调用常量池中的第五项，即StringBuilder.方法。以上三条指令的作用是生成一个StringBuilder的对象。20: aload_1 ：加载第一个参数的值，即”baseStr”21: invokevirtual #6 ：调用StringBuilder对象的append方法。24: ldc #7：加载常量池中的第七项（”01”）到栈中。26: invokevirtual #6：调用StringBuilder.append方法。29: invokevirtual #8：调用StringBuilder.toString方法。32: astore 5：将29中的结果引用赋值改第五个局部变量，即对变量str3的赋值。结果 #4：因为str3实际上是stringBuilder.append()生成的结果，所以与str1不相等，结果返回false。34: ldc #3：加载常量池中的第三项（”baseStr01”）到栈中。36: astore 6：将34中的引用赋值给第六个局部变量，即str4=”baseStr01”;结果 #5 ：因为str1和str4指向的都是常量池中的第三项，所以str1==str4返回true。这里我们还能发现一个现象，对于final字段，编译期直接进行了常量替换，而对于非final字段则是在运行期进行赋值处理的。38: new #9：创建String对象41: dup ：复制引用并压如栈中。42: ldc #3：加载常量池中的第三项（”baseStr01”）到栈中。44: invokespecial #10：调用String.”“方法，并传42步骤中的引用作为参数传入该方法。47: invokevirtual #11：调用String.intern方法。从38到41的对应的源码就是new String(“baseStr01”).intern()。50: astore 7：将47步返回的结果赋值给变量7，即str5指向baseStr01在常量池中的位置。结果 #6 ：因为str5和str1都指向的都是常量池中的同一个字符串，所以str1==str5返回true。运行代码段二，输出结果如下：## 代码段三解析：对于代码段三，在 JDK 1.6 和 JDK 1.7中的运行结果不同。我们先看一下运行结果，然后再来解释其原因：JDK 1.6 下的运行结果：JDK 1.7 下的运行结果：根据对代码段一的分析，应该可以很简单得出 JDK 1.6 的结果，因为 str2 和 str1本来就是指向不同的位置，理应返回false。比较奇怪的问题在于JDK 1.7后，对于第一种情况返回true，但是调换了一下位置返回的结果就变成了false。这个原因主要是从JDK 1.7后，HotSpot 将常量池从永久代移到了元空间，正因为如此，JDK 1.7 后的intern方法在实现上发生了比较大的改变，JDK 1.7后，intern方法还是会先去查询常量池中是否有已经存在，如果存在，则返回常量池中的引用，这一点与之前没有区别，区别在于，如果在常量池找不到对应的字符串，则不会再将字符串拷贝到常量池，而只是在常量池中生成一个对原字符串的引用。所以:结果 #7：在第一种情况下，因为常量池中没有“str01”这个字符串，所以会在常量池中生成一个对堆中的“str01”的引用，而在进行字面量赋值的时候，常量池中已经存在，所以直接返回该引用即可，因此str1和str2都指向堆中的字符串，返回true。结果 #8：调换位置以后，因为在进行字面量赋值（String str1 = “str01”）的时候，常量池中不存在，所以str1指向的常量池中的位置，而str2指向的是堆中的对象，再进行intern方法时，对str1和str2已经没有影响了，所以返回false。 常见面试题解答有了对以上的知识的了解，我们现在再来看常见的面试或笔试题就很简单了： Q：String s = new String(“xyz”)，创建了几个String Object? A：两个，常量池中的”xyz”和堆中对象。 Q：下列程序的输出结果： String s1 = “abc”;String s2 = “abc”;System.out.println(s1 == s2); A：true，均指向常量池中对象。 Q：下列程序的输出结果： String s1 = new String(“abc”);String s2 = new String(“abc”);System.out.println(s1 == s2); A：false，两个引用指向堆中的不同对象。 Q：下列程序的输出结果： String s1 = “abc”;String s2 = “a”;String s3 = “bc”;String s4 = s2 + s3;System.out.println(s1 == s4); A：false，因为s2+s3实际上是使用StringBuilder.append来完成，会生成不同的对象。 Q：下列程序的输出结果： String s1 = “abc”;final String s2 = “a”;final String s3 = “bc”;String s4 = s2 + s3;System.out.println(s1 == s4); A：true，因为final变量在编译后会直接替换成对应的值，所以实际上等于s4=”a”+”bc”，而这种情况下，编译器会直接合并为s4=”abc”，所以最终s1==s4。 Q：下列程序的输出结果： String s = new String(“abc”);String s1 = “abc”;String s2 = new String(“abc”); System.out.println(s == s1.intern());System.out.println(s == s2.intern());System.out.println(s1 == s2.intern()); A：false，false，true，具体原因参考第二部分内容。 原文地址 http://www.cnblogs.com/paddix/p/5326863.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8内存模型—永久代(PermGen)和元空间(Metaspace)]]></title>
    <url>%2F2016%2F07%2F26%2Fjava8-permgen-metaspace%2F</url>
    <content type="text"><![CDATA[JVM 内存模型根据 JVM 规范，JVM 内存共分为虚拟机栈、堆、方法区、程序计数器、本地方法栈五个部分。 虚拟机栈每个线程有一个私有的栈，随着线程的创建而创建。栈里面存着的是一种叫“栈帧”的东西，每个方法会创建一个栈帧，栈帧中存放了局部变量表（基本数据类型和对象引用）、操作数栈、方法出口等信息。栈的大小可以固定也可以动态扩展。当栈调用深度大于JVM所允许的范围，会抛出StackOverflowError的错误，不过这个深度范围不是一个恒定的值，我们通过下面这段程序可以测试一下这个结果：栈溢出测试源码：1234567891011121314151617181920package com.paddx.test.memory; public class StackErrorMock &#123; private static int index = 1; public void call()&#123; index++; call(); &#125; public static void main(String[] args) &#123; StackErrorMock mock = new StackErrorMock(); try &#123; mock.call(); &#125;catch (Throwable e)&#123; System.out.println("Stack deep : "+index); e.printStackTrace(); &#125; &#125;&#125; 代码段1 运行三次，可以看出每次栈的深度都是不一样的，输出结果如下。至于红色框里的值是怎么出来的，就需要深入到 JVM 的源码中才能探讨，这里不作详细阐述。 虚拟机栈除了上述错误外，还有另一种错误，那就是当申请不到空间时，会抛出 OutOfMemoryError。这里有一个小细节需要注意，catch 捕获的是 Throwable，而不是 Exception。因为 StackOverflowError 和 OutOfMemoryError 都不属于 Exception 的子类。 本地方法栈这部分主要与虚拟机用到的 Native 方法相关，一般情况下， Java 应用程序员并不需要关心这部分的内容 PC 寄存器PC 寄存器，也叫程序计数器。JVM支持多个线程同时运行，每个线程都有自己的程序计数器。倘若当前执行的是 JVM 的方法，则该寄存器中保存当前执行指令的地址；倘若执行的是native 方法，则PC寄存器中为空。 堆堆内存是 JVM 所有线程共享的部分，在虚拟机启动的时候就已经创建。所有的对象和数组都在堆上进行分配。这部分空间可通过 GC 进行回收。当申请不到空间时会抛出 OutOfMemoryError。下面我们简单的模拟一个堆内存溢出的情况：12345678910111213141516171819202122package com.paddx.test.memory; import java.util.ArrayList;import java.util.List; public class HeapOomMock &#123; public static void main(String[] args) &#123; List&lt;byte[]&gt; list = new ArrayList&lt;byte[]&gt;(); int i = 0; boolean flag = true; while (flag)&#123; try &#123; i++; list.add(new byte[1024 * 1024]);//每次增加一个1M大小的数组对象 &#125;catch (Throwable e)&#123; e.printStackTrace(); flag = false; System.out.println("count="+i);//记录运行的次数 &#125; &#125; &#125;&#125; 代码段2 运行上述代码，输出结果如下： 注意，这里我指定了堆内存的大小为16M，所以这个地方显示的count=14（这个数字不是固定的），至于为什么会是14或其他数字，需要根据 GC 日志来判断 方法区 方法区也是所有线程共享。主要用于存储类的信息、常量池、方法数据、方法代码等。方法区逻辑上属于堆的一部分，但是为了与堆进行区分，通常又叫“非堆”。 关于方法区内存溢出的问题会在下文中详细探讨。 PermGen（永久代）绝大部分 Java 程序员应该都见过 “java.lang.OutOfMemoryError: PermGen space “这个异常。这里的 “PermGen space”其实指的就是方法区。不过方法区和“PermGen space”又有着本质的区别。前者是 JVM 的规范，而后者则是 JVM 规范的一种实现，并且只有 HotSpot 才有 “PermGen space”，而对于其他类型的虚拟机，如 JRockit（Oracle）、J9（IBM） 并没有“PermGen space”。由于方法区主要存储类的相关信息，所以对于动态生成类的情况比较容易出现永久代的内存溢出。最典型的场景就是，在 jsp 页面比较多的情况，容易出现永久代内存溢出。我们现在通过动态生成类来模拟 “PermGen space”的内存溢出：1234package com.paddx.test.memory; public class Test &#123;&#125; 12345678910111213141516171819202122232425package com.paddx.test.memory; import java.io.File;import java.net.URL;import java.net.URLClassLoader;import java.util.ArrayList;import java.util.List; public class PermGenOomMock&#123; public static void main(String[] args) &#123; URL url = null; List&lt;ClassLoader&gt; classLoaderList = new ArrayList&lt;ClassLoader&gt;(); try &#123; url = new File("/tmp").toURI().toURL(); URL[] urls = &#123;url&#125;; while (true)&#123; ClassLoader loader = new URLClassLoader(urls); classLoaderList.add(loader); loader.loadClass("com.paddx.test.memory.Test"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 代码段3 运行结果如下： 本例中使用的 JDK 版本是 1.7，指定的 PermGen 区的大小为 8M。通过每次生成不同URLClassLoader对象来加载Test类，从而生成不同的类对象，这样就能看到我们熟悉的 “java.lang.OutOfMemoryError: PermGen space “ 异常了。这里之所以采用 JDK 1.7，是因为在 JDK 1.8 中， HotSpot 已经没有 “PermGen space”这个区间了，取而代之是一个叫做 Metaspace（元空间） 的东西。下面我们就来看看 Metaspace 与 PermGen space 的区别。 Metaspace（元空间）其实，移除永久代的工作从JDK1.7就开始了。JDK1.7中，存储在永久代的部分数据就已经转移到了Java Heap或者是 Native Heap。但永久代仍存在于JDK1.7中，并没完全移除，譬如符号引用(Symbols)转移到了native heap；字面量(interned strings)转移到了java heap；类的静态变量(class statics)转移到了java heap。我们可以通过一段程序来比较 JDK 1.6 与 JDK 1.7及 JDK 1.8 的区别，以字符串常量为例：12345678910111213141516package com.paddx.test.memory; import java.util.ArrayList;import java.util.List; public class StringOomMock &#123; static String base = "string"; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); for (int i=0;i&lt; Integer.MAX_VALUE;i++)&#123; String str = base + base; base = str; list.add(str.intern()); &#125; &#125;&#125; 代码段4 这段程序以2的指数级不断的生成新的字符串，这样可以比较快速的消耗内存。我们通过 JDK 1.6、JDK 1.7 和 JDK 1.8 分别运行：JDK 1.6 的运行结果：JDK 1.7的运行结果：JDK 1.8的运行结果：从上述结果可以看出，JDK 1.6下，会出现“PermGen Space”的内存溢出，而在 JDK 1.7和 JDK 1.8 中，会出现堆内存溢出，并且 JDK 1.8中 PermSize 和 MaxPermGen 已经无效。因此，可以大致验证 JDK 1.7 和 1.8 将字符串常量由永久代转移到堆中，并且 JDK 1.8 中已经不存在永久代的结论。现在我们看看元空间到底是一个什么东西？ 元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制，但可以通过以下参数来指定元空间的大小： -XX:MetaspaceSize，初始空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。 -XX:MaxMetaspaceSize，最大空间，默认是没有限制的。 除了上面两个指定大小的选项以外，还有两个与 GC 相关的属性： -XX:MinMetaspaceFreeRatio，在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集 -XX:MaxMetaspaceFreeRatio，在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集 现在我们在 JDK 8下重新运行一下代码段3，不过这次不再指定 PermSize 和 MaxPermSize。而是指定 MetaSpaceSize 和 MaxMetaSpaceSize的大小。输出结果如下：从输出结果，我们可以看出，这次不再出现永久代溢出，而是出现了元空间的溢出。 总结 通过上面分析，大家应该大致了解了 JVM 的内存划分，也清楚了 JDK 8 中永久代向元空间的转换。不过大家应该都有一个疑问，就是为什么要做这个转换？所以，最后给大家总结以下几点原因： 字符串存在永久代中，容易出现性能问题和内存溢出。 类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代溢出。 永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。 Oracle 可能会将HotSpot 与 JRockit 合二为一。 原文地址 http://www.cnblogs.com/paddix/p/5309550.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从字节码层面看“HelloWorld”]]></title>
    <url>%2F2016%2F07%2F23%2Fbytecode-hello-world%2F</url>
    <content type="text"><![CDATA[HelloWorld 字节码生成众所周知，Java 程序是在 JVM 上运行的，不过 JVM 运行的其实不是 Java 语言本身，而是 Java 程序编译成的字节码文件。可能一开始 JVM 是为 Java 语言服务的，不过随着编译技术和 JVM 自身的不断发展和成熟，JVM 已经不仅仅只运行 Java 程序。任何能编译成为符合 JVM 字节码规范的语言都可以在 JVM 上运行，比较常见的 Scala、Groove、JRuby等。今天，我就从大家最熟悉的程序“HelloWorld”程序入手，分析整个 Class 文件的结构。虽然这个程序比较简单，但是基本上包含了字节码规范中的所有内容，因此即使以后要分析更复杂的程序，那也只是“量”上的变化，本质上没有区别。 我们先直观的看下源码与字节码之间的对应关系:HelloWorld的源码：1234567package com.paddx.test.asm; public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println("Hello,World!"); &#125;&#125; 编译器采用JDK 1.7：12345678&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; 编译以后的字节码文件（使用UltraEdit的16进制模式打开）：红色框内的部分就是HelloWorld.class的内容，其他部分是UltraEdit自动生成的：红色框顶部的0~f代表列号，左边部分代表行号，右侧部分是二进制码对应的字符（utf-8编码）。 字节码解析要弄明白 HelloWorld.java 和 HelloWorld.class 文件是如何对应的，我们必须对 JVM 的字节码规范有所了解。字节码文件的结构非常紧凑，没有任何冗余的信息，连分隔符都没有，它采用的是固定的文件结构和数据类型来实现对内容的分割的。字节码中包括两种数据类型：无符号数和表。无符号数又包括 u1，u2，u4，u8四种，分别代表1个字节、2个字节、4个字节和8个字节。而表结构则是由无符号数据组成的。 字节码文件的格式固定如下： type descriptor u4 magic u2 minor_version u2 major_version u2 constant_pool_count cp_info constant_pool[cosntant_pool_count – 1] u2 access_flags u2 this_class u2 super_class u2 interfaces_count u2 interfaces[interfaces_count] u2 fields_count field_info fields[fields_count] u2 methods_count method_info methods[methods_count] u2 attributes_count attribute_info attributes[attributes_count] 现在，我们就按这个格式对上述HelloWorld.class文件进行分析： magic（u4）：CA FE BA BE ，代表该文件是一个字节码文件，我们平时区分文件类型都是通过后缀名来区分的，不过后缀名是可以随便修改的，所以仅靠后缀名不能真正区分一个文件的类型。区分文件类型的另个办法就是magic数字，JVM 就是通过 CA FE BA BE 来判断该文件是不是class文件。 minor_version（u2）：00 00，小版本号，因为我这里采用的1.7，所以小版本号为0. major_version（u2）：00 33，大版本号，x033转换为十进制为51，下表是jdk 1.6 以后对应支持的 Class 文件版本号： 编译器版本 -target参数 十六进制版本 十进制版本 JDK 1.6.0_01 不带（默认 -target 1.6） 00 00 00 32 50.0 JDK 1.6.0_01 -target 1.5 00 00 00 31 49.0 JDK 1.6.0_01 -target 1.4 -source 1.4 00 00 00 30 48.0 JDK 1.7.0 不带（默认 -target 1.7） 00 00 00 33 51.0 JDK 1.7.0 -target 1.6 00 00 00 32 50.0 JDK 1.7.0 -target 1.4 -source 1.4 00 00 00 30 48.0 JDK 1.8.0 不带（默认 -target 1.8） 00 00 00 34 52.0 constant_pool_count（u2）：00 22，常量池数量，转换为十进制后为34，这里需要注意的是，字节码的常量池是从1开始计数的，所以34表示为（34-1）=33项。 TAG（u1）：0A，常量池的数据类型是表，每一项的开始都有一个tag（u1），表示常量的类型，常量池的表的类型包括如下14种，这里A（10）表示CONSTANT_Methodref，代表方法引用。 常量类型 值 CONSTANT_Utf8_info 1 CONSTANT_Integer_info 3 CONSTANT_Float_info 4 CONSTANT_Long_info 5 CONSTANT_Double_info 6 CONSTANT_Class_info 7 CONSTANT_String_info 8 CONSTANT_Fieldref_info 9 CONSTANT_Methodref_info 10 CONSTANT_InterfaceMethodref_info 11 CONSTANT_NameAndType_info 12 CONSTANT_MethodHandle_info 15 CONSTANT_MethodType_info 16 CONSTANT_InvokeDynamic_info 18 每种常量类型对应表结构： 常量 项目 类型 描述 CONSTANT_Utf8_info tag u1 1 length u2 字节数 bytes u1 utf-8编码的字符串 CONSTANT_Integer_info tag u1 3 bytes u4 int值 CONSTANT_Float_info tag u4 4 bytes u1 float值 CONSTANT_Long_info tag u1 5 bytes u8 long值 CONSTANT_Double_info tag u1 6 bytes u8 double值 CONSTANT_Class_info tag u1 7 index u2 指向全限定名常量项的索引 CONSTANT_String_info tag u1 8 index u2 指向字符串常量的索引 CONSTANT_Fieldref_info tag u1 9 index u2 指向声明字段的类或接口描述符CONSTANT_Class_info的索引值 index u2 指向CONSTANT_NameAndType_info的索引值 CONSTANT_Methodref_info tag u1 10 index u2 指向声明方法的类描述符CONSTANT_Class_info的索引值 index u2 指向CONSTANT_NameAndType_info的索引值 CONSTANT_InterfaceMethodref_info tag u1 11 index u2 指向声明方法的接口描述符CONSTANT_Class_info的索引值 index u2 指向CONSTANT_NameAndType_info的索引值 CONSTANT_NameAndType_info tag u1 12 index u2 指向该字段或方法名称常量的索引值 index u2 指向该字段或方法描述符常量的索引值 CONSTANT_MethodHandle_info tag u1 15 reference_kind u1 值必须1~9，它决定了方法句柄的的类型 reference_index u2 对常量池的索引 CONSTANT_MethodType_info tag u1 16 description_index u2 对常量池中方法描述符的索引 CONSTANT_InvokeDynamic_info tag u1 18 bootstap_method_attr_index u2 对引导方法表的索引 name_and_type_index u2 对CONSTANT_NameAndType_info的索引 CONSTANT_Methodref_info（u2):00 06，因为tag为A，代表一个方法引用表（CONSTANT_Methodref_info），所以第二项（u2）应该是指向常量池的位置，即常量池的第六项，表示一个CONSTANT_Class_info表的索引，用类似的方法往下分析，可以发现常量池的第六项如下，tag类型为07，查询上表可知道其即为CONSTANT_Class_info。 07之后的00 1B表示对常量池地27项（CONSTANT_Utf8_info）的引用，查看第27项如下图，即（java/lang/Object）： CONSTANT_NameAndType_info（u2）：00 14,方法引用表的第三项（u2），常量池索引，指向第20项。 CONSTANT_Fieldref_info（u1）：tag为09。 ….. 常量池的分析都类似，其他的分析由于篇幅问题就不在此一一讲述了。跳过常量池就到了访问标识（u2）： JVM 对访问标示符的规范如下： Flag Name Value Remarks ACC_PUBLIC 0x0001 pubilc ACC_FINAL 0x0010 final ACC_SUPER 0x0020 用于兼容早期编译器，新编译器都设置该标记，以在使用 invokespecial指令时对子类方法做特定处理。 ACC_INTERFACE 0x0200 接口，同时需要设置：ACC_ABSTRACT。不可同时设置：ACC_FINAL、ACC_SUPER、ACC_ENUM ACC_ABSTRACT 0x0400 抽象类，无法实例化。不可与ACC_FINAL同时设置。 ACC_SYNTHETIC 0x1000 synthetic，由编译器产生，不存在于源代码中。 ACC_ANNOTATION 0x2000 注解类型（annotation），需同时设置：ACC_INTERFACE、ACC_ABSTRACT ACC_ENUM 0x4000 枚举类型 这个表里面无法直接查询到0021这个值，原因是0021=0020+0001，即public+invokespecial指令，源码中的方法main是public的，而invokespecial是现在的版本都有的，所以值为0021。 接着往下是this_class（u2）：是指向constant pool的索引值，该值必须是CONSTANT_Class_info类型，值为00 05，即指向常量池中的第五项，第五项指向常量池中的第26项，即com/paddx/test/asm/HelloWorld： super_class(u2)）：super_class是指向constant pool的索引值，该值必须是CONSTANT_Class_info类型，指定当前字节码定义的类或接口的直接父类。这里的取值为00 06，根据上面的分析，对应的指向的全限定性类名为java/lang/object，即当前类的父类为Object类。 interfaces_count（u2）：接口的数量，因为这里没有实现接口，所以值为 00 00。 interfaces[interfaces_count]：因为没有接口，所以就不存在interfces选项。 field_count：属性数量，00 00。 field_info：因为没有属性，所以不存在这个选项。 method_count：00 02，为什么会有两个方法呢？我们明明只写了一个方法，这是因为JVM 会自动生成一个 的方法。 method_info：方法表，其结构如下： Type Descriptor u2 access_flag u2 name_index u2 descriptor_index u2 attributes_count attribute_info attribute_info[attributes_count] HelloWorld.class文件中对应的数据： access_flag（u2）: 00 01 name_index（u2）:00 07 descriptor_index（u2）:00 08 可以看看 07、08对应的常量池里面的值： 即 07 对应的是 &#60;init&#62;，08 对应的是()； attributes_count:00 01，表示包含一个属性 attribute_info：属性表，该表的结构如下： Type Descriptor u2 attribute_name_index u4 attribute_length u1 bytes attribute_name_index（u2）: 00 09，指向常量池中的索引。 attribute_length（u4）：00 00 00 2F，属性的长度47。 attribute_info:具体属性的分析与上面类似，大家可以对着JVM的规范自己尝试分析一下。 第一个方法结束后，接着进入第二个方法： 第二个方法的属性长度为x037，转换为十进制为55个字节。两个方法之后紧跟着的是attribute_count和attributes： attribute_count（u2）:值为 00 01，即有一个属性。 attribute_name_index（u2）：指向常量池中的第十二项。 attribute_length（u4）：00 00 00 02，长度为2。 分析完毕！ 基于字节码的操作 通过对HelloWorld这个程序的字节码分析，我们应该能够比较清楚的认识到整个字节码的结构。那我们通过字节码，可以做些什么呢？其实通过字节码能做很多平时我们无法完成的工作。比如，在类加载之前添加某些操作或者直接动态的生成字节码，CGlib就是通过这种方式来实现动态代理的。现在，我们就来完成另一个版本的HelloWorld：1234567package com.paddx.test.asm; public class HelloWorld2 &#123; public static void sayHello()&#123; &#125;&#125; 我们有个空的方法 sayHello()，现在要实现调该方法的时候打印出“HelloWorld”，怎么处理？如果我们手动去修改字节码文件，将打印“HelloWorld”的代码插入到sayHello方法中，原理上肯定没问题，不过操作过程还是比较复杂的。Java 的最大优势就在于只要你能想到的功能，基本上就有第三方开源的库实现过。字节码操作的开源库也比较多，这里我就用 ASM 4.0来实现该功能：12345678910111213141516171819202122232425262728293031323334353637383940package com.paddx.test.asm; import org.objectweb.asm.*; import java.io.IOException;import java.lang.reflect.InvocationTargetException; public class AsmDemo extends ClassLoader&#123; public static void main(String[] args) throws IOException, IllegalAccessException, InstantiationException, InvocationTargetException &#123; ClassReader classReader = new ClassReader("com.paddx.test.asm.HelloWorld2"); ClassWriter cw=new ClassWriter(ClassWriter.COMPUTE_MAXS); CustomVisitor myv=new CustomVisitor(Opcodes.ASM4,cw); classReader.accept(myv, 0); byte[] code=cw.toByteArray(); AsmDemo loader=new AsmDemo(); Class&lt;?&gt; appClass=loader.defineClass(null, code, 0,code.length); appClass.getMethods()[0].invoke(appClass.newInstance(), new Object[]&#123;&#125;); &#125; &#125; class CustomVisitor extends ClassVisitor implements Opcodes &#123; public CustomVisitor(int api, ClassVisitor cv) &#123; super(api, cv); &#125; @Override public MethodVisitor visitMethod(int access, String name, String desc, String signature, String[] exceptions) &#123; MethodVisitor mv = super.visitMethod(access, name, desc, signature, exceptions); if (name.equals("sayHello")) &#123; mv.visitFieldInsn(GETSTATIC, "java/lang/System", "out", "Ljava/io/PrintStream;"); mv.visitLdcInsn("HelloWorld!"); mv.visitMethodInsn(INVOKEVIRTUAL, "java/io/PrintStream", "println", "(Ljava/lang/String;)V"); &#125; return mv; &#125;&#125; 运行结果如下： 关于 ASM 4的操作在这就不细说了。有兴趣的朋友可以自己去研究一下，有机会，我也可以再后续的博文中跟大家分享。 总结 本文通过HelloWorld这样一个大家都非常熟悉的例子，深入的分析了字节码文件的结构。利用这些特性，我们可以完成一些相对高级的功能，如动态代理等。这些例子虽然都很简单，但是“麻雀虽小五脏俱全”，即使再复杂的程序也逃离不了这些最基本的东西。技术层面的东西就是这样子，只要你能了解一个简单的程序的原理，举一反三，就能很容易的理解更复杂的程序，这就是技术“易”的方面。同时，反过来说，即使“HelloWorld”这样一个简单的程序，如果我们深入探究，也不一定能特别理解其原理，这就是技术“难”的方面。总之，技术这种东西只要你用心深入地去研究，总是能带给你意想不到的惊喜~ 原文地址 http://www.cnblogs.com/paddix/p/5282004.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[curl 命令]]></title>
    <url>%2F2016%2F04%2F24%2Flinux-curl%2F</url>
    <content type="text"><![CDATA[mark一下，当笔记，以后忘了查看 描述命令：curl在Linux中curl是一个利用URL规则在命令行下工作的文件传输工具，可以说是一款很强大的http命令行工具。它支持文件的上传和下载，是综合传输工具，但按传统，习惯称url为下载工具。1语法：# curl [option] [url] 常见参数：12345678910111213141516-A/--user-agent &lt;string&gt; 设置用户代理发送给服务器-b/--cookie &lt;name=string/file&gt; cookie字符串或文件读取位置-c/--cookie-jar &lt;file&gt; 操作结束后把cookie写入到这个文件中-C/--continue-at &lt;offset&gt; 断点续转-D/--dump-header &lt;file&gt; 把header信息写入到该文件中-e/--referer 来源网址-f/--fail 连接失败时不显示http错误-o/--output 把输出写到该文件中-O/--remote-name 把输出写到该文件中，保留远程文件的文件名-r/--range &lt;range&gt; 检索来自HTTP/1.1或FTP服务器字节范围-s/--silent 静音模式。不输出任何东西-T/--upload-file &lt;file&gt; 上传文件-u/--user &lt;user[:password]&gt; 设置服务器的用户和密码-w/--write-out [format] 什么输出完成后-x/--proxy &lt;host[:port]&gt; 在给定的端口上使用HTTP代理-#/--progress-bar 进度条显示当前的传送状态 基本用法1# curl http://www.linux.com 执行后，www.linux.com 的html就会显示在屏幕上了Ps：由于安装linux的时候很多时候是没有安装桌面的，也意味着没有浏览器，因此这个方法也经常用于测试一台服务器是否可以到达一个网站 保存访问的网页使用linux的重定向功能保存1# curl http://www.linux.com &gt;&gt; linux.html 可以使用curl的内置option:-o(小写)保存网页1$ curl -o linux.html http://www.linux.com 执行完成后会显示如下界面，显示100%则表示保存成功123% Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 79684 0 79684 0 0 3437k 0 --:--:-- --:--:-- --:--:-- 7781k 可以使用curl的内置option:-O(大写)保存网页中的文件要注意这里后面的url要具体到某个文件，不然抓不下来1# curl -O http://www.linux.com/hello.sh 测试网页返回值1# curl -o /dev/null -s -w %&#123;http_code&#125; www.linux.com Ps:在脚本中，这是很常见的测试网站是否正常的用法 指定proxy服务器以及其端口很多时候上网需要用到代理服务器(比如是使用代理服务器上网或者因为使用curl别人网站而被别人屏蔽IP地址的时候)，幸运的是curl通过使用内置option：-x来支持设置代理1# curl -x 192.168.100.100:1080 http://www.linux.com cookie有些网站是使用cookie来记录session信息。对于chrome这样的浏览器，可以轻易处理cookie信息，但在curl中只要增加相关参数也是可以很容易的处理cookie 保存http的response里面的cookie信息。内置option:-c（小写）1curl -c cookiec.txt http://www.linux.com 执行后cookie信息就被存到了cookiec.txt里面了 保存http的response里面的header信息。内置option: -D1# curl -D cookied.txt http://www.linux.com 执行后cookie信息就被存到了cookied.txt里面了 注意：-c(小写)产生的cookie和-D里面的cookie是不一样的。 使用cookie很多网站都是通过监视你的cookie信息来判断你是否按规矩访问他们的网站的，因此我们需要使用保存的cookie信息。内置option: -b1# curl -b cookiec.txt http://www.linux.com 模仿浏览器有些网站需要使用特定的浏览器去访问他们，有些还需要使用某些特定的版本。curl内置option:-A可以让我们指定浏览器去访问网站1# curl -A &quot;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.0)&quot; http://www.linux.com 这样服务器端就会认为是使用IE8.0去访问的 伪造referer（盗链）很多服务器会检查http访问的referer从而来控制访问。比如：你是先访问首页，然后再访问首页中的邮箱页面，这里访问邮箱的referer地址就是访问首页成功后的页面地址，如果服务器发现对邮箱页面访问的referer地址不是首页的地址，就断定那是个盗连了curl中内置option：-e可以让我们设定referer1# curl -e &quot;www.linux.com&quot; http://mail.linux.com 这样就会让服务器其以为你是从www.linux.com点击某个链接过来的 下载文件利用curl下载文件。使用内置option：-o(小写)1# curl -o dodo1.jpg http:www.linux.com/dodo1.JPG 使用内置option：-O（大写)1# curl -O http://www.linux.com/dodo1.JPG 这样就会以服务器上的名称保存文件到本地 循环下载有时候下载图片可以能是前面的部分名称是一样的，就最后的尾椎名不一样1# curl -O http://www.linux.com/dodo[1-5].JPG 这样就会把dodo1，dodo2，dodo3，dodo4，dodo5全部保存下来 下载重命名1# curl -O http://www.linux.com/&#123;hello,bb&#125;/dodo[1-5].JPG 由于下载的hello与bb中的文件名都是dodo1，dodo2，dodo3，dodo4，dodo5。因此第二次下载的会把第一次下载的覆盖，这样就需要对文件进行重命名。1# curl -o #1_#2.JPG http://www.linux.com/&#123;hello,bb&#125;/dodo[1-5].JPG 这样在hello/dodo1.JPG的文件下载下来就会变成hello_dodo1.JPG,其他文件依此类推，从而有效的避免了文件被覆盖 分块下载有时候下载的东西会比较大，这个时候我们可以分段下载。使用内置option：-r1234# curl -r 0-100 -o dodo1_part1.JPG http://www.linux.com/dodo1.JPG# curl -r 100-200 -o dodo1_part2.JPG http://www.linux.com/dodo1.JPG# curl -r 200- -o dodo1_part3.JPG http://www.linux.com/dodo1.JPG# cat dodo1_part* &gt; dodo1.JPG 这样就可以查看dodo1.JPG的内容了 通过ftp下载文件curl可以通过ftp下载文件，curl提供两种从ftp中下载的语法12# curl -O -u 用户名:密码 ftp://www.linux.com/dodo1.JPG# curl -O ftp://用户名:密码@www.linux.com/dodo1.JPG 显示下载进度条1# curl -# -O http://www.linux.com/dodo1.JPG 不会显示下载进度信息1# curl -s -O http://www.linux.com/dodo1.JPG 断点续传在windows中，我们可以使用迅雷这样的软件进行断点续传。curl可以通过内置option:-C同样可以达到相同的效果如果在下载dodo1.JPG的过程中突然掉线了，可以使用以下的方式续传1# curl -C -O http://www.linux.com/dodo1.JPG 上传文件curl不仅仅可以下载文件，还可以上传文件。通过内置option:-T来实现1# curl -T dodo1.JPG -u 用户名:密码 ftp://www.linux.com/img/ 这样就向ftp服务器上传了文件dodo1.JPG 显示抓取错误1# curl -f http://www.linux.com/error 其他参数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192-a/--append 上传文件时，附加到目标文件--anyauth 可以使用“任何”身份验证方法--basic 使用HTTP基本验证-B/--use-ascii 使用ASCII文本传输-d/--data &lt;data&gt; HTTP POST方式传送数据--data-ascii &lt;data&gt; 以ascii的方式post数据--data-binary &lt;data&gt; 以二进制的方式post数据--negotiate 使用HTTP身份验证--digest 使用数字身份验证--disable-eprt 禁止使用EPRT或LPRT--disable-epsv 禁止使用EPSV--egd-file &lt;file&gt; 为随机数据(SSL)设置EGD socket路径--tcp-nodelay 使用TCP_NODELAY选项-E/--cert &lt;cert[:passwd]&gt; 客户端证书文件和密码 (SSL)--cert-type &lt;type&gt; 证书文件类型 (DER/PEM/ENG) (SSL)--key &lt;key&gt; 私钥文件名 (SSL)--key-type &lt;type&gt; 私钥文件类型 (DER/PEM/ENG) (SSL)--pass &lt;pass&gt; 私钥密码 (SSL)--engine &lt;eng&gt; 加密引擎使用 (SSL). &quot;--engine list&quot; for list--cacert &lt;file&gt; CA证书 (SSL)--capath &lt;directory&gt; CA目 (made using c_rehash) to verify peer against (SSL)--ciphers &lt;list&gt; SSL密码--compressed 要求返回是压缩的形势 (using deflate or gzip)--connect-timeout &lt;seconds&gt; 设置最大请求时间--create-dirs 建立本地目录的目录层次结构--crlf 上传是把LF转变成CRLF--ftp-create-dirs 如果远程目录不存在，创建远程目录--ftp-method [multicwd/nocwd/singlecwd] 控制CWD的使用--ftp-pasv 使用 PASV/EPSV 代替端口--ftp-skip-pasv-ip 使用PASV的时候,忽略该IP地址--ftp-ssl 尝试用 SSL/TLS 来进行ftp数据传输--ftp-ssl-reqd 要求用 SSL/TLS 来进行ftp数据传输-F/--form &lt;name=content&gt; 模拟http表单提交数据-form-string &lt;name=string&gt; 模拟http表单提交数据-g/--globoff 禁用网址序列和范围使用&#123;&#125;和[]-G/--get 以get的方式来发送数据-h/--help 帮助-H/--header &lt;line&gt; 自定义头信息传递给服务器--ignore-content-length 忽略的HTTP头信息的长度-i/--include 输出时包括protocol头信息-I/--head 只显示文档信息-j/--junk-session-cookies 读取文件时忽略session cookie--interface &lt;interface&gt; 使用指定网络接口/地址--krb4 &lt;level&gt; 使用指定安全级别的krb4-k/--insecure 允许不使用证书到SSL站点-K/--config 指定的配置文件读取-l/--list-only 列出ftp目录下的文件名称--limit-rate &lt;rate&gt; 设置传输速度--local-port&lt;NUM&gt; 强制使用本地端口号-m/--max-time &lt;seconds&gt; 设置最大传输时间--max-redirs &lt;num&gt; 设置最大读取的目录数--max-filesize &lt;bytes&gt; 设置最大下载的文件总量-M/--manual 显示全手动-n/--netrc 从netrc文件中读取用户名和密码--netrc-optional 使用 .netrc 或者 URL来覆盖-n--ntlm 使用 HTTP NTLM 身份验证-N/--no-buffer 禁用缓冲输出-p/--proxytunnel 使用HTTP代理--proxy-anyauth 选择任一代理身份验证方法--proxy-basic 在代理上使用基本身份验证--proxy-digest 在代理上使用数字身份验证--proxy-ntlm 在代理上使用ntlm身份验证-P/--ftp-port &lt;address&gt; 使用端口地址，而不是使用PASV-Q/--quote &lt;cmd&gt; 文件传输前，发送命令到服务器--range-file 读取（SSL）的随机文件-R/--remote-time 在本地生成文件时，保留远程文件时间--retry &lt;num&gt; 传输出现问题时，重试的次数--retry-delay &lt;seconds&gt; 传输出现问题时，设置重试间隔时间--retry-max-time &lt;seconds&gt; 传输出现问题时，设置最大重试时间-S/--show-error 显示错误--socks4 &lt;host[:port]&gt; 用socks4代理给定主机和端口--socks5 &lt;host[:port]&gt; 用socks5代理给定主机和端口-t/--telnet-option &lt;OPT=val&gt; Telnet选项设置--trace &lt;file&gt; 对指定文件进行debug--trace-ascii &lt;file&gt; Like --跟踪但没有hex输出--trace-time 跟踪/详细输出时，添加时间戳--url &lt;URL&gt; Spet URL to work with-U/--proxy-user &lt;user[:password]&gt; 设置代理用户名和密码-V/--version 显示版本信息-X/--request &lt;command&gt; 指定什么命令-y/--speed-time 放弃限速所要的时间。默认为30-Y/--speed-limit 停止传输速度的限制，速度时间&apos;秒-z/--time-cond 传送时间设置-0/--http1.0 使用HTTP 1.0-1/--tlsv1 使用TLSv1（SSL）-2/--sslv2 使用SSLv2的（SSL）-3/--sslv3 使用的SSLv3（SSL）--3p-quote like -Q for the source URL for 3rd party transfer--3p-url 使用url，进行第三方传送--3p-user 使用用户名和密码，进行第三方传送-4/--ipv4 使用IP4-6/--ipv6 使用IP6]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>curl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis cluster管理工具redis-trib.rb详解]]></title>
    <url>%2F2016%2F04%2F22%2Fredis-trib%2F</url>
    <content type="text"><![CDATA[redis-trib.rb是redis官方推出的管理redis集群的工具，集成在redis的源码src目录下，是基于redis提供的集群命令封装成简单、便捷、实用的操作工具。redis-trib.rb是redis作者用ruby完成的。为了看懂redis-trib.rb，我特意花了一个星期学习了ruby，也被ruby的简洁、明了所吸引。ruby是门非常灵活的语言，redis-trib.rb只用了1600行左右的代码，就实现了强大的集群操作。本文对redis-trib.rb的介绍是基于redis 3.0.6版本的源码。阅读本文需要对redis集群功能有一定的了解。关于redis集群功能的介绍，可以参考本人的另一篇文章《redis3.0 cluster功能介绍》。 help 信息先从redis-trib.rb的help信息，看下redis-trib.rb提供了哪些功能。12345678910111213141516171819202122232425262728293031323334353637$ruby redis-trib.rb helpUsage: redis-trib &lt;command&gt; &lt;options&gt; &lt;arguments ...&gt; create host1:port1 ... hostN:portN --replicas &lt;arg&gt; check host:port info host:port fix host:port --timeout &lt;arg&gt; reshard host:port --from &lt;arg&gt; --to &lt;arg&gt; --slots &lt;arg&gt; --yes --timeout &lt;arg&gt; --pipeline &lt;arg&gt; rebalance host:port --weight &lt;arg&gt; --auto-weights --threshold &lt;arg&gt; --use-empty-masters --timeout &lt;arg&gt; --simulate --pipeline &lt;arg&gt; add-node new_host:new_port existing_host:existing_port --slave --master-id &lt;arg&gt; del-node host:port node_id set-timeout host:port milliseconds call host:port command arg arg .. arg import host:port --from &lt;arg&gt; --copy --replace help (show this help)For check, fix, reshard, del-node, set-timeout you can specify the host and port of any working node in the cluster. 可以看到redis-trib.rb具有以下功能：1、create：创建集群2、check：检查集群3、info：查看集群信息4、fix：修复集群5、reshard：在线迁移slot6、rebalance：平衡集群节点slot数量7、add-node：将新节点加入集群8、del-node：从集群中删除节点9、set-timeout：设置集群节点间心跳连接的超时时间10、call：在集群全部节点上执行命令11、import：将外部redis数据导入集群下面从redis-trib.rb使用和源码的角度详细介绍redis-trib.rb的每个功能。 redis-trib.rb主要有两个类：ClusterNode和RedisTrib。ClusterNode保存了每个节点的信息，RedisTrib则是redis-trib.rb各个功能的实现。 ClusterNode对象先分析ClusterNode源码。ClusterNode有下面几个成员变量（ruby的类成员变量是以@开头的）：@r：执行redis命令的客户端对象。@info：保存了该节点的详细信息，包括cluster nodes命令中自己这行的信息和cluster info的信息。@dirty：节点信息是否需要更新，如果为true，我们需要把内存的节点更新信息到节点上。@friends：保存了集群其他节点的info信息。其信息为通过cluster nodes命令获得的其他节点信息。ClusterNode有下面一些成员方法：initialize：ClusterNode的构造方法，需要传入节点的地址信息。friends：返回@friends对象。slots：返回该节点负责的slots信息。has_flag?：判断节点info信息的的flags中是否有给定的flag。to_s：类似java的toString方法，返回节点的地址信息。connect：连接redis节点。assert_cluster：判断节点开启了集群配置。assert_empty：确定节点目前没有跟任何其他节点握手，同时自己的db数据为空。load_info：通过cluster info和cluster nodes导入节点信息。add_slots：给节点增加slot，该操作只是在内存中修改，并把dirty设置成true，等待flush_node_config将内存中的数据同步在节点执行。set_as_replica：slave设置复制的master地址。dirty设置成true。flush_node_config：将内存的数据修改同步在集群节点中执行。info_string：简单的info信息。get_config_signature：用来验证集群节点见的cluster nodes信息是否一致。该方法返回节点的签名信息。info：返回@info对象，包含详细的info信息。is_dirty?：判断@dirty。r：返回执行redis命令的客户端对象。 有了ClusterNode对象，在处理集群操作的时候，就获得了集群的信息，可以进行集群相关操作。在此先简单介绍下redis-trib.rb脚本的使用，以create为例：12create host1:port1 ... hostN:portN --replicas &lt;arg&gt; host1:port1 … hostN:portN表示子参数，这个必须在可选参数之后，–replicas 是可选参数，带的表示后面必须填写一个参数，像–slave这样，后面就不带参数，掌握了这个基本规则，就能从help命令中获得redis-trib.rb的使用方法。 其他命令大都需要传递host:port，这是redis-trib.rb为了连接集群，需要选择集群中的一个节点，然后通过该节点获得整个集群的信息。 下面就一一详细介绍redis-trib.rb的每个功能。 create创建集群create命令可选replicas参数，replicas表示需要有几个slave。最简单命令使用如下：1$ruby redis-trib.rb create 10.180.157.199:6379 10.180.157.200:6379 10.180.157.201:6379 有一个slave的创建命令如下：1$ruby redis-trib.rb create --replicas 1 10.180.157.199:6379 10.180.157.200:6379 10.180.157.201:6379 10.180.157.202:6379 10.180.157.205:6379 10.180.157.208:6379 创建流程如下：1、首先为每个节点创建ClusterNode对象，包括连接每个节点。检查每个节点是否为独立且db为空的节点。执行load_info方法导入节点信息。2、检查传入的master节点数量是否大于等于3个。只有大于3个节点才能组成集群。3、计算每个master需要分配的slot数量，以及给master分配slave。分配的算法大致如下：先把节点按照host分类，这样保证master节点能分配到更多的主机中。不停遍历遍历host列表，从每个host列表中弹出一个节点，放入interleaved数组。直到所有的节点都弹出为止。master节点列表就是interleaved前面的master数量的节点列表。保存在masters数组。计算每个master节点负责的slot数量，保存在slots_per_node对象，用slot总数除以master数量取整即可。遍历masters数组，每个master分配slots_per_node个slot，最后一个master，分配到16384个slot为止。接下来为master分配slave，分配算法会尽量保证master和slave节点不在同一台主机上。对于分配完指定slave数量的节点，还有多余的节点，也会为这些节点寻找master。分配算法会遍历两次masters数组。第一次遍历masters数组，在余下的节点列表找到replicas数量个slave。每个slave为第一个和master节点host不一样的节点，如果没有不一样的节点，则直接取出余下列表的第一个节点。第二次遍历是在对于节点数除以replicas不为整数，则会多余一部分节点。遍历的方式跟第一次一样，只是第一次会一次性给master分配replicas数量个slave，而第二次遍历只分配一个，直到余下的节点被全部分配出去。4、打印出分配信息，并提示用户输入“yes”确认是否按照打印出来的分配方式创建集群。5、输入“yes”后，会执行flush_nodes_config操作，该操作执行前面的分配结果，给master分配slot，让slave复制master，对于还没有握手（cluster meet）的节点，slave复制操作无法完成，不过没关系，flush_nodes_config操作出现异常会很快返回，后续握手后会再次执行flush_nodes_config。6、给每个节点分配epoch，遍历节点，每个节点分配的epoch比之前节点大1。7、节点间开始相互握手，握手的方式为节点列表的其他节点跟第一个节点握手。8、然后每隔1秒检查一次各个节点是否已经消息同步完成，使用ClusterNode的get_config_signature方法，检查的算法为获取每个节点cluster nodes信息，排序每个节点，组装成node_id1:slots|node_id2:slot2|…的字符串。如果每个节点获得字符串都相同，即认为握手成功。9、此后会再执行一次flush_nodes_config，这次主要是为了完成slave复制操作。10、最后再执行check_cluster，全面检查一次集群状态。包括和前面握手时检查一样的方式再检查一遍。确认没有迁移的节点。确认所有的slot都被分配出去了。11、至此完成了整个创建流程，返回[OK] All 16384 slots covered.。 check检查集群检查集群状态的命令，没有其他参数，只需要选择一个集群中的一个节点即可。执行命令以及结果如下：123456789101112131415161718192021222324$ruby redis-trib.rb check 10.180.157.199:6379&gt;&gt;&gt; Performing Cluster Check (using node 10.180.157.199:6379)M: b2506515b38e6bbd3034d540599f4cd2a5279ad1 10.180.157.199:6379 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d376aaf80de0e01dde1f8cd4647d5ac3317a8641 10.180.157.205:6379 slots: (0 slots) slave replicates e36c46dbe90960f30861af00786d4c2064e63df2M: 15126fb33796c2c26ea89e553418946f7443d5a5 10.180.157.201:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 59fa6ee455f58a5076f6d6f83ddd74161fd7fb55 10.180.157.208:6379 slots: (0 slots) slave replicates 15126fb33796c2c26ea89e553418946f7443d5a5S: 460b3a11e296aafb2615043291b7dd98274bb351 10.180.157.202:6379 slots: (0 slots) slave replicates b2506515b38e6bbd3034d540599f4cd2a5279ad1M: e36c46dbe90960f30861af00786d4c2064e63df2 10.180.157.200:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 检查前会先执行load_cluster_info_from_node方法，把所有节点数据load进来。load的方式为通过自己的cluster nodes发现其他节点，然后连接每个节点，并加入nodes数组。接着生成节点间的复制关系。 load完数据后，开始检查数据，检查的方式也是调用创建时候使用的check_cluster。 info查看集群信息info命令用来查看集群的信息。info命令也是先执行load_cluster_info_from_node获取完整的集群信息。然后显示ClusterNode的info_string结果，示例如下：123456$ruby redis-trib.rb info 10.180.157.199:637910.180.157.199:6379 (b2506515...) -&gt; 0 keys | 5461 slots | 1 slaves.10.180.157.201:6379 (15126fb3...) -&gt; 0 keys | 5461 slots | 1 slaves.10.180.157.200:6379 (e36c46db...) -&gt; 0 keys | 5462 slots | 1 slaves.[OK] 0 keys in 3 masters.0.00 keys per slot on average. fix修复集群fix命令的流程跟check的流程很像，显示加载集群信息，然后在check_cluster方法内传入fix为true的变量，会在集群检查出现异常的时候执行修复流程。目前fix命令能修复两种异常，一种是集群有处于迁移中的slot的节点，一种是slot未完全分配的异常。 fix_open_slot方法是修复集群有处于迁移中的slot的节点异常。1、先检查该slot是谁负责的，迁移的源节点如果没完成迁移，owner还是该节点。没有owner的slot无法完成修复功能。2、遍历每个节点，获取哪些节点标记该slot为migrating状态，哪些节点标记该slot为importing状态。对于owner不是该节点，但是通过cluster countkeysinslot获取到该节点有数据的情况，也认为该节点为importing状态。3、如果migrating和importing状态的节点均只有1个，这可能是迁移过程中redis-trib.rb被中断所致，直接执行move_slot继续完成迁移任务即可。传递dots和fix为true。4、如果migrating为空，importing状态的节点大于0，那么这种情况执行回滚流程，将importing状态的节点数据通过move_slot方法导给slot的owner节点，传递dots、fix和cold为true。接着对importing的节点执行cluster stable命令恢复稳定。5、如果importing状态的节点为空，有一个migrating状态的节点，而且该节点在当前slot没有数据，那么可以直接把这个slot设为stable。6、如果migrating和importing状态不是上述情况，目前redis-trib.rb工具无法修复，上述的三种情况也已经覆盖了通过redis-trib.rb工具迁移出现异常的各个方面，人为的异常情形太多，很难考虑完全。 fix_slots_coverage方法能修复slot未完全分配的异常。未分配的slot有三种状态。1、所有节点的该slot都没有数据。该状态redis-trib.rb工具直接采用随机分配的方式，并没有考虑节点的均衡。本人尝试对没有分配slot的集群通过fix修复集群，结果slot还是能比较平均的分配，但是没有了连续性，打印的slot信息非常离散。2、有一个节点的该slot有数据。该状态下，直接把slot分配给该slot有数据的节点。3、有多个节点的该slot有数据。此种情况目前还处于TODO状态，不过redis作者列出了修复的步骤，对这些节点，除第一个节点，执行cluster migrating命令，然后把这些节点的数据迁移到第一个节点上。清除migrating状态，然后把slot分配给第一个节点。 reshard在线迁移slotreshard命令可以在线把集群的一些slot从集群原来slot负责节点迁移到新的节点，利用reshard可以完成集群的在线横向扩容和缩容。reshard的参数很多，下面来一一解释一番：1234567reshard host:port --from &lt;arg&gt; --to &lt;arg&gt; --slots &lt;arg&gt; --yes --timeout &lt;arg&gt; --pipeline &lt;arg&gt; host:port：这个是必传参数，用来从一个节点获取整个集群信息，相当于获取集群信息的入口。–from ：需要从哪些源节点上迁移slot，可从多个源节点完成迁移，以逗号隔开，传递的是节点的node id，还可以直接传递–from all，这样源节点就是集群的所有节点，不传递该参数的话，则会在迁移过程中提示用户输入。–to ：slot需要迁移的目的节点的node id，目的节点只能填写一个，不传递该参数的话，则会在迁移过程中提示用户输入。–slots ：需要迁移的slot数量，不传递该参数的话，则会在迁移过程中提示用户输入。–yes：设置该参数，可以在打印执行reshard计划的时候，提示用户输入yes确认后再执行reshard。–timeout ：设置migrate命令的超时时间。–pipeline ：定义cluster getkeysinslot命令一次取出的key数量，不传的话使用默认值为10。 迁移的流程如下：1、通过load_cluster_info_from_node方法装载集群信息。2、执行check_cluster方法检查集群是否健康。只有健康的集群才能进行迁移。3、获取需要迁移的slot数量，用户没传递–slots参数，则提示用户手动输入。4、获取迁移的目的节点，用户没传递–to参数，则提示用户手动输入。此处会检查目的节点必须为master节点。5、获取迁移的源节点，用户没传递–from参数，则提示用户手动输入。此处会检查源节点必须为master节点。–from all的话，源节点就是除了目的节点外的全部master节点。这里为了保证集群slot分配的平均，建议传递–from all。6、执行compute_reshard_table方法，计算需要迁移的slot数量如何分配到源节点列表，采用的算法是按照节点负责slot数量由多到少排序，计算每个节点需要迁移的slot的方法为：迁移slot数量 * (该源节点负责的slot数量 / 源节点列表负责的slot总数)。这样算出的数量可能不为整数，这里代码用了下面的方式处理：12345n = (numslots/source_tot_slots*s.slots.length)if i == 0 n = n.ceilelse n = n.floor 这样的处理方式会带来最终分配的slot与请求迁移的slot数量不一致，这个BUG已经在github上提给作者，https://github.com/antirez/redis/issues/2990。7、打印出reshard计划，如果用户没传–yes，就提示用户确认计划。8、根据reshard计划，一个个slot的迁移到新节点上，迁移使用move_slot方法，该方法被很多命令使用，具体可以参见下面的迁移流程。move_slot方法传递dots为true和pipeline数量。9、至此，就完成了全部的迁移任务。下面看下一次reshard的执行结果：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465$ruby redis-trib.rb reshard --from all --to 80b661ecca260c89e3d8ea9b98f77edaeef43dcd --slots 11 10.180.157.199:6379&gt;&gt;&gt; Performing Cluster Check (using node 10.180.157.199:6379)S: b2506515b38e6bbd3034d540599f4cd2a5279ad1 10.180.157.199:6379 slots: (0 slots) slave replicates 460b3a11e296aafb2615043291b7dd98274bb351S: d376aaf80de0e01dde1f8cd4647d5ac3317a8641 10.180.157.205:6379 slots: (0 slots) slave replicates e36c46dbe90960f30861af00786d4c2064e63df2M: 15126fb33796c2c26ea89e553418946f7443d5a5 10.180.157.201:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 59fa6ee455f58a5076f6d6f83ddd74161fd7fb55 10.180.157.208:6379 slots: (0 slots) slave replicates 15126fb33796c2c26ea89e553418946f7443d5a5M: 460b3a11e296aafb2615043291b7dd98274bb351 10.180.157.202:6379 slots:0-5460 (5461 slots) master 1 additional replica(s)M: 80b661ecca260c89e3d8ea9b98f77edaeef43dcd 10.180.157.200:6380 slots: (0 slots) master 0 additional replica(s)M: e36c46dbe90960f30861af00786d4c2064e63df2 10.180.157.200:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.Ready to move 11 slots. Source nodes: M: 15126fb33796c2c26ea89e553418946f7443d5a5 10.180.157.201:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s) M: 460b3a11e296aafb2615043291b7dd98274bb351 10.180.157.202:6379 slots:0-5460 (5461 slots) master 1 additional replica(s) M: e36c46dbe90960f30861af00786d4c2064e63df2 10.180.157.200:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s) Destination node: M: 80b661ecca260c89e3d8ea9b98f77edaeef43dcd 10.180.157.200:6380 slots: (0 slots) master 0 additional replica(s) Resharding plan: Moving slot 5461 from e36c46dbe90960f30861af00786d4c2064e63df2 Moving slot 5462 from e36c46dbe90960f30861af00786d4c2064e63df2 Moving slot 5463 from e36c46dbe90960f30861af00786d4c2064e63df2 Moving slot 5464 from e36c46dbe90960f30861af00786d4c2064e63df2 Moving slot 0 from 460b3a11e296aafb2615043291b7dd98274bb351 Moving slot 1 from 460b3a11e296aafb2615043291b7dd98274bb351 Moving slot 2 from 460b3a11e296aafb2615043291b7dd98274bb351 Moving slot 10923 from 15126fb33796c2c26ea89e553418946f7443d5a5 Moving slot 10924 from 15126fb33796c2c26ea89e553418946f7443d5a5 Moving slot 10925 from 15126fb33796c2c26ea89e553418946f7443d5a5Do you want to proceed with the proposed reshard plan (yes/no)? yesMoving slot 5461 from 10.180.157.200:6379 to 10.180.157.200:6380:Moving slot 5462 from 10.180.157.200:6379 to 10.180.157.200:6380:Moving slot 5463 from 10.180.157.200:6379 to 10.180.157.200:6380:Moving slot 5464 from 10.180.157.200:6379 to 10.180.157.200:6380:Moving slot 0 from 10.180.157.202:6379 to 10.180.157.200:6380:Moving slot 1 from 10.180.157.202:6379 to 10.180.157.200:6380:Moving slot 2 from 10.180.157.202:6379 to 10.180.157.200:6380:Moving slot 10923 from 10.180.157.201:6379 to 10.180.157.200:6380:Moving slot 10924 from 10.180.157.201:6379 to 10.180.157.200:6380:Moving slot 10925 from 10.180.157.201:6379 to 10.180.157.200:6380: move_slot方法可以在线将一个slot的全部数据从源节点迁移到目的节点，fix、reshard、rebalance都需要调用该方法迁移slot。move_slot接受下面几个参数，1、pipeline：设置一次从slot上获取多少个key。2、quiet：迁移会打印相关信息，设置quiet参数，可以不用打印这些信息。3、cold：设置cold，会忽略执行importing和migrating。4、dots：设置dots，则会在迁移过程打印迁移key数量的进度。5、update：设置update，则会更新内存信息，方便以后的操作。 move_slot流程如下：1、如果没有设置cold，则对源节点执行cluster importing命令，对目的节点执行migrating命令。fix的时候有可能importing和migrating已经执行过来，所以此种场景会设置cold。2、通过cluster getkeysinslot命令，一次性获取远节点迁移slot的pipeline个key的数量.3、对这些key执行migrate命令，将数据从源节点迁移到目的节点。4、如果migrate出现异常，在fix模式下，BUSYKEY的异常，会使用migrate的replace模式再执行一次，BUSYKEY表示目的节点已经有该key了，replace模式可以强制替换目的节点的key。不是fix模式就直接返回错误了。5、循环执行cluster getkeysinslot命令，直到返回的key数量为0，就退出循环。6、如果没有设置cold，对每个节点执行cluster setslot命令，把slot赋给目的节点。7、如果设置update，则修改源节点和目的节点的slot信息。8、至此完成了迁移slot的流程。 rebalance平衡集群节点slot数量rebalance命令可以根据用户传入的参数平衡集群节点的slot数量，rebalance功能非常强大，可以传入的参数很多，以下是rebalance的参数列表和命令示例。12345678rebalance host:port --weight &lt;arg&gt; --auto-weights --threshold &lt;arg&gt; --use-empty-masters --timeout &lt;arg&gt; --simulate --pipeline &lt;arg&gt; 1$ruby redis-trib.rb rebalance --threshold 1 --weight b31e3a2e=5 --weight 60b8e3a1=5 --use-empty-masters --simulate 10.180.157.199:6379 下面也先一一解释下每个参数的用法：host:port：这个是必传参数，用来从一个节点获取整个集群信息，相当于获取集群信息的入口。–weight ：节点的权重，格式为node_id=weight，如果需要为多个节点分配权重的话，需要添加多个–weight 参数，即–weight b31e3a2e=5 –weight 60b8e3a1=5，node_id可为节点名称的前缀，只要保证前缀位数能唯一区分该节点即可。没有传递–weight的节点的权重默认为1。–auto-weights：这个参数在rebalance流程中并未用到。–threshold ：只有节点需要迁移的slot阈值超过threshold，才会执行rebalance操作。具体计算方法可以参考下面的rebalance命令流程的第四步。–use-empty-masters：rebalance是否考虑没有节点的master，默认没有分配slot节点的master是不参与rebalance的，设置–use-empty-masters可以让没有分配slot的节点参与rebalance。–timeout ：设置migrate命令的超时时间。–simulate：设置该参数，可以模拟rebalance操作，提示用户会迁移哪些slots，而不会真正执行迁移操作。–pipeline ：与reshar的pipeline参数一样，定义cluster getkeysinslot命令一次取出的key数量，不传的话使用默认值为10。 rebalance命令流程如下：1、load_cluster_info_from_node方法先加载集群信息。2、计算每个master的权重，根据参数–weight ，为每个设置的节点分配权重，没有设置的节点，则权重默认为1。3、根据每个master的权重，以及总的权重，计算自己期望被分配多少个slot。计算的方式为：总slot数量 （自己的权重 / 总权重）。4、计算每个master期望分配的slot是否超过设置的阈值，即–threshold 设置的阈值或者默认的阈值。计算的方式为：先计算期望移动节点的阈值，算法为：(100-(100.0expected/n.slots.length)).abs，如果计算出的阈值没有超出设置阈值，则不需要为该节点移动slot。只要有一个master的移动节点超过阈值，就会触发rebalance操作。5、如果触发了rebalance操作。那么就开始执行rebalance操作，先将每个节点当前分配的slots数量减去期望分配的slot数量获得balance值。将每个节点的balance从小到大进行排序获得sn数组。6、用dst_idx和src_idx游标分别从sn数组的头部和尾部开始遍历。目的是为了把尾部节点的slot分配给头部节点。 sn数组保存的balance列表排序后，负数在前面，正数在后面。负数表示需要有slot迁入，所以使用dst_idx游标，正数表示需要有slot迁出，所以使用src_idx游标。理论上sn数组各节点的balance值加起来应该为0，不过由于在计算期望分配的slot的时候只是使用直接取整的方式，所以可能出现balance值之和不为0的情况，balance值之和不为0即为节点不平衡的slot数量，由于slot总数有16384个，不平衡数量相对于总数，基数很小，所以对rebalance流程影响不大。 7、获取sn[dst_idx]和sn[src_idx]的balance值较小的那个值，该值即为需要从sn[src_idx]节点迁移到sn[dst_idx]节点的slot数量。8、接着通过compute_reshard_table方法计算源节点的slot如何分配到源节点列表。这个方法在reshard流程中也有调用，具体步骤可以参考reshard流程的第六步。9、如果是simulate模式，则只是打印出迁移列表。10、如果没有设置simulate，则执行move_slot操作，迁移slot，传入的参数为:quiet=&gt;true,:dots=&gt;false,:update=&gt;true。11、迁移完成后更新sn[dst_idx]和sn[src_idx]的balance值。如果balance值为0后，游标向前进1。12、直到dst_idx到达src_idx游标，完成整个rebalance操作。 add-node将新节点加入集群add-node命令可以将新节点加入集群，节点可以为master，也可以为某个master节点的slave。123add-node new_host:new_port existing_host:existing_port --slave --master-id &lt;arg&gt; add-node有两个可选参数：–slave：设置该参数，则新节点以slave的角色加入集群–master-id：这个参数需要设置了–slave才能生效，–master-id用来指定新节点的master节点。如果不设置该参数，则会随机为节点选择master节点。可以看下add-node命令的执行示例：1234567891011121314151617181920$ruby redis-trib.rb add-node --slave --master-id dcb792b3e85726f012e83061bf237072dfc45f99 10.180.157.202:6379 10.180.157.199:6379&gt;&gt;&gt; Adding node 10.180.157.202:6379 to cluster 10.180.157.199:6379&gt;&gt;&gt; Performing Cluster Check (using node 10.180.157.199:6379)M: dcb792b3e85726f012e83061bf237072dfc45f99 10.180.157.199:6379 slots:0-5460 (5461 slots) master 0 additional replica(s)M: 464d740bf48953ebcf826f4113c86f9db3a9baf3 10.180.157.201:6379 slots:10923-16383 (5461 slots) master 0 additional replica(s)M: befa7e17b4e5f239e519bc74bfef3264a40f96ae 10.180.157.200:6379 slots:5461-10922 (5462 slots) master 0 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 10.180.157.202:6379 to make it join the cluster.Waiting for the cluster to join.&gt;&gt;&gt; Configure node as replica of 10.180.157.199:6379.[OK] New node added correctly. add-node流程如下：1、通过load_cluster_info_from_node方法转载集群信息，check_cluster方法检查集群是否健康。2、如果设置了–slave，则需要为该节点寻找master节点。设置了–master-id，则以该节点作为新节点的master，如果没有设置–master-id，则调用get_master_with_least_replicas方法，寻找slave数量最少的master节点。如果slave数量一致，则选取load_cluster_info_from_node顺序发现的第一个节点。load_cluster_info_from_node顺序的第一个节点是add-node设置的existing_host:existing_port节点，后面的顺序根据在该节点执行cluster nodes返回的结果返回的节点顺序。3、连接新的节点并与集群第一个节点握手。4、如果没设置–slave就直接返回ok，设置了–slave，则需要等待确认新节点加入集群，然后执行cluster replicate命令复制master节点。5、至此，完成了全部的增加节点的流程。 del-node从集群中删除节点del-node可以把某个节点从集群中删除。del-node只能删除没有分配slot的节点。删除命令传递两个参数：host:port：从该节点获取集群信息。node_id：需要删除的节点id。del-node执行结果示例如下：1234$ruby redis-trib.rb del-node 10.180.157.199:6379 d5f6d1d17426bd564a6e309f32d0f5b96962fe53&gt;&gt;&gt; Removing node d5f6d1d17426bd564a6e309f32d0f5b96962fe53 from cluster 10.180.157.199:6379&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; SHUTDOWN the node. del-node流程如下：1、通过load_cluster_info_from_node方法转载集群信息。2、根据传入的node id获取节点，如果节点没找到，则直接提示错误并退出。3、如果节点分配的slot不为空，则直接提示错误并退出。4、遍历集群内的其他节点，执行cluster forget命令，从每个节点中去除该节点。如果删除的节点是master，而且它有slave的话，这些slave会去复制其他master，调用的方法是get_master_with_least_replicas，与add-node没设置–master-id寻找master的方法一样。5、然后关闭该节点 set-timeout设置集群节点间心跳连接的超时时间set-timeout用来设置集群节点间心跳连接的超时时间，单位是毫秒，不得小于100毫秒，因为100毫秒对于心跳时间来说太短了。该命令修改是节点配置参数cluster-node-timeout，默认是15000毫秒。通过该命令，可以给每个节点设置超时时间，设置的方式使用config set命令动态设置，然后执行config rewrite命令将配置持久化保存到硬盘。以下是示例：12345678ruby redis-trib.rb set-timeout 10.180.157.199:6379 30000&gt;&gt;&gt; Reconfiguring node timeout in every cluster node...*** New timeout set for 10.180.157.199:6379*** New timeout set for 10.180.157.205:6379*** New timeout set for 10.180.157.201:6379*** New timeout set for 10.180.157.200:6379*** New timeout set for 10.180.157.208:6379&gt;&gt;&gt; New node timeout set. 5 OK, 0 ERR. call在集群全部节点上执行命令call命令可以用来在集群的全部节点执行相同的命令。call命令也是需要通过集群的一个节点地址，连上整个集群，然后在集群的每个节点执行该命令。1234567$ruby redis-trib.rb call 10.180.157.199:6379 get key&gt;&gt;&gt; Calling GET key10.180.157.199:6379: MOVED 12539 10.180.157.201:637910.180.157.205:6379: MOVED 12539 10.180.157.201:637910.180.157.201:6379:10.180.157.200:6379: MOVED 12539 10.180.157.201:637910.180.157.208:6379: MOVED 12539 10.180.157.201:6379 import将外部redis数据导入集群import命令可以把外部的redis节点数据导入集群。导入的流程如下：1、通过load_cluster_info_from_node方法转载集群信息，check_cluster方法检查集群是否健康。2、连接外部redis节点，如果外部节点开启了cluster_enabled，则提示错误。3、通过scan命令遍历外部节点，一次获取1000条数据。4、遍历这些key，计算出key对应的slot。5、执行migrate命令,源节点是外部节点,目的节点是集群slot对应的节点，如果设置了–copy参数，则传递copy参数，如果设置了–replace，则传递replace参数。6、不停执行scan命令，直到遍历完全部的key。7、至此完成整个迁移流程这中间如果出现异常，程序就会停止。没使用–copy模式，则可以重新执行import命令，使用–copy的话，最好清空新的集群再导入一次。 import命令更适合离线的把外部Redis数据导入，在线导入的话最好使用更专业的导入工具，以slave的方式连接redis节点去同步节点数据应该是更好的方式。 下面是一个例子12./redis-trib.rb import --from 10.0.10.1:6379 10.10.10.1:7000上面的命令是把 10.0.10.1:6379（redis 2.8）上的数据导入到 10.10.10.1:7000这个节点所在的集群 原文地址http://blog.csdn.net/huwei2003/article/details/50973967]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>redis cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[route 命令]]></title>
    <url>%2F2016%2F03%2F24%2Flinux-route%2F</url>
    <content type="text"><![CDATA[mark一下，当笔记，以后忘了查看 概述使用下面的 route 命令可以查看 Linux 内核路由表。12345$ route Destination Gateway Genmask Flags Metric Ref Use Iface 192.168.0.0 * 255.255.255.0 U 0 0 0 eth0 169.254.0.0 * 255.255.0.0 U 0 0 0 eth0 default 192.168.0.1 0.0.0.0 UG 0 0 0 eth0 route 命令的输出项说明输出项 说明Destination 目标网段或者主机Gateway 网关地址，”*” 表示目标是本主机所属的网络，不需要路由Genmask 网络掩码Flags 标记。一些可能的标记如下： U — 路由是活动的 H — 目标是一个主机 G — 路由指向网关 R — 恢复动态路由产生的表项 D — 由路由的后台程序动态地安装 M — 由路由的后台程序修改 ! — 拒绝路由Metric 路由距离，到达指定网络所需的中转数（linux 内核中没有使用）Ref 路由项引用次数（linux 内核中没有使用）Use 此路由项被路由软件查找的次数Iface 该路由表项对应的输出接口 3 种路由类型主机路由主机路由是路由选择表中指向单个IP地址或主机名的路由记录。主机路由的Flags字段为H。例如，在下面的示例中，本地主机通过IP地址192.168.1.1的路由器到达IP地址为10.0.0.10的主机。123Destination Gateway Genmask Flags Metric Ref Use Iface----------- ------- ------- ----- ------ --- --- -----10.0.0.10 192.168.1.1 255.255.255.255 UH 0 0 0 eth0 网络路由网络路由是代表主机可以到达的网络。网络路由的Flags字段为N。例如，在下面的示例中，本地主机将发送到网络192.19.12的数据包转发到IP地址为192.168.1.1的路由器。123Destination Gateway Genmask Flags Metric Ref Use Iface----------- ------- ------- ----- ----- --- --- -----192.19.12 192.168.1.1 255.255.255.0 UN 0 0 0 eth0 默认路由当主机不能在路由表中查找到目标主机的IP地址或网络路由时，数据包就被发送到默认路由（默认网关）上。默认路由的Flags字段为G。例如，在下面的示例中，默认路由是IP地址为192.168.1.1的路由器。123Destination Gateway Genmask Flags Metric Ref Use Iface----------- ------- ------- ----- ------ --- --- -----default 192.168.1.1 0.0.0.0 UG 0 0 0 eth0 配置静态路由route 命令设置和查看路由表都可以用 route 命令，设置内核路由表的命令格式是：1$ route [add|del] [-net|-host] target [netmask Nm] [gw Gw] [[dev] If] 其中： add : 添加一条路由规则 del : 删除一条路由规则 -net : 目的地址是一个网络 -host : 目的地址是一个主机 target : 目的网络或主机 netmask : 目的地址的网络掩码 gw : 路由数据包通过的网关 dev : 为路由指定的网络接口 route 命令使用举例添加到主机的路由12$ route add -host 192.168.1.2 dev eth0 $ route add -host 10.20.30.148 gw 10.20.30.40 #添加到10.20.30.148的网关 添加到网络的路由123$ route add -net 10.20.30.40 netmask 255.255.255.248 eth0 #添加10.20.30.40的网络$ route add -net 10.20.30.48 netmask 255.255.255.248 gw 10.20.30.41 #添加10.20.30.48的网络$ route add -net 192.168.1.0/24 eth1 添加默认路由1$ route add default gw 192.168.1.1 删除路由123456$ route del -host 192.168.1.2 dev eth0:0$ route del -host 10.20.30.148 gw 10.20.30.40$ route del -net 10.20.30.40 netmask 255.255.255.248 eth0$ route del -net 10.20.30.48 netmask 255.255.255.248 gw 10.20.30.41$ route del -net 192.168.1.0/24 eth1$ route del default gw 192.168.1.1 设置包转发在 CentOS 中默认的内核配置已经包含了路由功能，但默认并没有在系统启动时启用此功能。开启 Linux 的路由功能可以通过调整内核的网络参数来实现。要配置和调整内核参数可以使用 sysctl 命令。例如：要开启 Linux 内核的数据包转发功能可以使用如下的命令。1$ sysctl -w net.ipv4.ip_forward=1 这样设置之后，当前系统就能实现包转发，但下次启动计算机时将失效。为了使在下次启动计算机时仍然有效，需要将下面的行写入配置文件/etc/sysctl.conf。12$ vi /etc/sysctl.confnet.ipv4.ip_forward = 1 用户还可以使用如下的命令查看当前系统是否支持包转发。1$ sysctl net.ipv4.ip_forward]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>route</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Java中怎样把数组转换为ArrayList?]]></title>
    <url>%2F2016%2F02%2F09%2Fjava-arraylist-array%2F</url>
    <content type="text"><![CDATA[先来一个数组1Element[] array = &#123;new Element(1),new Element(2),new Element(3)&#125;; 方法一1ArrayList&lt;Element&gt; arrayList = new ArrayList&lt;Element&gt;(Arrays.asList(array)); 首先，我们来看下ArrayList的构造方法的文档。ArrayList(Collection &lt; ? extends E &gt; c) : 构造一个包含特定容器的元素的列表，并且根据容器迭代器的顺序返回。所以构造方法所做的事情如下： 将容器c转换为一个数组 将数组拷贝到ArrayList中称为”elementData”的数组中 ArrayList的构造方法的源码如下：1234567public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); size = elementData.length; if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class);&#125; 方法二1List&lt;Element&gt; list = Arrays.asList(array); 这不是最好的，因为asList()返回的列表的大小是固定的。事实上，返回的列表不是java.util.ArrayList，而是定义在java.util.Arrays中一个私有静态类。我们知道ArrayList的实现本质上是一个数组，而asList()返回的列表是由原始数组支持的固定大小的列表。这种情况下，如果添加或删除列表中的元素，程序会抛出异常UnsupportedOperationException。 123list.add(new Element(4));Exception in thread "main" java.lang.ClassCastException: java.util.Arrays$ArrayList cannot be cast to java.util.ArrayList at collection.ConvertArray.main(ConvertArray.java:22) 方法三12List&lt;element&gt; list = new ArrayList&lt;element&gt;(array.length);Collections.addAll(list, array);]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>arraylist</tag>
        <tag>array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Maven跑Java main的3种方法]]></title>
    <url>%2F2016%2F02%2F06%2Fmaven-run-main%2F</url>
    <content type="text"><![CDATA[概述Maven exec plugin可以使我们运行自己工程的Java类的main方法，并在classpath里自动包含工程的dependencies。本文用示例代码展示了使用maven exec plugin来运行java main方法的3种方法。 在命令行（Command line）运行用这种方式运行的话，并没有在某个maven phase中，所以你首先需要compile（编译）一下代码。请记住exec:java不会自动编译代码，需要先编译才行。1mvn compile 不带参数跑：1mvn exec:java -Dexec.mainClass=&quot;com.vineetmanohar.module.Main&quot; 带参数跑：1mvn exec:java -Dexec.mainClass=&quot;com.vineetmanohar.module.Main&quot; -Dexec.args=&quot;arg0 arg1 arg2&quot; 在classpath里用runtime依赖1mvn exec:java -Dexec.mainClass=&quot;com.vineetmanohar.module.Main&quot; -Dexec.classpathScope=runtime 在pom.xml文件的某个phase里运行也可以在maven的某个phase里运行main方法。比如，作为test phase的一部分运行CodeGenerator.main()方法123456789101112131415161718192021222324&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;java&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;mainClass&gt;com.vineetmanohar.module.CodeGenerator&lt;/mainClass&gt; &lt;arguments&gt; &lt;argument&gt;arg0&lt;/argument&gt; &lt;argument&gt;arg1&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 用以上配置运行exec plugin，运行相应的phase就行了。1mvn test 在pom.xml文件的某个profile运行也可以用不同的profile运行main方法。只要用&lt;profile&gt;标签包裹住以上配置就行。1234567891011121314151617181920212223242526272829&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;code-generator&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;java&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;mainClass&gt;com.vineetmanohar.module.CodeGenerator&lt;/mainClass&gt; &lt;arguments&gt; &lt;argument&gt;arg0&lt;/argument&gt; &lt;argument&gt;arg1&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;/profiles&gt; 调用以上profile，运行以下命令就行：1mvn test -Pcode-generator]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM监控与调优]]></title>
    <url>%2F2015%2F09%2F12%2Fjava-jvm-monitor-optimization%2F</url>
    <content type="text"><![CDATA[JVM监控与调优主要的着眼点在于如何配置、如何监控、如何优化3点上。下面就将针对这3点进行学习 参数设置在Java虚拟机的参数中，有3种表示方法,用ps -ef |grep java命令，可以得到当前Java进程的所有启动参数和配置参数： 标准参数（-），所有的JVM实现都必须实现这些参数的功能，而且向后兼容； 非标准参数（-X），默认jvm实现这些参数的功能，但是并不保证所有jvm实现都满足，且不保证向后兼容； 非Stable参数（-XX），此类参数各个jvm实现会有所不同，将来可能会随时取消，需要慎重使用（但是，这些参数往往是非常有用的）; 标准参数其实标准参数是用过Java的人都最熟悉的，就是你在运行java命令时后面加上的参数，如java -version, java -jar 等，输入命令java -help或java -?就能获得当前机器所有java的标准参数列表。-client设置jvm使用client模式，这是一般在pc机器上使用的模式，启动很快，但性能和内存管理效率并不高；多用于桌面应用；-server使用server模式，启动速度虽然慢（比client模式慢10%左右），但是性能和内存管理效率很高，适用于服务器，用于生成环境、开发环境或测试环境的服务端；如果没有指定-server或-client，JVM启动的时候会自动检测当前主机是否为服务器，如果是就以server模式启动，64位的JVM只有server模式，所以无法使用-client参数；默认情况下，不同的启动模式，执行GC的方式有所区别： 启动模式 新生代GC方式 旧生代和持久代GC的方式 client 串行 串行 server 并行 并发 如果没有指定-server或-client模式，则判断方法如下：-classpath / -cpJVM加载和搜索文件的目录路径，多个路径用;分隔。注意，如果使用了-classpath，JVM就不会再搜索环境变量中定义的CLASSPATH路径。JVM搜索路径的顺序为：1.先搜索JVM自带的jar或zip包（Bootstrat，搜索路径可以用System.getProperty(“sun.boot.class.path”)获得）；2.搜索JRE_HOME/lib/ext下的jar包（Extension，搜索路径可以用System.getProperty(“java.ext.dirs”)获得）；3.搜索用户自定义目录，顺序为：当前目录（.），CLASSPATH，-cp；（搜索路径用System.getProperty(“java.class.path”)获得） -DpropertyName=value定义系统的全局属性值，如配置文件地址等，如果value有空格，可以用-Dname=”space string”这样的形式来定义，用System.getProperty(“propertyName”)可以获得这些定义的属性值，在代码中也可以用System.setProperty(“propertyName”,”value”)的形式来定义属性。 -verbose这是查询GC问题最常用的命令之一，具体参数如：-verbose:class 输出jvm载入类的相关信息，当jvm报告说找不到类或者类冲突时可此进行诊断。-verbose:gc 输出每次GC的相关情况，后面会有更详细的介绍。-verbose:jni 输出native方法调用的相关情况，一般用于诊断jni调用错误信息。 非标准参数非标准参数，是在标准参数的基础上进行扩展的参数，输入java -X命令，能够获得当前JVM支持的所有非标准参数列表（你会发现，其实并不多哦）。在不同类型的JVM中，采用的参数有所不同，在讲解非标准参数时，请参考下面的图，对内存区域的大小有个形象的了解-Xmn新生代内存大小的最大值，包括E区和两个S区的总和，使用方法如：-Xmn65535，-Xmn1024k，-Xmn512m，-Xmn1g (-Xms,-Xmx也是种写法)-Xmn只能使用在JDK1.4或之后的版本中，（之前的1.3/1.4版本中，可使用-XX:NewSize设置年轻代大小，用-XX:MaxNewSize设置年轻代最大值）；如果同时设置了-Xmn和-XX:NewSize，-XX:MaxNewSize，则谁设置在后面，谁就生效；如果同时设置了-XX:NewSize -XX:MaxNewSize与-XX:NewRatio则实际生效的值是：min(MaxNewSize,max(NewSize, heap/(NewRatio+1)))在开发、测试环境，可以-XX:NewSize和-XX:MaxNewSize来设置新生代大小，但在线上生产环境，使用-Xmn一个即可（推荐），或者将-XX:NewSize和-XX:MaxNewSize设置为同一个值，这样能够防止在每次GC之后都要调整堆的大小（即：抖动，抖动会严重影响性能） -Xms初始堆的大小，也是堆大小的最小值，默认值是总共的物理内存/64（且小于1G），默认情况下，当堆中可用内存小于40%(这个值可以用-XX: MinHeapFreeRatio调整，如-X:MinHeapFreeRatio=30)时，堆内存会开始增加，一直增加到-Xmx的大小； -Xmx堆的最大值，默认值是总共的物理内存/64（且小于1G），如果-Xms和-Xmx都不设置，则两者大小会相同，默认情况下，当堆中可用内存大于70%（这个值可以用-XX:MaxHeapFreeRatio调整，如-X:MaxHeapFreeRatio=60)时，堆内存会开始减少，一直减小到-Xms的大小；整个堆的大小=年轻代大小+年老代大小，堆的大小不包含持久代大小，如果增大了年轻代，年老代相应就会减小，官方默认的配置为年老代大小/年轻代大小=2/1左右（使用-XX:NewRatio可以设置-XX:NewRatio=5，表示年老代/年轻代=5/1）；建议在开发测试环境可以用-Xms和-Xmx分别设置最小值最大值，但是在线上生产环境，-Xms和-Xmx设置的值必须一样，原因与年轻代一样——防止抖动； -Xss这个参数用于设置每个线程的栈内存，默认1M，一般来说是不需要改的。除非代码不多，可以设置的小点，另外一个相似的参数是-XX:ThreadStackSize，这两个参数在1.6以前，都是谁设置在后面，谁就生效；1.6版本以后，-Xss设置在后面，则以-Xss为准，-XXThreadStackSize设置在后面，则主线程以-Xss为准，其它线程以-XX:ThreadStackSize -Xrs减少JVM对操作系统信号（OS Signals）的使用（JDK1.3.1之后才有效），当此参数被设置之后，jvm将不接收控制台的控制handler，以防止与在后台以服务形式运行的JVM冲突（这个用的比较少）。 -Xprof跟踪正运行的程序，并将跟踪数据在标准输出输出；适合于开发环境调试。 -Xnoclassgc关闭针对class的gc功能；因为其阻止内存回收，所以可能会导致OutOfMemoryError错误，慎用； -Xincgc开启增量gc（默认为关闭）；这有助于减少长时间GC时应用程序出现的停顿；但由于可能和应用程序并发执行，所以会降低CPU对应用的处理能力。 -Xloggc:file与-verbose:gc功能类似，只是将每次GC事件的相关情况记录到一个文件中，文件的位置最好在本地，以避免网络的潜在问题。若与verbose命令同时出现在命令行中，则以-Xloggc为准。 非Stable参数（非静态参数）以-XX表示的非Stable参数，虽然在官方文档中是不确定的，不健壮的，各个公司的实现也各有不同，但往往非常实用，所以这部分参数对于GC非常重要。JVM（Hotspot）中主要的参数可以大致分为3类 性能参数（Performance Options）：用于JVM的性能调优和内存分配控制，如初始化内存大小的设置； 行为参数（Behavioral Options）：用于改变JVM的基础行为，如GC的方式和算法的选择； 调试参数（Debugging Options）：用于监控、打印、输出等jvm参数，用于显示jvm更加详细的信息； 对于非Stable参数，使用方法有4种： -XX:+[option] 启用选项 -XX:-[option] 不启用选项 -XX:[option]=[number] 给选项设置一个数字类型值，可跟单位，例如 32k, 1024m, 2g -XX:[option]=[string] 给选项设置一个字符串值，例如-XX:HeapDumpPath=./dump.core 首先介绍性能参数，性能参数往往用来定义内存分配的大小和比例，相比于行为参数和调试参数，一个比较明显的区别是性能参数后面往往跟的有数值，常用如下： 参数及其默认值 描述 -XX:NewSize=2.125m 新生代对象生成时占用内存的默认值 -XX:MaxNewSize=size 新生成对象能占用内存的最大值 -XX:MaxPermSize=64m 方法区所能占用的最大内存（非堆内存） -XX:PermSize=64m 方法区分配的初始内存 -XX:MaxTenuringThreshold=15 对象在新生代存活区切换的次数（坚持过MinorGC的次数，每坚持过一次，该值就增加1），大于该值会进入老年代 -XX:MaxHeapFreeRatio=70 GC后java堆中空闲量占的最大比例，大于该值，则堆内存会减少 -XX:MinHeapFreeRatio=40 GC后java堆中空闲量占的最小比例，小于该值，则堆内存会增加 -XX:NewRatio=2 新生代内存容量与老生代内存容量的比例 -XX:ReservedCodeCacheSize= 32m 保留代码占用的内存容量 -XX:ThreadStackSize=512 设置线程栈大小，若为0则使用系统默认值 -XX:LargePageSizeInBytes=4m 设置用于Java堆的大页面尺寸 -XX:PretenureSizeThreshold= size 大于该值的对象直接晋升入老年代（这种对象少用为好） -XX:SurvivorRatio=8 Eden区域Survivor区的容量比值，如默认值为8，代表Eden：Survivor1：Survivor2=8:1:1 常用的行为参数，主要用来选择使用什么样的垃圾收集器组合，以及控制运行过程中的GC策略等： 参数及其默认值 描述 -XX:+UseSerialGC 启用串行GC，即采用Serial+Serial Old模式 -XX:+UseParallelGC 启用并行GC，即采用Parallel Scavenge+Serial Old收集器组合（-Server模式下的默认组合） -XX:GCTimeRatio=99 设置用户执行时间占总时间的比例（默认值99，即1%的时间用于GC） -XX:MaxGCPauseMillis=time 设置GC的最大停顿时间（这个参数只对Parallel Scavenge有效） -XX:+UseParNewGC 使用ParNew+Serial Old收集器组合 -XX:ParallelGCThreads 设置执行内存回收的线程数，在+UseParNewGC的情况下使用 -XX:+UseParallelOldGC 使用Parallel Scavenge +Parallel Old组合收集器 -XX:+UseConcMarkSweepGC 使用ParNew+CMS+Serial Old组合并发收集，优先使用ParNew+CMS，当用户线程内存不足时，采用备用方案Serial Old收集。 -XX:+DisableExplicitGC 禁止调用System.gc()；但jvm的gc仍然有效 -XX:+ScavengeBeforeFullGC 新生代GC优先于Full GC执行 常用的调试参数，主要用于监控和打印GC的信息： 参数及其默认值 描述 -XX:+CITime 打印消耗在JIT编译的时间 -XX:ErrorFile=./hs_err_pid[pid].log 保存错误日志或者数据到文件中 -XX:+ExtendedDTraceProbes 开启solaris特有的dtrace探针 -XX:HeapDumpPath=./java_pid[pid].hprof 指定导出堆信息时的路径或文件名 -XX:+HeapDumpOnOutOfMemoryError 当首次遭遇OOM时导出此时堆中相关信息 -XX:OnError=”[cmd args&gt;];[cmd args]” 出现致命ERROR之后运行自定义命令 -XX:OnOutOfMemoryError=”[cmd args];[cmd args]” 当首次遭遇OOM时执行自定义命令 -XX:+PrintClassHistogram 遇到Ctrl-Break后打印类实例的柱状信息，与jmap -histo功能相同 -XX:+PrintConcurrentLocks 遇到Ctrl-Break后打印并发锁的相关信息，与jstack -l功能相同 -XX:+PrintCommandLineFlags 打印在命令行中出现过的标记 -XX:+PrintCompilation 当一个方法被编译时打印相关信息 -XX:+PrintGC 每次GC时打印相关信息 -XX:+PrintGC Details 每次GC时打印详细信息 -XX:+PrintGCTimeStamps 打印每次GC的时间戳 -XX:+TraceClassLoading 跟踪类的加载信息 -XX:+TraceClassLoadingPreorder 跟踪被引用到的所有类的加载信息 -XX:+TraceClassResolution 跟踪常量池 -XX:+TraceClassUnloading 跟踪类的卸载信息 -XX:+TraceLoaderConstraints 跟踪类加载器约束的相关信息 这些参数将为我们进行GC的监控与调优提供很大助力，是我们进行GC相关操作的重要工具。 收集器搭配在介绍了常用的配置参数之后，我们将开始真正的JVM实操征程，首先，我们要为应用程序选择一个合适的垃圾收集器组合，及上节中的行为参数。图中两个收集器之间有连线，说明它们可以配合使用新生代收集器如下：Serial收集器： Serial收集器是在client模式下默认的新生代收集器，其收集效率大约是100M左右的内存需要几十到100多毫秒；在client模式下，收集桌面应用的内存垃圾，基本上不影响用户体验。所以，一般的Java桌面应用中，直接使用Serial收集器（不需要配置参数，用默认即可）。ParNew收集器：Serial收集器的多线程版本，这种收集器默认开通的线程数与CPU数量相同，-XX:ParallelGCThreads可以用来设置开通的线程数。可以与CMS收集器配合使用，事实上用-XX:+UseConcMarkSweepGC选择使用CMS收集器时，默认使用的就是ParNew收集器，所以不需要额外设置-XX:+UseParNewGC，设置了也不会冲突，因为会将ParNew+Serial Old作为一个备选方案；如果单独使用-XX:+UseParNewGC参数，则选择的是ParNew+Serial Old收集器组合收集器。一般情况下，在server模式下，如果选择CMS收集器，则优先选择ParNew收集器。Parallel Scavenge收集器：关注的是吞吐量，意味着强调任务更快的完成，而如CMS等关注停顿时间短的收集器，强调的是用户交互体验。在需要关注吞吐量的场合，比如数据运算服务器等，就可以使用Parallel Scavenge收集器。 老年代收集器如下：Serial Old收集器：在1.5版本及以前可以与 Parallel Scavenge结合使用（事实上，也是当时Parallel Scavenge唯一能用的版本），另外就是在使用CMS收集器时的备用方案，发生 Concurrent Mode Failure时使用。如果是单独使用，Serial Old一般用在client模式中。Parallel Old收集器：在1.6版本之后，与 Parallel Scavenge结合使用，以更好的贯彻吞吐量优先的思想，如果是关注吞吐量的服务器，建议使用Parallel Scavenge + Parallel Old 收集器。CMS收集器：这是当前阶段使用很广的一种收集器，国内很多大的互联网公司线上服务器都使用这种垃圾收集器,CMS收集器以获取最短回收停顿时间为目标，非常适合对用户响应比较高的B/S架构服务器。CMSIncrementalMode： CMS收集器变种，属增量式垃圾收集器，在并发标记和并发清理时交替运行垃圾收集器和用户线程。G1 收集器：面向服务器端应用的垃圾收集器，计划未来替代CMS收集器。 一般来说，如果是Java桌面应用，建议采用Serial+Serial Old收集器组合，即：-XX:+UseSerialGC（-client下的默认参数） 在开发/测试环境，可以采用默认参数，即采用Parallel Scavenge+Serial Old收集器组合，即：-XX:+UseParallelGC（-server下的默认参数） 在线上运算优先的环境，建议采用Parallel Scavenge+Serial Old收集器组合，即：-XX:+UseParallelGC 在线上服务响应优先的环境，建议采用ParNew+CMS+Serial Old收集器组合，即：-XX:+UseConcMarkSweepGC 另外在选择了垃圾收集器组合之后，还要配置一些辅助参数，以保证收集器可以更好的工作 选用了ParNew收集器，你可能需要配置4个参数： -XX:SurvivorRatio, -XX:PretenureSizeThreshold, -XX:+HandlePromotionFailure,-XX:MaxTenuringThreshold； 选用了 Parallel Scavenge收集器，你可能需要配置3个参数： -XX:MaxGCPauseMillis，-XX:GCTimeRatio， -XX:+UseAdaptiveSizePolicy ； 选用了CMS收集器，你可能需要配置3个参数： -XX:CMSInitiatingOccupancyFraction， -XX:+UseCMSCompactAtFullCollection, -XX:CMSFullGCsBeforeCompaction； 启动内存分配关于GC有一个常见的疑问是，在启动时，我的内存如何分配？经过前面的学习，已经很容易知道，用-Xmn，-Xmx，-Xms，-Xss，-XX:NewSize，-XX:MaxNewSize，-XX:MaxPermSize，-XX:PermSize，-XX:SurvivorRatio，-XX:PretenureSizeThreshold，-XX:MaxTenuringThreshold就基本可以配置内存启动时的分配情况。但是，具体配置多少？设置小了，频繁GC（甚至内存溢出），设置大了，内存浪费。结合前面对于内存区域和其作用的学习，尽量考虑如下建议： -XX:PermSize尽量比-XX:MaxPermSize小，-XX:MaxPermSize&gt;= 2 * -XX:PermSize, -XX:PermSize&gt; 64m，一般对于4G内存的机器，-XX:MaxPermSize不会超过256m； -Xms = -Xmx（线上Server模式），以防止抖动，大小受操作系统和内存大小限制，如果是32位系统，则一般-Xms设置为1g-2g（假设有4g内存），在64位系统上，没有限制，不过一般为机器最大内存的一半左右； -Xmn，在开发环境下，可以用-XX:NewSize和-XX:MaxNewSize来设置新生代的大小（-XX:NewSize&lt;=-XX:MaxNewSize），在生产环境，建议只设置-Xmn，一般-Xmn的大小是-Xms的1/2左右，不要设置的过大或过小，过大导致老年代变小，频繁Full GC，过小导致minor GC频繁。如果不设置-Xmn，可以采用-XX:NewRatio=2来设置，也是一样的效果； -Xss一般是不需要改的，默认值即可。 -XX:SurvivorRatio一般设置8-10左右，推荐设置为10，也即：Survivor区的大小是Eden区的1/10，一般来说，普通的Java程序应用，一次minorGC后，至少98%-99%的对象，都会消亡，所以，survivor区设置为Eden区的1/10左右，能使Survivor区容纳下10-20次的minor GC才满，然后再进入老年代，这个与 -XX:MaxTenuringThreshold的默认值15次也相匹配的。如果XX:SurvivorRatio设置的太小，会导致本来能通过minor回收掉的对象提前进入老年代，产生不必要的full gc；如果XX:SurvivorRatio设置的太大，会导致Eden区相应的被压缩。 -XX:MaxTenuringThreshold默认为15，也就是说，经过15次Survivor轮换（即15次minor GC），就进入老年代， 如果设置的小的话，则年轻代对象在survivor中存活的时间减小，提前进入年老代，对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象在年轻代的存活时间，增加在年轻代即被回收的概率。需要注意的是，设置了 -XX:MaxTenuringThreshold，并不代表着，对象一定在年轻代存活15次才被晋升进入老年代，它只是一个最大值，事实上，存在一个动态计算机制，计算每次晋入老年代的阈值，取阈值和MaxTenuringThreshold中较小的一个为准。 -XX:PretenureSizeThreshold一般采用默认值即可。 监控工具和方法在JVM运行的过程中，为保证其稳定、高效，或在出现GC问题时分析问题原因，我们需要对GC进行监控。所谓监控，其实就是分析清楚当前GC的情况。其目的是鉴别JVM是否在高效的进行垃圾回收，以及有没有必要进行调优。通过监控GC，我们可以搞清楚很多问题，如： minor GC和full GC的频率； 执行一次GC所消耗的时间； 新生代的对象何时被移到老生代以及花费了多少时间； 每次GC中，其它线程暂停（Stop the world）的时间； 每次GC的效果如何，是否不理想； 监控GC的工具分为2种：命令行工具和图形工具；常用的命令行工具有：注：下面的命令都在JAVA_HOME/bin中，是java自带的命令。如果您发现无法使用，请直接进入Java安装目录调用或者先设置Java的环境变量，一个简单的办法为：直接运行命令 export PATH=$JAVA_HOME/bin:$PATH；另外，一般的，在Linux下，下面的命令需要sudo权限，在windows下，部分命令的部分选项不能使用。还是无法使用，要保证你装的是否是jdk还是jre，jre是不带很多工具的。 jpsjps命令用于查询正在运行的JVM进程，常用的参数为： -q:只输出LVMID，省略主类的名称 -m:输出虚拟机进程启动时传给主类main()函数的参数 -l:输出主类的全类名，如果进程执行的是Jar包，输出Jar路径 -v:输出虚拟机进程启动时JVM参数命令格式:jps [option] [hostid]一个简单的例子：在上图中，有一个vid为309的apache进程在提供web服务。 jstatjstat可以实时显示本地或远程JVM进程中类装载、内存、垃圾收集、JIT编译等数据（如果要显示远程JVM信息，需要远程主机开启RMI支持）。如果在服务启动时没有指定启动参数-verbose:gc，则可以用jstat实时查看gc情况。jstat有如下选项： -class:监视类装载、卸载数量、总空间及类装载所耗费的时间 -gc:监听Java堆状况，包括Eden区、两个Survivor区、老年代、永久代等的容量，以用空间、GC时间合计等信息 -gccapacity:监视内容与-gc基本相同，但输出主要关注java堆各个区域使用到的最大和最小空间 -gcutil:监视内容与-gc基本相同，但输出主要关注已使用空间占总空间的百分比 -gccause:与-gcutil功能一样，但是会额外输出导致上一次GC产生的原因 -gcnew:监视新生代GC状况 -gcnewcapacity:监视内同与-gcnew基本相同，输出主要关注使用到的最大和最小空间 -gcold:监视老年代GC情况 -gcoldcapacity:监视内同与-gcold基本相同，输出主要关注使用到的最大和最小空间 -gcpermcapacity:输出永久代使用到最大和最小空间 -compiler:输出JIT编译器编译过的方法、耗时等信息 -printcompilation:输出已经被JIT编译的方法命令格式:jstat [option vmid [interval[s|ms] [count]]]jstat可以监控远程机器，命令格式中VMID和LVMID特别说明：如果是本地虚拟机进程，VMID和LVMID是一致的，如果是远程虚拟机进程，那么VMID格式是: [protocol:][//]lvmid[@hostname[:port]/servername]，如果省略interval和count，则只查询一次查看gc情况的例子：在图中，命令sudo jstat -gc 309 1000 5代表着：搜集vid为309的java进程的整体gc状态， 每1000ms收集一次，共收集5次；XXXC表示该区容量，XXXU表示该区使用量，各列解释如下：S0C：S0区容量（S1区相同，略）S0U：S0区已使用EC：E区容量EU：E区已使用OC：老年代容量OU：老年代已使用PC：Perm容量PU：Perm区已使用YGC：Young GC（Minor GC）次数YGCT：Young GC总耗时FGC：Full GC次数FGCT：Full GC总耗时GCT：GC总耗时 用gcutil查看内存的例子：图中的各列与用gc参数时基本一致，不同的是这里显示的是已占用的百分比，如S0为86.53，代表着S0区已使用了86.53% jinfo用于查询当前运行这的JVM属性和参数的值。jinfo可以使用如下选项： -flag:显示未被显示指定的参数的系统默认值 -flag [+|-]name或-flag name=value: 修改部分参数 -sysprops:打印虚拟机进程的System.getProperties() 命令格式:jinfo [option] pid jmap用于显示当前Java堆和永久代的详细信息（如当前使用的收集器，当前的空间使用率等） -dump:生成java堆转储快照 -heap:显示java堆详细信息(只在Linux/Solaris下有效) -F:当虚拟机进程对-dump选项没有响应时，可使用这个选项强制生成dump快照(只在Linux/Solaris下有效) -finalizerinfo:显示在F-Queue中等待Finalizer线程执行finalize方法的对象(只在Linux/Solaris下有效) -histo:显示堆中对象统计信息 -permstat:以ClassLoader为统计口径显示永久代内存状态(只在Linux/Solaris下有效) 命令格式:jmap [option] vmid其中前面3个参数最重要，如：查看对详细信息：sudo jmap -heap 309生成dump文件： sudo jmap -dump:file=./test.prof 309部分用户没有权限时，采用admin用户：sudo -u admin -H jmap -dump:format=b,file=文件名.hprof pid查看当前堆中对象统计信息：sudo jmap -histo 309：该命令显示3列，分别为对象数量，对象大小，对象名称，通过该命令可以查看是否内存中有大对象；有的用户可能没有jmap权限：sudo -u admin -H jmap -histo 309 | less jhat用于分析使用jmap生成的dump文件，是JDK自带的工具，使用方法为：jhat -J -Xmx512m [file]不过jhat没有mat好用，推荐使用mat（Eclipse插件： http://www.eclipse.org/mat ），mat速度更快，而且是图形界面。 jstack用于生成当前JVM的所有线程快照，线程快照是虚拟机每一条线程正在执行的方法,目的是定位线程出现长时间停顿的原因。 -F:当正常输出的请求不被响应时，强制输出线程堆栈 -l:除堆栈外，显示关于锁的附加信息 -m:如果调用到本地方法的话，可以显示C/C++的堆栈命令格式:jstack [option] vmid -verbosegc-verbosegc是一个比较重要的启动参数，记录每次gc的日志，下面的表格对比了jstat和-verbosegc： jstat -verbosegc 监控对象 运行在本机的Java应用可以把日志输出到终端上，或者借助jstatd命令通过网络连接远程的Java应用。 只有那些把-verbogc作为启动参数的JVM。 输出信息 堆状态（已用空间，最大限制，GC执行次数/时间，等等） 执行GC前后新生代和老年代空间大小，GC执行时间。 输出时间 Every designated time每次设定好的时间。 每次GC发生的时候。 用途 观察堆空间变化情况 了解单次GC产生的效果。 与-verbosegc配合使用的一些常用参数为： -XX:+PrintGCDetails，打印GC信息，这是-verbosegc默认开启的选项 -XX:+PrintGCTimeStamps，打印每次GC的时间戳 -XX:+PrintHeapAtGC：每次GC时，打印堆信息 -XX:+PrintGCDateStamps (from JDK 6 update 4) ：打印GC日期，适合于长期运行的服务器 -Xloggc:/home/admin/logs/gc.log：制定打印信息的记录的日志位置每条verbosegc打印出的gc日志，都类似于下面的格式：time [GC [&lt;collector&gt;: &lt;starting occupancy1&gt; -&gt; &lt;ending occupancy1&gt;(total occupancy1), &lt;pause time1&gt; secs] &lt;starting occupancy3&gt; -&gt; &lt;ending occupancy3&gt;(total occupancy3), &lt;pause time3&gt; secs]如：这些选项的意义是：time：执行GC的时间，需要添加-XX:+PrintGCDateStamps参数才有；collector：minor gc使用的收集器的名字。starting occupancy1：GC执行前新生代空间大小。ending occupancy1：GC执行后新生代空间大小。total occupancy1：新生代总大小pause time1：因为执行minor GC，Java应用暂停的时间。starting occupancy3：GC执行前堆区域总大小ending occupancy3：GC执行后堆区域总大小total occupancy3：堆区总大小pause time3：Java应用由于执行堆空间GC（包括full GC）而停止的时间。 可视化工具监控和分析GC也有一些可视化工具，比较常见的有JConsole和VisualVM JConsoleJConsole工具在JDK/bin目录下，启动JConsole后，将自动搜索本机运行的jvm进程，不需要jps命令来查询指定。双击其中一个jvm进程即可开始监控，也可使用“远程进程”来连接远程服务器。进入JConsole主界面，有“概述”、“内存”、“线程”、“类”、“VM摘要”和”Mbean”六个页签：内存页签相当于jstat命令，用于监视收集器管理的虚拟机内存(Java堆和永久代)变化趋势，还可在详细信息栏观察全部GC执行的时间及次数。线程页签最后一个常用页签，VM页签，可清楚的了解显示指定的JVM参数及堆信息。 VisualVMVisualVM是一个集成多个JDK命令行工具的可视化工具。VisualVM基于NetBeans平台开发，它具备了插件扩展功能的特性，通过插件的扩展，可用于显示虚拟机进程及进程的配置和环境信息(jps，jinfo)，监视应用程序的CPU、GC、堆、方法区及线程的信息(jstat、jstack)等。VisualVM在JDK/bin目录下。安装插件： 工具- 插件VisualVM主界面在VisualVM中生成dump文件： 调优方法一切都是为了这一步，调优，在调优之前，我们需要记住下面的原则： 多数的Java应用不需要在服务器上进行GC优化； 多数导致GC问题的Java应用，都不是因为我们参数设置错误，而是代码问题； 在应用上线之前，先考虑将机器的JVM参数设置到最优（最适合）； 减少创建对象的数量； 减少使用全局变量和大对象； GC优化是到最后不得已才采用的手段； 在实际使用中，分析GC情况优化代码比优化GC参数要多得多； GC优化的目的有两个: 将转移到老年代的对象数量降低到最小； 减少full GC的执行时间； 为了达到上面的目的，一般地，你需要做的事情有： 减少使用全局变量和大对象； 调整新生代的大小到最合适； 设置老年代的大小为最合适； 选择合适的GC收集器； 在上面的4条方法中，用了几个“合适”，那究竟什么才算合适，一般的，请参考上面“收集器搭配”和“启动内存分配”两节中的建议。但这些建议不是万能的，需要根据您的机器和应用情况进行发展和变化，实际操作中，可以将两台机器分别设置成不同的GC参数，并且进行对比，选用那些确实提高了性能或减少了GC时间的参数。真正熟练的使用GC调优，是建立在多次进行GC监控和调优的实战经验上的，进行监控和调优的一般步骤为： 监控GC的状态使用各种JVM工具，查看当前日志，分析当前JVM参数设置，并且分析当前堆内存快照和gc日志，根据实际的各区域内存划分和GC执行时间，觉得是否进行优化； 分析结果，判断是否需要优化如果各项参数设置合理，系统没有超时日志出现，GC频率不高，GC耗时不高，那么没有必要进行GC优化；如果GC时间超过1-3秒，或者频繁GC，则必须优化；注：如果满足下面的指标，则一般不需要进行GC： Minor GC执行时间不到50ms； Minor GC执行不频繁，约10秒一次； Full GC执行时间不到1s； Full GC执行频率不算频繁，不低于10分钟1次； 调整GC类型和内存分配如果内存分配过大或过小，或者采用的GC收集器比较慢，则应该优先调整这些参数，并且先找1台或几台机器进行beta，然后比较优化过的机器和没有优化的机器的性能对比，并有针对性的做出最后选择； 不断的分析和调整通过不断的试验和试错，分析并找到最合适的参数 全面应用参数如果找到了最合适的参数，则将这些参数应用到所有服务器，并进行后续跟踪。 调优实例上面的内容都是纸上谈兵，下面我们以一些真实例子来进行说明： 实例1笔者昨日发现部分开发测试机器出现异常：java.lang.OutOfMemoryError: GC overhead limit exceeded，这个异常代表：GC为了释放很小的空间却耗费了太多的时间，其原因一般有两个：1，堆太小，2，有死循环或大对象；笔者首先排除了第2个原因，因为这个应用同时是在线上运行的，如果有问题，早就挂了。所以怀疑是这台机器中堆设置太小；使用ps -ef |grep &quot;java&quot;查看，发现：该应用的堆区设置只有768m，而机器内存有2g，机器上只跑这一个java应用，没有其他需要占用内存的地方。另外，这个应用比较大，需要占用的内存也比较多；笔者通过上面的情况判断，只需要改变堆中各区域的大小设置即可，于是改成下面的情况：跟踪运行情况发现，相关异常没有再出现； 实例2一个服务系统，经常出现卡顿，分析原因，发现Full GC时间太长：123jstat -gcutil:S0 S1 E O P YGC YGCT FGC FGCT GCT12.16 0.00 5.18 63.78 20.32 54 2.047 5 6.946 8.993 分析上面的数据，发现Young GC执行了54次，耗时2.047秒，每次Young GC耗时37ms，在正常范围，而Full GC执行了5次，耗时6.946秒，每次平均1.389s，数据显示出来的问题是：Full GC耗时较长，分析该系统的是指发现，NewRatio=9，也就是说，新生代和老生代大小之比为1:9，这就是问题的原因： 新生代太小，导致对象提前进入老年代，触发老年代发生Full GC； 老年代较大，进行Full GC时耗时较大；优化的方法是调整NewRatio的值，调整到4，发现Full GC没有再发生，只有Young GC在执行。这就是把对象控制在新生代就清理掉，没有进入老年代（这种做法对一些应用是很有用的，但并不是对所有应用都要这么做） 实例3一应用在性能测试过程中，发现内存占用率很高，Full GC频繁，使用sudo -u admin -H jmap -dump:format=b,file=文件名.hprof pid来dump内存，生成dump文件，并使用Eclipse下的mat差距进行分析，发现：从图中可以看出，这个线程存在问题，队列LinkedBlockingQueue所引用的大量对象并未释放，导致整个线程占用内存高达378m，此时通知开发人员进行代码优化，将相关对象释放掉即可。 参考 http://www.cnblogs.com/zhguang/p/Java-JVM-GC.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java的内存分配和回收]]></title>
    <url>%2F2015%2F08%2F12%2Fjava-memory-gc%2F</url>
    <content type="text"><![CDATA[java的内存分配和回收，主要都在堆中分配和回收，所以堆内存是本文讲解的重点。 Java内存分配机制Java内存分配和回收的机制概括的说，就是：分代分配，分代回收。对象将根据存活的时间被分为：年轻代（Young Generation）、年老代（Old Generation）、永久代（Permanent Generation，也就是方法区）。如下图 年轻代（Young Generation）对象被创建时，内存的分配首先发生在年轻代（大对象可以直接 被创建在年老代），大部分的对象在创建后很快就不再使用，因此很快变得不可达，于是被年轻代的GC机制清理掉（IBM的研究表明，98%的对象都是很快消 亡的），这个GC机制被称为Minor GC或叫Young GC。注意，Minor GC并不代表年轻代内存不足，它事实上只表示在Eden区上的GC。年轻代上的内存分配是这样的，年轻代可以分为3个区域：Eden区和两个存活区（Survivor 0 、Survivor 1）。内存分配过程如下图 绝大多数刚创建的对象会被分配在Eden区，其中的大多数对象很快就会消亡。Eden区是连续的内存空间，因此在其上分配内存极快； 当Eden区满的时候，执行Minor GC，将消亡的对象清理掉，并将剩余的对象复制到一个存活区Survivor0（此时，Survivor1是空白的，两个Survivor总有一个是空白的）； 此后，每次Eden区满了，就执行一次Minor GC，并将剩余的对象都添加到Survivor0； 当Survivor0也满的时候，将其中仍然活着的对象直接复制到Survivor1，以后Eden区执行Minor GC后，就将剩余的对象添加Survivor1（此时，Survivor0是空白的）。 当两个存活区切换了几次（HotSpot虚拟机默认15次，用-XX:MaxTenuringThreshold控制，大于该值进入老年代）之后，仍然存活的对象（其实只有一小部分，比如，我们自己定义的对象），将被复制到老年代。 从上面的过程可以看出，Eden区是连续的空间，且Survivor总有一个为空。经过一次GC和复制，一个Survivor中保存着当前还活 着的对象，而Eden区和另一个Survivor区的内容都不再需要了，可以直接清空，到下一次GC时，两个Survivor的角色再互换。因此，这种方式分配内存和清理内存的效率都极高，这种垃圾回收的方式就是著名的“停止-复制（Stop-and-copy）”清理法（将Eden区和一个Survivor中仍然存活的对象拷贝到另一个Survivor中），这不代表着停止复制清理法很高效，其实，它也只在这种情况下高效，如果在老年代采用停止复制，则挺悲剧的。在Eden区，HotSpot虚拟机使用了两种技术来加快内存分配。分别是bump-the-pointer和TLAB（Thread-Local Allocation Buffers），这两种技术的做法分别是：由于Eden区是连续的，因此bump-the-pointer技术的核心就是跟踪最后创建的一个对象，在对象创建时，只需要检查最后一个对象后面是否有足够的内存即可，从而大大加快内存分配速度；而对于TLAB技术是对于多线程而言的，将Eden区分为若干段，每个线程使用独立的一段，避免相互影响。TLAB结合bump-the-pointer技术，将保证每个线程都使用Eden区的一段，并快速的分配内存。 年老代（Old Generation）对象如果在年轻代存活了足够长的时间而没有被清理掉（即在几次 Young GC后存活了下来），则会被复制到年老代，年老代的空间一般比年轻代大，能存放更多的对象，在年老代上发生的GC次数也比年轻代少。当年老代内存不足时， 将执行Major GC，也叫 Full GC。 可以使用-XX:+UseAdaptiveSizePolicy开关来控制是否采用动态控制策略，如果动态控制，则动态调整Java堆中各个区域的大小以及进入老年代的年龄。 如果对象比较大（比如长字符串或大数组），Young空间不足，则大对象会直接分配到老年代上（大对象可能触发提前GC，应少用，更应避免使用短命的大对象）。用-XX:PretenureSizeThreshold来控制直接升入老年代的对象大小，大于这个值的对象会直接分配在老年代上。 可能存在年老代对象引用新生代对象的情况，如果需要执行Young GC，则可能需要查询整个老年代以确定是否可以清理回收，这显然是低效的。解决的方法是，年老代中维护一个512 byte的块——”card table“，所有老年代对象引用新生代对象的记录都记录在这里。Young GC时，只要查这里即可，不用再去查全部老年代，因此性能大大提高。 Java GC机制GC机制的基本算法是：分代收集，这个不用赘述。下面阐述每个分代的收集方法。 年轻代事实上，在上一节，已经介绍了新生代的主要垃圾回收方法，在新生代中，使用“停止-复制”算法进行清理，将新生代内存分为2部分，1部分 Eden区较大，1部分Survivor比较小，并被划分为两个等量的部分。每次进行清理时，将Eden区和一个Survivor中仍然存活的对象拷贝到 另一个Survivor中，然后清理掉Eden和刚才的Survivor。 这里也可以发现，停止复制算法中，用来复制的两部分并不总是相等的（传统的停止复制算法两部分内存相等，但新生代中使用1个大的Eden区和2个小的Survivor区来避免这个问题） 由于绝大部分的对象都是短命的，甚至存活不到Survivor中，所以，Eden区与Survivor的比例较大，HotSpot默认是 8:1，即分别占新生代的80%，10%，10%。如果一次回收中，Survivor+Eden中存活下来的内存超过了10%，则需要将一部分对象分配到 老年代。用-XX:SurvivorRatio参数来配置Eden区域Survivor区的容量比值，默认是8，代表Eden：Survivor1：Survivor2=8:1:1. 老年代老年代存储的对象比年轻代多得多，而且不乏大对象，对老年代进行内存清理时，如果使用停止-复制算法，则相当低效。一般，老年代用的算法是标记-整理算法，即：标记出仍然存活的对象（存在引用的），将所有存活的对象向一端移动，以保证内存的连续。在发生Minor GC时，虚拟机会检查每次晋升进入老年代的大小是否大于老年代的剩余空间大小，如果大于，则直接触发一次Full GC，否则，就查看是否设 置了-XX:+HandlePromotionFailure（允许担保失败），如果允许，则只会进行MinorGC，此时可以容忍内存分配失败；如果不允许，则仍然进行Full GC（这代表着如果设置-XX:+HandlePromotionFailure，则触发MinorGC就会同时触发Full GC，哪怕老年代还有很多内存，所以，最好不要这样做）。 方法区（永久代）永久代的回收有两种：常量池中的常量(1.7 已移出方法区)，无用的类信息，常量的回收很简单，没有引用了就可以被回收。对于无用的类进行回收，必须保证3点： 类的所有实例都已经被回收 加载类的ClassLoader已经被回收 类对象的Class对象没有被引用（即没有通过反射引用该类的地方） 永久代的回收并不是必须的，可以通过参数来设置是否对类进行回收。HotSpot提供-Xnoclassgc进行控制使用-verbose，-XX:+TraceClassLoading、-XX:+TraceClassUnLoading可以查看类加载和卸载信息-verbose、-XX:+TraceClassLoading可以在Product版HotSpot中使用；-XX:+TraceClassUnLoading需要fastdebug版HotSpot支持 垃圾收集器在GC机制中，起重要作用的是垃圾收集器，垃圾收集器是GC的具体实现，Java虚拟机规范中对于垃圾收集器没有任何规定，所以不同厂商实现的垃圾 收集器各不相同，HotSpot 1.6版使用的垃圾收集器如下图（图来源于《深入理解Java虚拟机：JVM高级特效与最佳实现》，图中两个收集器之间有连线，说明它们可以配合使用）：在介绍垃圾收集器之前，需要明确一点，就是在新生代采用的停止复制算法中，“停 止（Stop-the-world）”的意义是在回收内存时，需要暂停其他所 有线程的执行。这个是很低效的，现在的各种新生代收集器越来越优化这一点，但仍然只是将停止的时间变短，并未彻底取消停止。 Serial收集器：新生代收集器，使用停止复制算法，使用一个线程进行GC，其它工作线程暂停。使用-XX:+UseSerialGC可以使用Serial+Serial Old模式运行进行内存回收（这也是虚拟机在Client模式下运行的默认值） ParNew收集器：新生代收集器，使用停止复制算法，Serial收集器的多线程版，用多个线程进行GC，其它工作线程暂停，关注缩短垃圾收集时间。使用-XX:+UseParNewGC开关来控制使用ParNew+Serial Old收集器组合收集内存；使用-XX:ParallelGCThreads来设置执行内存回收的线程数。 Parallel Scavenge 收集器：新生代收集器，使用停止复制算法，关注CPU吞吐量，即运行用户代码的时间/总时间，比如：JVM运行100分钟，其中运行用户代码99分钟，垃圾收集1分钟，则吞吐量是99%，这种收集器能最高效率的利用CPU，适合运行后台运算（关注缩短垃圾收集时间的收集器，如CMS，等待时间很少，所以适合用户交互，提高用户体验）。使用-XX:+UseParallelGC开关控制使用 Parallel Scavenge+Serial Old收集器组合回收垃圾（这也是在Server模式下的默认值）；使用-XX:GCTimeRatio来设置用户执行时间占总时间的比例，默认99，即 1%的时间用来进行垃圾回收。使用-XX:MaxGCPauseMillis设置GC的最大停顿时间（这个参数只对Parallel Scavenge有效） Serial Old收集器：老年代收集器，单线程收集器，使用标记整理（整理的方法是Sweep（清理）和Compact（压缩），清理是将废弃的对象干掉，只留幸存的对象，压缩是将移动对象，将空间填满保证内存分为2块，一块全是对象，一块空闲）算法，使用单线程进行GC，其它工作线程暂停（注意，在老年代中进行标记整理算法清理，也需要暂停其它线程），在JDK1.5之前，Serial Parallel Old收集器：老年代收集器，多线程，多线程机制与Parallel Scavenge差不错，使用标记整理（与Serial Old不同，这里的整理是Summary（汇总）和Compact（压缩），汇总的意思就是将幸存的对象复制到预先准备好的区域，而不是像Sweep（清 理）那样清理废弃的对象）算法，在Parallel Old执行时，仍然需要暂停其它线程。Parallel Old在多核计算中很有用。Parallel Old出现后（JDK 1.6），与Parallel Scavenge配合有很好的效果，充分体现Parallel Scavenge收集器吞吐量优先的效果。使用-XX:+UseParallelOldGC开关控制使用Parallel Scavenge +Parallel Old组合收集器进行收集。 CMS（Concurrent Mark Sweep）收集器：老年代收集器，致力于获取最短回收停顿时间，使用标记清除算法，多线程，优点是并发收集（用户线程可以和GC线程同时工作），停顿小。使用-XX:+UseConcMarkSweepGC进行ParNew+CMS+Serial Old进行内存回收，优先使用ParNew+CMS（原因见后面），当用户线程内存不足时，采用备用方案Serial Old收集。CMS收集的方法是：先3次标记，再1次清除，3次标记中前两次是初始标记和重新标记（此时仍然需要停止（stop the world））， 初始标记（Initial Remark）是标记GC Roots能关联到的对象（即有引用的对象），停顿时间很短；并发标记（Concurrent remark）是执行GC Roots查找引用的过程，不需要用户线程停顿；重新标记（Remark）是在初始标记和并发标记期间，有标记变动的那部分仍需要标记，所以加上这一部分 标记的过程，停顿时间比并发标记小得多，但比初始标记稍长。在完成标记之后，就开始并发清除，不需要用户线程停顿。所以在CMS清理过程中，只有初始标记和重新标记需要短暂停顿，并发标记和并发清除都不需要暂停用户线程，因此效率很高，很适合高交互的场合。CMS也有缺点，它需要消耗额外的CPU和内存资源，在CPU和内存资源紧张，CPU较少时，会加重系统负担（CMS默认启动线程数为(CPU数量+3)/4）。另外，在并发收集过程中，用户线程仍然在运行，仍然产生内存垃圾，所以可能产生“浮动垃圾”，本次无法清理，只能下一次Full GC才清理，因此在GC期间，需要预留足够的内存给用户线程使用。所以使用CMS的收集器并不是老年代满了才触发Full GC，而是在使用了一大半（默认68%，即2/3，使用-XX:CMSInitiatingOccupancyFraction来设置）的时候就要进行Full GC，如果用户线程消耗内存不是特别大，可以适当调高-XX:CMSInitiatingOccupancyFraction以降低GC次数，提高性能，如果预留的用户线程内存不够，则会触发Concurrent Mode Failure，此时，将触发备用方案：使用Serial Old 收集器进行收集，但这样停顿时间就长了，因此-XX:CMSInitiatingOccupancyFraction不宜设的过大。还有，CMS采用的是标记清除算法，会导致内存碎片的产生，可以使用-XX：+UseCMSCompactAtFullCollection来设置是否在Full GC之后进行碎片整理，用-XX：CMSFullGCsBeforeCompaction来设置在执行多少次不压缩的Full GC之后，来一次带压缩的Full GC。 G1收集器：在JDK1.7中正式发布，与现状的新生代、老年代概念有很大不同，目前使用较少，不做介绍。 参考：http://www.cnblogs.com/zhguang/p/3257367.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python在不同层级目录import模块的方法]]></title>
    <url>%2F2015%2F02%2F18%2Fpython-import-module%2F</url>
    <content type="text"><![CDATA[使用python进行程序编写时，经常会使用第三方模块包。这种包我们可以通过python setup install 进行安装后，通过import XXX或from XXX import yyy 进行导入。不过如果是自己遍写的依赖包，又不想安装到python的相应目录，可以放到本目录里进行import进行调用；为了更清晰的理清程序之间的关系，例如我们会把这种包放到lib目录再调用。本篇就针对常见的模块调用方法汇总下。 同级目录下的调有程序结构如下：123-- src |-- mod1.py |-- test1.py 若在程序test1.py中导入模块mod1, 则直接使用123import mod1或from mod1 import *; 调用子目录下的模块程序结构如下：12345-- src |-- mod1.py |-- lib | |-- mod2.py |-- test1.py 这时看到test1.py和lib目录（即mod2.py的父级目录），如果想在程序test1.py中导入模块mod2.py ，可以在lib件夹中建立空文件init.py文件(也可以在该文件中自定义输出模块接口)，然后使用：123from lib.mod2 import *或import lib.mod2 调用上级目录下的文件程序结构如下：123456-- src |-- mod1.py |-- lib | |-- mod2.py |-- sub | |-- test2.py 这里想要实现test2.py调用mod1.py和mod2.py ，做法是我们先跳到src目录下面，直接可以调用mod1，然后在lib上当下建一个空文件init.py ，就可以像第二步调用子目录下的模块一样，通过import lib.mod2进行调用了。具体代码如下：1234import syssys.path.append("..")import mod1import lib.mod2]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启用Nginx状态监控]]></title>
    <url>%2F2014%2F10%2F12%2Fnginx-status-monitor%2F</url>
    <content type="text"><![CDATA[编译Nginx添加http_stub_status_module编译Nginx的时候添加参数：–with-http_stub_status_module123456cd nginx-&#123;version&#125;/./configure --prefix=/opt/nginx --with-http_stub_status_module --with-http_ssl_modulemake &amp;&amp; make install 启用nginx status配置修改Nginx配置文件nginx.conf，在HTTP段中添加1vi /opt/nginx/conf/nginx.conf 123456789101112server&#123; listen 80; server_name localhost; location /nginx_status &#123; #主要是这里代表根目录显示信息 stub_status on; access_log off; &#125;&#125; 打开status页面浏览器访问监控页面地址http://{your IP}/nginx-status,显示如下1Active connections: 2 server accepts handled requests 8 8 33 Reading: 0 Writing: 1 Waiting: 1 解析：Active connections //当前 Nginx 正处理的活动连接数。server accepts handledrequests //总共处理了8 个连接 , 成功创建 8 次握手,总共处理了33个请求。Reading //nginx 读取到客户端的 Header 信息数。Writing //nginx 返回给客户端的 Header 信息数。Waiting //开启 keep-alive 的情况下，这个值等于 active – (reading + writing)，意思就是 Nginx 已经处理完正在等候下一次请求指令的驻留连接]]></content>
      <categories>
        <category>ngnix</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx location 语法和优先级]]></title>
    <url>%2F2014%2F10%2F07%2Fnginx-location%2F</url>
    <content type="text"><![CDATA[验证下，记录下，以后不用到处找 语法:location [=|~|~*|^~] /uri/ { … } = 精确匹配，例如 location =/test 只匹配 /test ~ 后面接正则，区分大小写,匹配满足此正则的url ~* 后面接正则，不区分大小写,匹配满足此正则的url ^~ 匹配以什么开头的，例如 location ^~ /test 匹配 /test,/test/1,/test/2 ; 什么都不加的话，类似第四点，location /test 匹配 /test,/test/1,/test/2 ;优先级= 大于 ^~ 大于 ~ 或者~* 大于 什么都不加 假如遇到同级的话 ~ 和~ （他们是同级的）比较的话，就是根据他们在文件的出现顺序。例如location ~ /sb.txtlocation ~ /sb.txt如果访问 http://localhost/sb.txt,就会跳到第一个，访问 http://localhost/SB.txt 跳到第二个，换个顺序location ~* /sb.txtlocation ~ /sb.txt不管访问 http://localhost/sb.txt 还是 http://localhost/SB.txt ,都只会跳到第一个 ^~ 的话，就根据他们的匹配度，例如location ^~ /sb/location ^~ /sb/sb2/如果访问http://localhost/sb/sb2/,就会跳到第二个去处理，因为他更加匹配吧。如果访问http://localhost/sb/，就会跳到第一个处理，如果访问http://localhost/sb/sb3,也会跳到第一个。 什么都不加的话，也是根据他们的匹配度，例如location /sb/location /sb/sb2/如果访问http://localhost/sb/sb2/,就会跳到第二个去处理，因为他更加匹配吧。如果访问http://localhost/sb/，就会跳到第一个处理，如果访问http://localhost/sb/sb3,也会跳到第一个。]]></content>
      <categories>
        <category>ngix</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lvs+keepalived 做负载和热备]]></title>
    <url>%2F2014%2F10%2F07%2Flinux-lvs%2F</url>
    <content type="text"><![CDATA[一直觉得nginx做负载不错了，可发现居然这玩意这么好，特别keepalived热备。 准备工作本实验基于VMware+centos,以下是环境信息：假如有4台服务器master：192.168.153.132backup：192.168.153.133vip（virtual ip 虚拟ip）：192.168.153.100 （实际浏览器访问的地址）realserver 1: 192.168.153.128realserver 2: 192.168.153.129热备的意思就是master坏了，backup能补上去，由于都是使用vip，所以外面浏览器访问没有影响realserver1和2是用来做负载均衡，按照权重来调到相应的服务器来处理请求。lvs和keepalived都要装到master和backup上 下载keepalived和lvs的admin程序1$ wget http://www.keepalived.org/software/keepalived-1.2.13.tar.gz 1$ wget http://www.linuxvirtualserver.org/software/kernel-2.6/ipvsadm-1.26.tar.gz 安装依赖1$ yum -y install libnl* openssl* popt* kernel-devel 解压安装12$ tar -zxvf keepalived-1.2.13.tar.gz$ tar -zxvf ipvsadm-1.26.tar.gz 123$ cd keepalived-1.2.13$ ./configure --prefix=/usr --sysconf=/etc$ make &amp;&amp; make install 12$ cd ipvsadm-1.26$ make &amp;&amp; make install 验证lvs1$ ipvsadm ln 验证keepalived1$ service keepalived start 如果显示无误就代表安装成功了。 配置配置master1$ vi /etc/keepalived/keepalived.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455global_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; state MASTER #表示master interface eth1 #网络接口，有可能是eth0，可以用命令ifconfig查看 virtual_router_id 51 priority 100 #优先级 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.153.100 #vip配置 &#125;&#125;virtual_server 192.168.153.100 80 &#123; delay_loop 6 lb_algo rr #负载算法 lb_kind DR #负载方式 DR，这种方式的话VIP端口和realserver端口必须一致 #persistence_timeout 20 protocol TCP real_server 192.168.153.128 80 &#123; weight 3 #权重 TCP_CHECK &#123; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 connect_port 80&#125;&#125; real_server 192.168.153.129 80 &#123; weight 3 #权重 TCP_CHECK &#123; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 connect_port 80&#125;&#125;&#125; 配置backup基本跟master一致，只是在上面红色的第一处改成BACKUP，第二处改成比master小的值，例如90 重启keepalived1$ service keepalived restart 配置realserver在real server 1和2上，把以下脚本放在/etc/rc.d/init.d/下12345678910111213141516171819202122232425262728293031323334#!/bin/bash# description: Config realserver lo and apply noarp SNS_VIP=192.168.153.100 . /etc/rc.d/init.d/functions case "$1" instart) ifconfig lo:0 $SNS_VIP netmask 255.255.255.255 broadcast $SNS_VIP /sbin/route add -host $SNS_VIP dev lo:0 echo "1" &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore echo "2" &gt;/proc/sys/net/ipv4/conf/lo/arp_announce echo "1" &gt;/proc/sys/net/ipv4/conf/all/arp_ignore echo "2" &gt;/proc/sys/net/ipv4/conf/all/arp_announce sysctl -p &gt;/dev/null 2&gt;&amp;1 echo "RealServer Start OK" ;;stop) ifconfig lo:0 down route del $SNS_VIP &gt;/dev/null 2&gt;&amp;1 echo "0" &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore echo "0" &gt;/proc/sys/net/ipv4/conf/lo/arp_announce echo "0" &gt;/proc/sys/net/ipv4/conf/all/arp_ignore echo "0" &gt;/proc/sys/net/ipv4/conf/all/arp_announce echo "RealServer Stoped" ;;*) echo "Usage: $0 &#123;start|stop&#125;" exit 1esac exit 0 命名成realserver,然后可以用以下命令启动：1$ service realserver start 关闭iptable如果不想关掉的话在master 加上以下规则1-A INPUT -i eth1 -p vrrp -s 192.168.153.133 -j ACCEPT 在backup加上以下规则1-A INPUT -i eth1 -p vrrp -s 192.168.153.132 -j ACCEPT 其实就是让热备之间可以通讯，注意eth1是你的网络接口，如果是eth0就要换成eth0realserver就只要开启80端口就可以了。 验证 开启realserver的web应用程序 执行realserver脚本 开启master和backup的keepalived 用watch ipvsadm -ln在master和backup上可以查看状态如果有两条记录分别指向realserver的ip就代表运行正确了。 浏览器访问vip，看能否正确访问web应用程序。 关掉master 的keepalived，如果还能访问的话，就代表backup接管了。可以在backup的日志上面看到，tail -f /var/log/message如果有transition MASTER的，就代表接管了。 再启动master的keepalived，再到backup的日志上面看到transition BACKUP的，就代表已经交回master了。 关掉其中一个realserver，用watch ipvsadm -ln可以看到该realserver的ip被剔除了。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>lvs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos 启用ftp功能]]></title>
    <url>%2F2014%2F08%2F17%2Fcentos-ftp%2F</url>
    <content type="text"><![CDATA[1.安装vsftpd组件1yum -y install vsftpd 安装完后，有个/etc/vsftpd/vsftpd.conf 文件，用来配置还有自动新建了一个ftp用户和ftp的组，home目录为/var/ftp,默认是nologin（不能登录系统）1cat /etc/passwd | grep ftp 默认ftp服务是没有启动的，用下面命令启动1service vsftpd start 2.安装ftp客户端组件（用来验证是否vsftpd）1yum -y install ftp 执行命令尝试登录1ftp localhost 输入用户名ftp，密码随便（因为默认是允许匿名的） 登录成功，就代表ftp服务可用了。 但是，外网是访问不了的，所以还要继续配置。 3.取消匿名登陆1vi /etc/vsftpd/vsftpd.conf 把第一行的 anonymous_enable=YES ，改为NO重启1service vsftpd restart 4.新建一个用户(ftpuser为用户名，随便就可以)1useradd ftpuser 修改密码（输入两次）1passwd ftpuser 这样一个用户建完，可以用这个登录，记得用普通登录不要用匿名了。登录后默认的路径为 /home/ftpuser. 5.开放21端口 因为ftp默认的端口为21，而centos默认是没有开启的，所以要修改iptables文件1vi /etc/sysconfig/iptables 在行上面有22 -j ACCEPT 下面另起一行输入跟那行差不多的，只是把22换成21，然后：wq保存。还要运行下,重启iptables1service iptables restart 外网是可以访问上去了，可是发现没法返回目录，也上传不了，因为selinux作怪了。6.修改selinux1getsebool -a | grep ftp 执行上面命令，再返回的结果看到两行都是off，代表，没有开启外网的访问12345.... allow_ftpd_full_access off ........ftp_home_dir off 只要把上面都变成on就行执行12setsebool -P allow_ftpd_full_access 1 setsebool -P ftp_home_dir off 1 再重启一下vsftpd1service vsftpd restart 这样应该没问题了（如果，还是不行，看看是不是用了ftp客户端工具用了passive模式访问了，如提示Entering Passive mode，就代表是passive模式，默认是不行的，因为ftp passive模式被iptables挡住了，下面会讲怎么开启，如果懒得开的话，就看看你客户端ftp是否有port模式的选项，或者把passive模式的选项去掉。如果客户端还是不行，看看客户端上的主机的电脑是否开了防火墙，关吧） 7.开启passive模式 默认是开启的，但是要指定一个端口范围，打开vsftpd.conf文件，在后面加上123pasv_min_port=30000pasv_max_port=30999 表示端口范围为30000~30999，这个可以随意改。改完重启一下vsftpd 由于指定这段端口范围，iptables也要相应的开启这个范围，所以像上面那样打开iptables文件 也是在21上下面另起一行，更那行差不多，只是把21 改为30000:30999,然后:wq保存，重启下iptables。这样就搞定了。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[List、Set、Map集合存放null分析]]></title>
    <url>%2F2014%2F08%2F16%2Fjava-collection-null%2F</url>
    <content type="text"><![CDATA[验证下。 上代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package com.sky.code.collection;import java.util.*;import java.util.concurrent.ConcurrentHashMap;public class TestNull &#123; public static void main(String[] args)&#123; System.out.println("测试ArrayList"); List&lt;Object&gt; list = new ArrayList&lt;Object&gt;(); list.add(null); list.add(null); System.out.println(list); System.out.println("测试LinkedList"); List&lt;Object&gt; list1 = new LinkedList&lt;Object&gt;(); list1.add(null); list1.add(null); System.out.println(list1); System.out.println("测试HashSet"); Set&lt;Object&gt; set = new HashSet&lt;Object&gt;(); set.add(null); set.add(null); System.out.println(set); System.out.println("测试TreeSet"); Set&lt;Object&gt; treeSet = new TreeSet&lt;Object&gt;(); try &#123; treeSet.add(null); &#125;catch (Exception e)&#123; System.out.println(e); &#125; System.out.println("测试LinkedHashSet"); Set&lt;Object&gt; set1 = new LinkedHashSet&lt;Object&gt;(); set1.add(null); set1.add(null); System.out.println(set1); System.out.println("测试HashMap"); Map&lt;Object, Object&gt; hashMap = new HashMap&lt;&gt;(); hashMap.put(null, null); hashMap.put(null, null); hashMap.put(1, null); hashMap.put(1, 12); System.out.println(hashMap); System.out.println("测试LinkedHashMap"); Map&lt;Object, Object&gt; linkedHashMap = new LinkedHashMap&lt;&gt;(); linkedHashMap.put(null, null); linkedHashMap.put(null, null); linkedHashMap.put(1, null); linkedHashMap.put(1, 12); System.out.println(linkedHashMap); System.out.println("测试TreeMap"); Map&lt;Object, Object&gt; treemap = new TreeMap&lt;&gt;(); treemap.put(1, null); try &#123; treemap.put(null, 1); &#125; catch (Exception e)&#123; System.out.println(e); &#125; treemap.put(2, 12); System.out.println(treemap); System.out.println("测试ConcurrentHashMap"); Map&lt;Object, Object&gt; concurrentHashMap = new ConcurrentHashMap&lt;&gt;(); try &#123; concurrentHashMap.put(null, 1); &#125;catch (Exception e)&#123; System.out.println(e); &#125; try &#123; concurrentHashMap.put(1, null); &#125; catch (Exception e)&#123; System.out.println(e); &#125; concurrentHashMap.put(1, 12); System.out.println(treemap); System.out.println("测试Hashtable"); Map&lt;Object, Object&gt; hashtable = new Hashtable&lt;&gt;(); try &#123; hashtable.put(null,"null"); &#125; catch (Exception e)&#123; System.out.println(e); &#125; try &#123; hashtable.put(3,null); &#125; catch (Exception e)&#123; System.out.println(e); &#125; &#125;&#125; 结果123456789101112131415161718192021222324测试ArrayList[null, null]测试LinkedList[null, null]测试HashSet[null]测试TreeSetjava.lang.NullPointerException测试LinkedHashSet[null]测试HashMap&#123;null=null, 1=12&#125;测试LinkedHashMap&#123;null=null, 1=12&#125;测试TreeMapjava.lang.NullPointerException&#123;1=null, 2=12&#125;测试ConcurrentHashMapjava.lang.NullPointerExceptionjava.lang.NullPointerException&#123;1=null, 2=12&#125;测试Hashtablejava.lang.NullPointerExceptionjava.lang.NullPointerException 结论很明显tree结构的都不接受key为null,hashtable和concurrenthashmap是key和value都不支持null的，其他都就随便吧。🙂 所有代码在 https://github.com/ejunjsh/java-code/tree/master/src/main/java/com/sky/code/collection]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[javascript在浏览器跨域访问的几种处理方式]]></title>
    <url>%2F2014%2F08%2F02%2Fjquery-cross-domain-ajax%2F</url>
    <content type="text"><![CDATA[这里说的js跨域是指通过js在不同的域之间进行数据传输或通信，比如用ajax向一个不同的域请求数据。只要协议、域名、端口有任何一个不同，都被当作是不同的域。下表给出了相对http://store.company.com/dir/page.html 同源检测的结果: URL 结果 原因 http://store.company.com/dir2/other.html 成功 http://store.company.com/dir/inner/another.html 成功 https://store.company.com/dir/inner/another.html 失败 协议不同 http://store.company.com:81/dir/inner/another.html 失败 端口不同 http://news.company.com/dir/inner/another.html 失败 域名不同 要解决跨域的问题，我们可以使用以下几种方法： 通过jsonp在js中，我们直接用XMLHttpRequest请求不同域上的数据时，是不可以的。但是，在页面上引入不同域上的js脚本文件却是可以的，jsonp正是利用这个特性来实现的。比如，有个a.html页面，它里面的代码需要利用ajax获取一个不同域上的json数据，假设这个json数据地址是http://example.com/data.php, 那么a.html中的代码就可以这样：123456&lt;scirpt&gt;function dosomething(jsondata)&#123; //处理获得的json数据&#125;&lt;/scirpt&gt;&lt;scirpt src="http://example.com/data.php?callback=dosomething"&gt;&lt;/scirpt&gt; 我们看到获取数据的地址后面还有一个callback参数，按惯例是用这个参数名，但是你用其他的也一样。当然如果获取数据的jsonp地址页面不是你自己能控制的，就得按照提供数据的那一方的规定格式来操作了。因为是当做一个js文件来引入的，所以http://example.com/data.php返回的必须是一个能执行的js文件，所以这个页面的php代码可能是这样的:12345&lt;?php$callback=&amp;_GET[`callback`];//获得回调函数名$data=array('a','b','c');//要返回的数据echo $callback.'('.json_encode($data).')';//输出?&gt; 最终那个页面输出的结果是:1dosomething(['a','b','c']) 所以通过http://example.com/data.php?callback=dosomething得到的js文件，就是我们之前定义的dosomething函数,并且它的参数就是我们需要的json数据，这样我们就跨域获得了我们需要的数据。 这样jsonp的原理就很清楚了，通过script标签引入一个js文件，这个js文件载入成功后会执行我们在url参数中指定的函数，并且会把我们需要的json数据作为参数传入。所以jsonp是需要服务器端的页面进行相应的配合的。 知道jsonp跨域的原理后我们就可以用js动态生成script标签来进行跨域操作了，而不用特意的手动的书写那些script标签。如果你的页面使用jquery，那么通过它封装的方法就能很方便的来进行jsonp操作了。12345&lt;scirpt&gt;$.getJSON('http://example.com/data.php?callback=?',function(jsondata)&#123; ////处理获得的json数据&#125;);&lt;/scirpt&gt; 原理是一样的，只不过我们不需要手动的插入script标签以及定义回掉函数。jquery会自动生成一个全局函数来替换callback=?中的问号，之后获取到数据后又会自动销毁，实际上就是起一个临时代理函数的作用。$.getJSON方法会自动判断是否跨域，不跨域的话，就调用普通的ajax方法；跨域的话，则会以异步加载js文件的形式来调用jsonp的回调函数。 通过XHR2HTML5中提供的XMLHTTPREQUEST Level2（及XHR2）已经实现了跨域访问。但ie10以下不支持只需要在服务端填上响应头123header(&quot;Access-Control-Allow-Origin:*&quot;);/*星号表示所有的域都可以接受，*/header(&quot;Access-Control-Allow-Methods:GET,POST&quot;);]]></content>
      <categories>
        <category>javascript</category>
      </categories>
      <tags>
        <tag>javascirpt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用cglib生成的代理类取不到注解的问题]]></title>
    <url>%2F2014%2F07%2F19%2Fjava-cglib%2F</url>
    <content type="text"><![CDATA[经常用cglib来创建代理类来实现aop的功能，可是，当想用反射来取得代理类所代理的类的注解的时候，却怎么也取不到。。。。 然后搜了下stackoverflow，http://stackoverflow.com/questions/1706751/retain-annotations-on-cglib-proxies用@Inherited,注解自己的注解（绕~~）12345@Inherited@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface MyAnnotation &#123;&#125; 然后用这个注解注解你要代理的类，那样通过反射可以拿到被代理的注解。原来CGLIB 返回的代理类是被代理的类的子类，加上这个标志就可以令子类继承这个注解，@Inherited 字面意思就是有继承的意思。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH免密码登陆以及穿越跳板机]]></title>
    <url>%2F2014%2F03%2F27%2Flinux-ssh-gateway%2F</url>
    <content type="text"><![CDATA[免密码直连 [user@hostA ~] $ssh hostBSTEP1. 在hostA上生成RSA公钥私钥（在~/.ssh/下生成RSA私钥id_rsa，公钥id_rsa.pub）1[user@hostA ~]$ ssh-keygen -t rsa STEP2. 将hostA的公钥传给hostB机器（在hostB机器的.ssh/authorized_keys里面添加A机器的id_rsa.pub）方法一：使用 ssh-copy-id工具1[user@hostA ~]$ ssh-copy-id -i .ssh/id_rsa.pub hostB 方法二：人肉拼接12[user@hostA ~]$ scp .ssh/id_rsa.pub hostB:~/.ssh/temp[user@hostB ~]$ cat .ssh/temp &gt;&gt; .ssh/authorized_keys 穿越跳板机 desktop -&gt; gateway -&gt; server在我厂，从desktop访问server，必须先登录到gateway，即:12[user@desktop ~]$ ssh gateway[user@gateway ~]$ ssh server 根据”1. 免密码直连”虽然可以减少两次输入密码，但仍然很麻烦。如果可以一步到位该多好。另外，SCP文件更郁闷，先把文件从desktop拷到gateway，再拷到server，奔溃了。。 STEP1. 在desktop上生成RSA公钥私钥，方法同1.STEP1STEP2. 在desktop的.ssh目录下生成config文件，文件内容如下12Host shortServer ProxyCommand ssh user@gateway nc server %p 2&gt;/dev/null 其中，gateway为跳板机ip，server为服务器ip，shortServer自定义一个缩写的服务器名称。例如：我厂gateway地址x.y.g.w，以及N多服务器，例如*.a.b.c。12Host 123 ProxyCommand ssh user@x.y.g.w nc 123.a.b.c %p 2&gt;/dev/null STEP3. 把deskptop的公钥分发给gateway和server分发给gateway，因为可以直连desktop和gateway，直接用1.STEP2.方法一分发给server，因为暂时无法直连desktop和server，所以只能使用1.STEP2.方法二 连接方法：1[user@desktop ~]$ ssh 123 此时SCP也能直接用了1[user@desktop ~]$ scp ToCopy.txt 123:]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么要使用SLF4J而不是Log4J]]></title>
    <url>%2F2014%2F03%2F27%2Fjava-slf4j-log4j%2F</url>
    <content type="text"><![CDATA[每一个Java程序员都知道日志对于任何一个Java应用程序，尤其是服务端程序是至关重要的，而很多程序员也已经熟悉各种不同的日志库如java.util.logging、Apache log4j、logback。但如果你还不知道SLF4J（Simple logging facade for Java）的话，那么是时候去在你项目中学习使用SLF4J了。在这篇文章中，我们将学习为什么使用SLF4J比log4j或者java.util.logging要优秀。 不管怎样，让我们回到这个话题，SLF4J不同于其他日志类库，与其它有很大的不同。SLF4J(Simple logging Facade for Java)不是一个真正的日志实现，而是一个抽象层（ abstraction layer），它允许你在后台使用任意一个日志类库。如果是在编写供内外部都可以使用的API或者通用类库，那么你真不会希望使用你类库的客户端必须使用你选择的日志类库。 如果一个项目已经使用了log4j，而你加载了一个类库，比方说 Apache Active MQ——它依赖于于另外一个日志类库logback，那么你就需要把它也加载进去。但如果Apache Active MQ使用了SLF4J，你可以继续使用你的日志类库而无语忍受加载和维护一个新的日志框架的痛苦。 总的来说，SLF4J使你的代码独立于任意一个特定的日志API，这是一个对于开发API的开发者很好的思想。虽然抽象日志类库的思想已经不是新鲜的事物而且Apache commons logging也已经在使用这种思想了，但现在SLF4J正迅速成为Java世界的日志标准。让我们再看看几个使用SLF4J而不是log4j、logback或者java.util.logging的理由。SLF4J对比Log4J，logback和java.util.Logging的优势正如我之前说的，在你的代码中使用SLF4J写日志语句的主要出发点是使得你的程序独立于任意特定的日志类库，依赖于特定类可能需要不同与你已有的配置，并且导致更多维护的麻烦。但除此之外，还要一个SLF4J API的特性使得我坚持使用SLF4J而抛弃我长期间钟爱的Lof4j的理由，是被称为占位符(place holder)，在代码中表示为“{}”的特性。占位符是一个非常类似于在String的format()方法中的%s，因为它会在运行时被某个提供的实际字符串所替换。这不仅降低了你代码中字符串连接次数，而且还节省了新建的String对象。即使你可能没需要那些对象，但这个依旧成立，取决于你的生产环境的日志级别，例如在DEBUG或者INFO级别的字符串连接。因为String对象是不可修改的并且它们建立在一个String池中，它们消耗堆内存( heap memory)而且大多数时间他们是不被需要的，例如当你的应用程序在生产环境以ERROR级别运行时候，一个String使用在DEBUG语句就是不被需要的。通过使用SLF4J,你可以在运行时延迟字符串的建立，这意味着只有需要的String对象才被建立。而如果你已经使用log4j，那么你已经对于在if条件中使用debug语句这种变通方案十分熟悉了，但SLF4J的占位符就比这个好用得多。 这是你在Log4j中使用的方案，但肯定这一点都不有趣并且降低了代码可读性因为增加了不必要的繁琐重复代码(boiler-plate code)：123if (logger.isDebugEnabled()) &#123; logger.debug("Processing trade with id: " + id + " symbol: " + symbol);&#125; 另一方面，如果你使用SLF4J的话，你可以得到在极简洁的格式的结果，就像以下展示的一样：1logger.debug("Processing trade with id: &#123;&#125; and symbol : &#123;&#125; ", id, symbol); 在SLF4J，我们不需要字符串连接而且不会导致暂时不需要的字符串消耗。取而代之的，我们在一个以占位符和以参数传递实际值的模板格式下写日志信息。你可能会在想万一我有很个参数怎么办？嗯，那么你可以选择使用变量参数版本的日志方法或者用以Object数组传递。这是一个相当的方便和高效方法的打日志方法。记住，在生产最终日志信息的字符串之前，这个方法会检查一个特定的日志级别是不是打开了，这不仅降低了内存消耗而且预先降低了CPU去处理字符串连接命令的时间。这里是使用SLF4J日志方法的代码，来自于slf4j-log4j12-1.6.1.jar中的Log4j的适配器类Log4jLoggerAdapter。123456public void debug(String format, Object arg1, Object arg2) &#123; if (logger.isDebugEnabled()) &#123; FormattingTuple ft = MessageFormatter.format(format, arg1, arg2); logger.log(FQCN, Level.DEBUG, ft.getMessage(), ft.getThrowable()); &#125;&#125; 同时，我们也很值得知道打日志是对应用程序的性能有着很大影响的，在生产环节上只进行必要的日志记录是我们所建议的。 怎么用SLF4J做Log4J的日志记录除了以上好处，我想还有一个告诫，就是为了使用SLF4J，你不仅需要包含SLF4J的API jar包，例如 slf4j-api-1.6.1.jar，还需要相关Jar包，这取决于你在后台使用的日志类库。如果你想要使用和Log4J 一起使用SLF4J ，Simple Logging Facade for Java,，你需要包含以下的Jar包在你的classpath中，取决于哪个SLF4J和你在使用的Log4J的版本。例如： slf4j-api-1.6.1.jar – JAR for SLF4J API log4j-1.2.16.jar – JAR for Log4J API slf4j-log4j12-1.6.1.jar – Log4J Adapter for SLF4J 如果你在使用Maven去管理你的项目依赖，你只需要包含SLF4J JAR包，maven会包含它的依赖的相关包。为了和SLF4J一起中使用Log4J，你可以包含以下的依赖在你项目中的pom.xml。1234567891011&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt;&lt;/dependency&gt; 还有，如果你对于使用变量参数版本（variable argument version ）的日志方法感兴趣的话，那么就导入SLF4J 1.7的版本吧。 总结总结这次说的，我建议使用SLF4J的而不是直接使用 Log4j, commons logging, logback 或者 java.util.logging 已经足够充分了。 在你的开源或内部类库中使用SLF4J会使得它独立于任何一个特定的日志实现，这意味着不需要管理多个日志配置或者多个日志类库，你的客户端会很感激这点。SLF4J提供了基于占位符的日志方法，这通过去除检查isDebugEnabled(), isInfoEnabled()等等，提高了代码可读性。通过使用SLF4J的日志方法，你可以延迟构建日志信息（Srting）的开销，直到你真正需要，这对于内存和CPU都是高效的。作为附注，更少的暂时的字符串意味着垃圾回收器（Garbage Collector）需要做更好的工作，这意味着你的应用程序有为更好的吞吐量和性能。这些好处只是冰山一角，你将在开始使用SL4J和阅读其中代码的时候知道更多的好处。我强烈建议，任何一个新的Java程序员，都应该使用SLF4J做日志而不是使用包括Log4J在内的其他日志API。 原文链接： javarevisited 翻译： ImportNew.com - Jaskey译文链接： http://www.importnew.com/7450.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux硬盘分区相关命令]]></title>
    <url>%2F2014%2F03%2F02%2Flinux-disk-related%2F</url>
    <content type="text"><![CDATA[分区fdiskfdisk命令参数介绍fdisk -l 显示所有分区详细信息fdisk [设备名]进入交互模式，以下是交互参数： p、打印分区表。 n、新建一个新分区。 d、删除一个分区。 q、退出不保存。 w、把分区写进分区表，保存并退出。 实例1$ fdisk /dev/hdd 按”p”键打印分区表这块硬磁尚未分区按”n”键新建一个分区。出现两个菜单e表示扩展分区，p表示主分区按”p”键出现提示：”Partition number (1-4): “选择主分区号输入”1”表示第一个主分区。直接按回车表示1柱面开始分区。提示最后一个柱面或大小。输入+5620M 按回车表示第一个分区为5G空间。按”p”查看一下分区这样一个主分区就分好了。接下来分第二个主分区，把剩余空间都给第二个主分区。按”n”键新增一个分区按”p”键设为主分区输入”2”把主分区编号设为2按两下回车把剩余空间分给第二个主分区。按”p”键打印分区表按”w”键保存退出。读者可根据自己的硬盘大小来划分合适的分区。 格式化mkfs分好区要使用的话，还要格式化下mkfs -t ext4 [分区名]12$ mkfs -t ext4 /dev/hdd1$ mkfs -t ext4 /dev/hdd2 挂载分区mount12$ mount /dev/hdd1 /hdd1$ mount /dev/hdd2 /hdd2 查看挂载df1234567$ df -hFilesystem Size Used Avail Use% Mounted on/dev/hda3 7.5G 2.8G 4.3G 40% //dev/hda1 99M 17M 78M 18% /boottmpfs 62M 0 62M 0% /dev/shm/dev/hdd1 2.5G 68M 2.3G 3% /hdd1/dev/hdd2 2.5G 68M 2.3G 3% /hdd2 上面的两个分区分别挂载在/hdd1,/hdd2 修改标签e2label一般上面的分区之后都分配一个很长的label，可以通过e2label [分区] [新label] 改下标签1$ e2label /dev/hdd1 mydisk 卸载分区umount命令类似于mount12$ umount /dev/hdd1 /hdd1$ umount /dev/hdd2 /hdd2 查看文件夹大小-dudu的英文原义为“disk usage”，含义为显示磁盘空间的使用情况，统计目录（或文件）所占磁盘空间的大小。该命令的功能是逐级进入指定目录的每一个子目录并显示该目录占用文件系统数据块（1024字节）的情况。若没有给出指定目录，则对当前目录进行统计。 df命令的各个选项含义如下：12345678-s：对每个Names参数只给出占用的数据块总数。-a：递归地显示指定目录中各文件及子目录中各文件占用的数据块数。若既不指定-s，也不指定-a，则只显示Names中的每一个目录及其中的各子目录所占的磁盘块数。-b：以字节为单位列出磁盘空间使用情况（系统默认以k字节为单位）。-k：以1024字节为单位列出磁盘空间使用情况。-c：最后再加上一个总计（系统默认设置）。-l：计算所有的文件大小，对硬链接文件，则计算多次。-x：跳过在不同文件系统上的目录不予统计。-h: 人类可读的显示出对应的文件夹的大小，例如（10K，22M） 下面举例说明du命令的使用：123456789101112131415# 查看/mnt目录占用磁盘空间的情况$ du –abk /mnt1 /mnt/cdrom1 /mnt/floppy3 /mnt # 列出各目录所占的磁盘空间，但不详细列出每个文件所占的空间# 输出清单中的第1列是以块为单位计的磁盘空间容量，第2列列出目录中使用这些空间的目录名称。$ du3684 ./log84 ./libnids-1.17/doc720 ./libnids-1.17/src32 ./libnids-1.17/samples1064 ./libnids-1.174944 . 这可能是一个很长的清单，有时只需要一个总数。这时可在du命令中加-s选项来取得总数： 12345678$ du –s /mnt 3 /mnt # 列出所有文件和目录所占的空间（使用a选项），并以字节为单位（使用b选项）来计算大小$ du –ab /root/mail6144 mail/sent-mail1024 mail/saved-messages8192 mail 其实用-h 选项可以显示更加可读的数据12345678910$ du -h12K ./.gnupg0 ./bin16K ./.ssh0 ./.local/share/systemd0 ./.local/share0 ./.local8.0K ./.vim/colors8.0K ./.vim175M .]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>fdisk</tag>
        <tag>e2label</tag>
        <tag>mkfs</tag>
        <tag>mount</tag>
        <tag>umount</tag>
        <tag>df</tag>
        <tag>du</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven resource 记录]]></title>
    <url>%2F2014%2F02%2F20%2Fmaven-resouce%2F</url>
    <content type="text"><![CDATA[今天遇到maven打包的时候发现在main/java里面的xml没有打包进jar上。 上网搜了下，maven默认打包main/resource的资源，要想打包main/java的要像下面这样配。12345678910111213&lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;include&gt;**/*.tld&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; 搞定收工。。。不知道有没有更好到方法呢？]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins + git+maven做持续集成]]></title>
    <url>%2F2014%2F02%2F20%2Fjenkins-jenkins-git-maven%2F</url>
    <content type="text"><![CDATA[下个jenkins，官网去下 http://jenkins-ci.org/，里面提供war包下载，直接部署到tomcat什么上面吧。 部署成功后打开网站例如：http://localhost/jenkin，默认是不带git的插件的，所以先去下一个先，点击主页的右侧“系统管理”=&gt;&quot;管理插件&quot;=&gt;“可选插件” 找到”git plungin” 然后点击直接安装。（这可能要花点时间） 下完git插件后就要配环境了，还是点击右侧“系统管理”=&gt;“系统设置” 主要配jdk和maven的环境。（把自动安装勾掉就可以输路径了），保存下就可以了。 点击右侧“新建”=&gt;“构建一个maven项目” 输入名字到下一步如下图勾上“丢弃旧的构建”，按照自己的需要配置，否则很占硬盘。配置git仓库（如果是私有库，必须添加一个Credentials，点击右侧Add，在弹出界面录入帐号密码）接下来配置定时构建（勾上Build periodically,图中设置是每15分钟一次），配置要执行的maven命令 clean install (mvn不用输)保存后，一个构建就可以了（可以立即构建试试，也可以定时执行）。jenkins提供了一堆的页面去展示构建的过程，很不错。如果web程序想自动部署到本地的tomcat，可以试下cargo插件，加上下面代码到项目pom上。下面代码改下路径就可以了。当然也可以部署到远程，就不贴了。 12345678910111213141516171819202122&lt;plugin&gt; &lt;groupId&gt;org.codehaus.cargo&lt;/groupId&gt; &lt;artifactId&gt;cargo-maven2-plugin&lt;/artifactId&gt; &lt;version&gt;1.4.5&lt;/version&gt; &lt;configuration&gt; &lt;container&gt; &lt;containerId&gt;tomcat7x&lt;/containerId&gt; &lt;home&gt;/opt/apache-tomcat-7.0.47&lt;/home&gt; &lt;/container&gt; &lt;configuration&gt; &lt;type&gt;existing&lt;/type&gt; &lt;home&gt;/opt/apache-tomcat-7.0.47&lt;/home&gt; &lt;/configuration&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;tomcat-deploy&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt;&lt;goal&gt;deploy&lt;/goal&gt;&lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 这样一个持续集成就配好了。想想那边提交代码，另一边就自动部署到tomcat上，爽歪歪了。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jenkins</tag>
        <tag>git</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux中Kill进程的N种方法]]></title>
    <url>%2F2014%2F02%2F20%2Flinux-kill%2F</url>
    <content type="text"><![CDATA[常规篇首先，用ps查看进程，方法如下：12345678910$ ps -ef …… smx 1822 1 0 11:38 ? 00:00:49 gnome-terminal smx 1823 1822 0 11:38 ? 00:00:00 gnome-pty-helper smx 1824 1822 0 11:38 pts/0 00:00:02 bash smx 1827 1 4 11:38 ? 00:26:28 /usr/lib/firefox-3.6.18/firefox-bin smx 1857 1822 0 11:38 pts/1 00:00:00 bash smx 1880 1619 0 11:38 ? 00:00:00 update-notifier …… smx 11946 1824 0 21:41 pts/0 00:00:00 ps -ef 或者：12345678910$ ps -aux …… smx 1822 0.1 0.8 58484 18152 ? Sl 11:38 0:49 gnome-terminal smx 1823 0.0 0.0 1988 712 ? S 11:38 0:00 gnome-pty-helper smx 1824 0.0 0.1 6820 3776 pts/0 Ss 11:38 0:02 bash smx 1827 4.3 5.8 398196 119568 ? Sl 11:38 26:13 /usr/lib/firefox-3.6.18/firefox-bin smx 1857 0.0 0.1 6688 3644 pts/1 Ss 11:38 0:00 bash smx 1880 0.0 0.6 41536 12620 ? S 11:38 0:00 update-notifier …… smx 11953 0.0 0.0 2716 1064 pts/0 R+ 21:42 0:00 ps -aux 此时如果我想杀了火狐的进程就在终端输入：$ kill -s 9 1827其中-s 9 制定了传递给进程的信号是９，即强制、尽快终止进程。各个终止信号及其作用见附录。 进阶篇改进１把ps的查询结果通过管道给grep查找包含特定字符串的进程。管道符“|”用来隔开两个命令，管道符左边命令的输出会作为管道符右边命令的输入。123$ ps -ef | grep firefox smx 1827 1 4 11:38 ? 00:27:33 /usr/lib/firefox-3.6.18/firefox-bin smx 12029 1824 0 21:54 pts/0 00:00:00 grep --color=auto firefox 这次就清爽了。然后就是1$ kill -s 9 1827 改进２——使用pgrep一看到pgrep首先会想到什么？没错，grep！pgrep的p表明了这个命令是专门用于进程查询的grep。12$ pgrep firefox 1827 看到了什么？没错火狐的PID，接下来又要打字了：1$ kill -s 9 1827 改进３——使用pidof看到pidof想到啥？没错pid of xx，字面翻译过来就是 xx的PID。12$ pidof firefox-bin 1827 和pgrep相比稍显不足的是，pidof必须给出进程的全名。然后就是老生常谈：1$ kill -s 9 1827 无论使用ps 然后慢慢查找进程PID 还是用grep查找包含相应字符串的进程，亦或者用pgrep直接查找包含相应字符串的进程pid，然后手动输入给kill杀掉，都稍显麻烦。有没有更方便的方法？有！ 改进４1$ ps -ef | grep firefox | grep -v grep | cut -c 9-15 | xargs kill -s 9 说明：grep firefox的输出结果是，所有含有关键字“firefox”的进程。grep -v grep是在列出的进程中去除含有关键字“grep”的进程。cut -c 9-15是截取输入行的第9个字符到第15个字符，而这正好是进程号PID。xargs kill -s 9中的xargs命令是用来把前面命令的输出结果（PID）作为kill -s 9命令的参数，并执行该命令。kill -s 9会强行杀掉指定进程。难道你不想抱怨点什么？没错太长了 改进５知道pgrep和pidof两个命令，干嘛还要打那么长一串！1$ pgrep firefox | xargs kill -s 9 改进６12345678$ ps -ef | grep firefox | grep -v grep | awk '&#123;print $2&#125;' | xargs kill -9 ```` 其中awk '&#123;print $2&#125;' 的作用就是打印（print）出第二列的内容。根据常规篇，可以知道ps输出的第二列正好是PID。就把进程相应的PID通过xargs传递给kill作参数，杀掉对应的进程。 ## 改进７难道每次都要调用xargs把PID传递给kill？答案是否定的： ````bash$ kill -s 9 `ps -aux | grep firefox | grep -v grep | awk '&#123;print $2&#125;'` 改进８没错，命令依然有点长，换成pgrep。1$ kill -s 9 `pgrep firefox` 改进9——pkill看到pkill想到了什么？没错pgrep和kill！pkill＝pgrep+kill。1$ pkill -９ firefox 说明：”-9” 即发送的信号是9，pkill与kill在这点的差别是：pkill无须 “ｓ”，终止信号等级直接跟在 “-“ 后面。之前我一直以为是 “-s 9”，结果每次运行都无法终止进程。 改进10——killallkillall和pkill是相似的,不过如果给出的进程名不完整，killall会报错。pkill或者pgrep只要给出进程名的一部分就可以终止进程。1$ killall -9 firefox 附录：各种信号及其用途1234567891011121314151617181920212223242526272829Signal Description Signal number on Linux x86[1] SIGABRT Process aborted 6 SIGALRM Signal raised by alarm 14 SIGBUS Bus error: &quot;access to undefined portion of memory object&quot; 7 SIGCHLD Child process terminated, stopped (or continued*) 17 SIGCONT Continue if stopped 18 SIGFPE Floating point exception: &quot;erroneous arithmetic operation&quot; 8 SIGHUP Hangup 1 SIGILL Illegal instruction 4 SIGINT Interrupt 2 SIGKILL Kill (terminate immediately) 9 SIGPIPE Write to pipe with no one reading 13 SIGQUIT Quit and dump core 3 SIGSEGV Segmentation violation 11 SIGSTOP Stop executing temporarily 19 SIGTERM Termination (request to terminate) 15 SIGTSTP Terminal stop signal 20 SIGTTIN Background process attempting to read from tty (&quot;in&quot;) 21 SIGTTOU Background process attempting to write to tty (&quot;out&quot;) 22 SIGUSR1 User-defined 1 10 SIGUSR2 User-defined 2 12 SIGPOLL Pollable event 29 SIGPROF Profiling timer expired 27 SIGSYS Bad syscall 31 SIGTRAP Trace/breakpoint trap 5 SIGURG Urgent data available on socket 23 SIGVTALRM Signal raised by timer counting virtual time: &quot;virtual timer expired&quot; 26 SIGXCPU CPU time limit exceeded 24 SIGXFSZ File size limit exceeded 25]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>awk</tag>
        <tag>grep</tag>
        <tag>kill</tag>
        <tag>ps</tag>
        <tag>killall</tag>
        <tag>pkill</tag>
        <tag>pgrep</tag>
        <tag>xargs</tag>
        <tag>pidof</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet获取URL地址]]></title>
    <url>%2F2014%2F02%2F20%2Fjava-servlet-url%2F</url>
    <content type="text"><![CDATA[老是不记得，果断mark. 这里来说说用Servlet获取URL地址。在HttpServletRequest类里，有以下六个取URL的函数： getContextPath 取得项目名 getServletPath 取得Servlet名 getPathInfo 取得Servlet后的URL名，不包括URL参数 getRequestURL 取得不包括参数的URL getRequestURI 取得不包括参数的URI，即去掉协议和服务器名的URL 具体如下图：相对应的函数的值如下： getContextPath：/ServletTest getServletPath：/main getPathInfo：/index/testpage/test getRequestURL：http://localhost:8080/ServletTest/main/index/testpage/test getRequestURI：/ServletTest/main/index/testpage/test]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[find 命令]]></title>
    <url>%2F2014%2F02%2F14%2Flinux-find%2F</url>
    <content type="text"><![CDATA[mark一下，当笔记，以后忘了查看 查找文件find ./ -type f 查找目录find ./ -type d 查找名字为test的文件或目录find ./ -name test 查找名字符合正则表达式的文件,注意前面的‘.’(查找到的文件带有目录)find ./ -regex .so.*.gz 查找目录并列出目录下的文件(为找到的每一个目录单独执行ls命令，没有选项-print时文件列表前一行不会显示目录名称)find ./ -type d -print -exec ls {} \; 查找目录并列出目录下的文件(为找到的每一个目录单独执行ls命令,执行命令前需要确认)find ./ -type d -ok ls {} \; 查找目录并列出目录下的文件(将找到的目录添加到ls命令后一次执行，参数过长时会分多次执行)find ./ -type d -exec ls {} + 查找文件名匹配.c的文件find ./ -name \.c 打印test文件名后，打印test文件的内容find ./ -name test -print -exec cat {} \; 不打印test文件名，只打印test文件的内容find ./ -name test -exec cat {} \; 查找文件更新日时在距现在时刻二天以内的文件find ./ -mtime -2 查找文件更新日时在距现在时刻二天以上的文件find ./ -mtime +2 查找文件更新日时在距现在时刻一天以上二天以内的文件find ./ -mtime 2 查找文件更新日时在距现在时刻二分以内的文件find ./ -mmin -2 查找文件更新日时在距现在时刻二分以上的文件find ./ -mmin +2 查找文件更新日时在距现在时刻一分以上二分以内的文件find ./ -mmin 2 查找文件更新时间比文件abc的内容更新时间新的文件find ./ -newer abc 查找文件访问时间比文件abc的内容更新时间新的文件find ./ -anewer abc 查找空文件或空目录find ./ -empty 查找空文件并删除find ./ -empty -type f -print -delete 查找权限为644的文件或目录(需完全符合)find ./ -perm 664 查找用户/组权限为读写，其他用户权限为读(其他权限不限)的文件或目录find ./ -perm -664 查找用户有写权限或者组用户有写权限的文件或目录find ./ -perm /220find ./ -perm /u+w,g+wfind ./ -perm /u=w,g=w 查找所有者权限有读权限的目录或文件find ./ -perm -u=r 查找用户组权限有读权限的目录或文件find ./ -perm -g=r 查找其它用户权限有读权限的目录或文件find ./ -perm -o=r 查找所有者为lzj的文件或目录find ./ -user lzj 查找组名为gname的文件或目录find ./ -group gname 查找文件的用户ID不存在的文件find ./ -nouser 查找文件的组ID不存在的文件find ./ -nogroup 查找有执行权限但没有可读权限的文件find ./ -executable ! -readable 查找文件size小于10个字节的文件或目录find ./ -size -10c 查找文件size等于10个字节的文件或目录find ./ -size 10c 查找文件size大于10个字节的文件或目录find ./ -size +10c 查找文件size小于10k的文件或目录find ./ -size -10k 查找文件size小于10M的文件或目录find ./ -size -10M 查找文件size小于10G的文件或目录find ./ -size -10G]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>find</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crontab 命令]]></title>
    <url>%2F2014%2F02%2F13%2Flinux-crontab%2F</url>
    <content type="text"><![CDATA[描述一般编写调度让系统帮你定时做事情有两种方式： 在/etc目录下有几个cron.开头的文件夹，这里存放有系统运行的一些调度程序,可以把自己的调度写在这里。 每个用户可以用crontab命令建立自己的调度,调度文件存放在以用户名为名的文件/var/spool/cron/crontabs/[username]。 crontab命令有三种形式的命令行结构： crontab [-u user] [file] crontab [-u user] [-e|-l|-r] crontab -l -u [-e|-l|-r] 第一个命令行中，file是命令文件的名字。如果在命令行中指定了这个文件，那么执行crontab命令，则会把这个文件保存到/var/spool/cron/crontabs/[username]；如果在命令行中没有制定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将他们也存放在上面的那个文件。 命令行中-u选项的作用用来切换用户，默认不加这个选项是表示当前用户 命令行中-r选项的作用其实是删除当前用户的/var/spool/cron/crontabs/[username] 这个文件； 命令行中-l选项的作用是显示当前用户/var/spool/cron/crontabs/[username]文件的内容。 使用命令crontab -u user -e命令编辑用户user的cron(c)作业。用户通过编辑文件来增加或修改任何作业请求。 执行命令crontab -u user -r即可删除指定用户的所有的cron作业。 文件里的每一个请求必须包含以spaces和tabs分割的六个域。前五个字段可以取整数值，指定何时开始工作，第六个域是字符串，称为命令字段，其中包括了crontab调度执行的命令。 第一道第五个字段的整数取值范围及意义是： 0～59 表示分 1～23 表示小时 1～31 表示日 1～12 表示月份 （也可以是英文缩写） 0～6 表示星期（其中0表示星期日,也可以是英文缩写如sun，mon） 20个超实用的Crontab使用实例 每天 02:00 执行任务 10 2 * * * /bin/sh backup.sh 每天 5:00和17:00执行任务 10 5,17 * * * /scripts/script.sh 每分钟执行一次任务 通常情况下，我们并没有每分钟都需要执行的脚本 1* * * * * /scripts/script.sh 每周日 17:00 执行任务 10 17 * * sun /scripts/script.sh 每 10min 执行一次任务 1*/10 * * * * /scripts/monitor.sh 在特定的某几个月执行任务 1* * * jan,may,aug * /script/script.sh 在特定的某几天执行任务 10 17 * * sun,fri /script/scripy.sh 在每周五、周日的17点执行任务 在某个月的第一个周日执行任务 10 2 * * sun [ $(date +%d) -le 07 ] &amp;&amp; /script/script.sh 每四个小时执行一个任务 10 */4 * * * /scripts/script.sh 每周一、周日执行任务 10 4,17 * * sun,mon /scripts/script.sh 每个30秒执行一次任务我们没有办法直接通过上诉类似的例子去执行，因为最小的是1min。但是我们可以通过如下的方法。 12* * * * * /scripts/script.sh* * * * * sleep 30; /scripts/script.sh 多个任务在一条命令中配置 1* * * * * /scripts/script.sh; /scripts/scrit2.sh 每年执行一次任务 1@yearly /scripts/script.sh @yearly 类似于“0 0 1 1 *”。它会在每年的第一分钟内执行，通常我们可以用这个发送新年的问候。 每月执行一次任务 1@monthly /scripts/script.sh 每周执行一次任务 1@weekly /scripts/script.sh 每天执行一次任务 1@daily /scripts/script.sh 每小时执行一次任务 1@hourly /scripts/script.sh 系统重启时执行 1@reboot /scripts/script.sh 将 Cron 结果重定向的特定的账户默认情况下，cron 只会将结果详情发送给 cron 被指定的用户。如果需要发送给其他用户，可以设置MAIL属性： 123# crontab -lMAIL=bob0 2 * * * /script/backup.sh 将所有的 cron 命令备份到文本文件当中这是一个当我们丢失了cron命令后方便快速的一个恢复方式。下面是利用这个方式恢复cron的一个小例子。（看看就行~）首先：检查当前的cron 123# crontab -lMAIL=rahul0 2 * * * /script/backup.sh 然后：备份cron到文件中 1234# crontab -l &gt; cron-backup.txt# cat cron-backup.txtMAIL=rahul0 2 * * * /script/backup.sh 接着：移除当前的cron 123# crontab -r# crontab -lno crontab for root 恢复：从text file中恢复 1234# crontab cron-backup.txt# crontab -lMAIL=rahul0 2 * * * /script/backup.sh]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>crontab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jQuery.extend 函数详解]]></title>
    <url>%2F2014%2F02%2F05%2Fjquery-extend%2F</url>
    <content type="text"><![CDATA[Jquery的扩展方法extend是我们在写插件的过程中常用的方法，该方法有一些重载原型，在此，我们一起去了解了解。 如何使用Jquery的扩展方法原型是: 1extend(dest,src1,src2,src3...); 它的含义是将src1,src2,src3…合并到dest中,返回值为合并后的dest,由此可以看出该方法合并后，是修改了dest的结构的。如果想要得到合并的结果却又不想修改dest的结构，可以如下使用：1var newSrc=$.extend(&#123;&#125;,src1,src2,src3...)//也就是将"&#123;&#125;"作为dest参数。 这样就可以将src1,src2,src3…进行合并，然后将合并结果返回给newSrc了。如下例：1var result=$.extend(&#123;&#125;,&#123;name:"Tom",age:21&#125;,&#123;name:"Jerry",sex:"Boy"&#125;) 那么合并后的结果1result=&#123;name:"Jerry",age:21,sex:"Boy"&#125; 也就是说后面的参数如果和前面的参数存在相同的名称，那么后面的会覆盖前面的参数值。 省略dest参数上述的extend方法原型中的dest参数是可以省略的，如果省略了，则该方法就只能有一个src参数，而且是将该src合并到调用extend方法的对象中去，如： $.extend(src)该方法就是将src合并到jquery的全局对象中去，如：123$.extend(&#123; hello:function()&#123;alert('hello');&#125; &#125;); 就是将hello方法合并到jquery的全局对象中。 $.fn.extend(src)该方法将src合并到jquery的实例对象中去，如:123$.fn.extend(&#123; hello:function()&#123;alert('hello');&#125; &#125;); 就是将hello方法合并到jquery的实例对象中。 举个例子1234567$.extend(&#123;net:&#123;&#125;&#125;);```` 这是在jquery全局对象中扩展一个net命名空间。 ````javascript$.extend($.net,&#123; hello:function()&#123;alert('hello');&#125; &#125;) 这是将hello方法扩展到之前扩展的Jquery的net命名空间中去。 Jquery的extend方法还有一个重载原型1234567extend(boolean,dest,src1,src2,src3...)```` 第一个参数boolean代表是否进行深度拷贝，其余参数和前面介绍的一致，什么叫深层拷贝，我们看一个例子： ````javascriptvar result=$.extend( true, &#123;&#125;, &#123; name: &quot;John&quot;, location: &#123;city: &quot;Boston&quot;,county:&quot;USA&quot;&#125; &#125;, &#123; last: &quot;Resig&quot;, location: &#123;state: &quot;MA&quot;,county:&quot;China&quot;&#125; &#125; ); 我们可以看出src1中嵌套子对象location:{city:”Boston”},src2中也嵌套子对象location:{state:”MA”},第一个深度拷贝参数为true，那么合并后的结果就是：12result=&#123;name:"John",last:"Resig", location:&#123;city:"Boston",state:"MA",county:"China"&#125;&#125; 也就是说它会将src中的嵌套子对象也进行合并，而如果第一个参数boolean为false，我们看看合并的结果是什么，如下：1234var result=$.extend( false, &#123;&#125;, &#123; name: "John", location:&#123;city: "Boston",county:"USA"&#125; &#125;, &#123; last: "Resig", location: &#123;state: "MA",county:"China"&#125; &#125; ); 那么合并后的结果就是:1result=&#123;name:"John",last:"Resig",location:&#123;state:"MA",county:"China"&#125;&#125; 以上就是$.extend()在项目中经常会使用到的一些细节。]]></content>
      <categories>
        <category>jQuery</category>
      </categories>
      <tags>
        <tag>jQuery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一分钟教你知道乐观锁和悲观锁的区别]]></title>
    <url>%2F2014%2F01%2F27%2Fsql-pessimistic-optimistic-lock%2F</url>
    <content type="text"><![CDATA[悲观锁(Pessimistic Lock), 顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 乐观锁(Optimistic Lock), 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>悲观锁</tag>
        <tag>乐观锁</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于jQuery新的事件绑定机制on()的使用技巧]]></title>
    <url>%2F2014%2F01%2F11%2Fjquery-on%2F</url>
    <content type="text"><![CDATA[本篇文章介绍了，关于jQuery新的事件绑定机制on()的使用技巧。需要的朋友参考下今天浏览jQuery的deprecated列表，发现live()和die()在里面了，赶紧看了一下，发现从jQuery1.7开始，jQuery引入了全新的事件绑定机制，on()和off()两个函数统一处理事件绑定。因为在此之前有bind(), live(), delegate()等方法来处理事件绑定，jQuery从性能优化以及方式统一方面考虑决定推出新的函数来统一事件绑定方法并且替换掉以前的方法。 用法1on(events,[selector],[data],fn) events:一个或多个用空格分隔的事件类型和可选的命名空间，如”click”或”keydown.myPlugin” 。selector:一个选择器字符串用于过滤器的触发事件的选择器元素的后代。如果选择器为null或省略，当它到达选定的元素，事件总是触发。data:当一个事件被触发时要传递event.data给事件处理函数。fn:该事件被触发时执行的函数。 false 值也可以做一个函数的简写，返回false。 替换bind()当第二个参数’selector’为null时，on()和bind()其实在用法上基本上没有任何区别了，所以我们可以认为on()只是比bind()多了一个可选的’selector’参数，所以on()可以非常方便的换掉bind() 替换live()在1.4之前相信大家非常喜欢使用live(),因为它可以把事件绑定到当前以及以后添加的元素上面，当然在1.4之后delegate()也可以做类似的事情了。live()的原理很简单，它是通过document进行事件委派的，因此我们也可以使用on()通过将事件绑定到document来达到live()一样的效果。 live()写法123 $('#list li').live('click', '#list li', function() &#123; //function code here. &#125;); on()写法123$(document).on('click', '#list li', function() &#123; //function code here.&#125;); 这里的关键就是第二个参数’selector’在起作用了。它是一个过滤器的作用，只有被选中元素的后代元素才会触发事件。 替换delegate()delegate()是1.4引入的，目的是通过祖先元素来代理委派后代元素的事件绑定问题，某种程度上和live()优点相似。只不过live()是通过document元素委派，而delegate则可以是任意的祖先节点。使用on()实现代理的写法和delegate()基本一致。 delegate()的写法123$('#list').delegate('li', 'click', function() &#123; //function code here. &#125;); on()写法123$('#list').on('click', 'li', function() &#123; //function code here.&#125;); 貌似第一个和第二个参数的顺序颠倒了一下，别的基本一样。 总结jQuery推出on()的目的有2个，一是为了统一接口，二是为了提高性能，所以从现在开始用on()替换bind(), live(), delegate吧。尤其是不要再用live()了，因为它已经处于不推荐使用列表了，随时会被干掉。如果只绑定一次事件，那接着用one()吧，这个没有变化。]]></content>
      <categories>
        <category>jQuery</category>
      </categories>
      <tags>
        <tag>jQuery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux awk命令详解]]></title>
    <url>%2F2014%2F01%2F06%2Flinux-awk%2F</url>
    <content type="text"><![CDATA[简介awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。 awk有3个不同版本: awk、nawk和gawk，未作特别说明，一般指gawk，gawk 是 AWK 的 GNU 版本。 awk其名称得自于它的创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母。实际上 AWK 的确拥有自己的语言： AWK 程序设计语言 ， 三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。 使用方法1awk &apos;&#123;pattern + action&#125;&apos; &#123;filenames&#125; 尽管操作可能会很复杂，但语法总是这样，其中 pattern 表示 AWK 在数据中查找的内容，而 action 是在找到匹配内容时所执行的一系列命令。花括号（{}）不需要在程序中始终出现，但它们用于根据特定的模式对一系列指令进行分组。 pattern就是要表示的正则表达式，用斜杠括起来。 awk语言的最基本功能是在文件或者字符串中基于指定规则浏览和抽取信息，awk抽取信息后，才能进行其他文本操作。完整的awk脚本通常用来格式化文本文件中的信息。 通常，awk是以文件的一行为处理单位的。awk每接收文件的一行，然后执行相应的命令，来处理文本。 调用awk有三种方式调用awk 1.命令行方式awk [-F field-separator] ‘commands’ input-file(s)其中，commands 是真正awk命令，[-F域分隔符]是可选的。 input-file(s) 是待处理的文件。在awk中，文件的每一行中，由域分隔符分开的每一项称为一个域。通常，在不指名-F域分隔符的情况下，默认的域分隔符是空格。 2.shell脚本方式将所有的awk命令插入一个文件，并使awk程序可执行，然后awk命令解释器作为脚本的首行，一遍通过键入脚本名称来调用。相当于shell脚本首行的：#!/bin/sh可以换成：#!/bin/awk 3.将所有的awk命令插入一个单独文件，然后调用：1awk -f awk-script-file input-file(s) 其中，-f选项加载awk-script-file中的awk脚本，input-file(s)跟上面的是一样的。 本章重点介绍命令行方式。 入门实例假设last -n 5的输出如下 123456[root@www ~]# last -n 5 &lt;==仅取出前五行root pts/1 192.168.1.100 Tue Feb 10 11:21 still logged inroot pts/1 192.168.1.100 Tue Feb 10 00:46 - 02:28 (01:41)root pts/1 192.168.1.100 Mon Feb 9 11:41 - 18:30 (06:48)dmtsai pts/1 192.168.1.100 Mon Feb 9 11:41 - 11:41 (00:00)root tty1 Fri Sep 5 14:09 - 14:10 (00:01) 如果只是显示最近登录的5个帐号 123456#last -n 5 | awk &apos;&#123;print $1&#125;&apos;rootrootrootdmtsairoot awk工作流程是这样的：读入有’\n’换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域。默认域分隔符是”空白键” 或 “[tab]键”,所以$1表示登录用户，$3表示登录用户ip,以此类推。 如果只是显示/etc/passwd的账户 12345#cat /etc/passwd |awk -F &apos;:&apos; &apos;&#123;print $1&#125;&apos; rootdaemonbinsys 这种是awk+action的示例，每行都会执行action{print $1}。 -F指定域分隔符为’:’。 如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以tab键分割 12345#cat /etc/passwd |awk -F &apos;:&apos; &apos;&#123;print $1&quot;\t&quot;$7&#125;&apos;root /bin/bashdaemon /bin/shbin /bin/shsys /bin/sh 如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以逗号分割,而且在所有行添加列名name,shell,在最后一行添加”blue,/bin/nosh”。 12345678cat /etc/passwd |awk -F &apos;:&apos; &apos;BEGIN &#123;print &quot;name,shell&quot;&#125; &#123;print $1&quot;,&quot;$7&#125; END &#123;print &quot;blue,/bin/nosh&quot;&#125;&apos;name,shellroot,/bin/bashdaemon,/bin/shbin,/bin/shsys,/bin/sh....blue,/bin/nosh awk工作流程是这样的：先执行BEGING，然后读取文件，读入有/n换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域， $0则表示所有域,$1表示第一个域,$n表示第n个域 ,随后开始执行模式所对应的动作action。接着开始读入第二条记录······直到所有的记录都读完，最后执行END操作。 搜索/etc/passwd有root关键字的所有行 12#awk -F: &apos;/root/&apos; /etc/passwdroot:x:0:0:root:/root:/bin/bash 这种是pattern的使用示例，匹配了pattern(这里是root)的行才会执行action(没有指定action，默认输出每行的内容)。 搜索支持正则，例如找root开头的: awk -F: ‘/^root/‘ /etc/passwd 搜索/etc/passwd有root关键字的所有行，并显示对应的shell 12# awk -F: &apos;/root/&#123;print $7&#125;&apos; /etc/passwd /bin/bash 这里指定了action{print $7} awk内置变量awk有许多内置变量用来设置环境信息，这些变量可以被改变，下面给出了最常用的一些变量。 1234567891011ARGC 命令行参数个数ARGV 命令行参数排列ENVIRON 支持队列中系统环境变量的使用FILENAME awk浏览的文件名FNR 浏览文件的记录数FS 设置输入域分隔符，等价于命令行 -F选项NF 浏览记录的域的个数NR 已读的记录数OFS 输出域分隔符ORS 输出记录分隔符RS 控制记录分隔符 此外, $0变量是指整条记录。$1表示当前行的第一个域,$2表示当前行的第二个域,……以此类推。 统计/etc/passwd:文件名，每行的行号，每行的列数，对应的完整行内容: 12345#awk -F &apos;:&apos; &apos;&#123;print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF &quot;,linecontent:&quot;$0&#125;&apos; /etc/passwdfilename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/bashfilename:/etc/passwd,linenumber:2,columns:7,linecontent:daemon:x:1:1:daemon:/usr/sbin:/bin/shfilename:/etc/passwd,linenumber:3,columns:7,linecontent:bin:x:2:2:bin:/bin:/bin/shfilename:/etc/passwd,linenumber:4,columns:7,linecontent:sys:x:3:3:sys:/dev:/bin/sh 使用printf替代print,可以让代码更加简洁，易读 1awk -F &apos;:&apos; &apos;&#123;printf(&quot;filename:%10s,linenumber:%s,columns:%s,linecontent:%s\n&quot;,FILENAME,NR,NF,$0)&#125;&apos; /etc/passwd print和printfawk中同时提供了print和printf两种打印输出的函数。 其中print函数的参数可以是变量、数值或者字符串。字符串必须用双引号引用，参数用逗号分隔。如果没有逗号，参数就串联在一起而无法区分。这里，逗号的作用与输出文件的分隔符的作用是一样的，只是后者是空格而已。 printf函数，其用法和c语言中printf基本相似,可以格式化字符串,输出复杂时，printf更加好用，代码更易懂。 awk编程 变量和赋值 除了awk的内置变量，awk还可以自定义变量。 下面统计/etc/passwd的账户人数1234awk &apos;&#123;count++;print $0;&#125; END&#123;print &quot;user count is &quot;, count&#125;&apos; /etc/passwdroot:x:0:0:root:/root:/bin/bash......user count is 40 count是自定义变量。之前的action{}里都是只有一个print,其实print只是一个语句，而action{}可以有多个语句，以;号隔开。 这里没有初始化count，虽然默认是0，但是妥当的做法还是初始化为0: 12345awk &apos;BEGIN &#123;count=0;print &quot;[start]user count is &quot;, count&#125; &#123;count=count+1;print $0;&#125; END&#123;print &quot;[end]user count is &quot;, count&#125;&apos; /etc/passwd[start]user count is 0root:x:0:0:root:/root:/bin/bash...[end]user count is 40 统计某个文件夹下的文件占用的字节数12ls -l |awk &apos;BEGIN &#123;size=0;&#125; &#123;size=size+$5;&#125; END&#123;print &quot;[end]size is &quot;, size&#125;&apos;[end]size is 8657198 如果以M为单位显示:12ls -l |awk &apos;BEGIN &#123;size=0;&#125; &#123;size=size+$5;&#125; END&#123;print &quot;[end]size is &quot;, size/1024/1024,&quot;M&quot;&#125;&apos; [end]size is 8.25889 M 注意，统计不包括文件夹的子目录。 条件语句awk中的条件语句是从C语言中借鉴来的，见如下声明方式： 12345678910111213141516171819if (expression) &#123; statement; statement; ... ...&#125;if (expression) &#123; statement;&#125; else &#123; statement2;&#125;if (expression) &#123; statement1;&#125; else if (expression1) &#123; statement2;&#125; else &#123; statement3;&#125; 统计某个文件夹下的文件占用的字节数,过滤4096大小的文件(一般都是文件夹):12ls -l |awk &apos;BEGIN &#123;size=0;print &quot;[start]size is &quot;, size&#125; &#123;if($5!=4096)&#123;size=size+$5;&#125;&#125; END&#123;print &quot;[end]size is &quot;, size/1024/1024,&quot;M&quot;&#125;&apos; [end]size is 8.22339 M 循环语句 awk中的循环语句同样借鉴于C语言，支持while、do/while、for、break、continue，这些关键字的语义和C语言中的语义完全相同。 数组 因为awk中数组的下标可以是数字和字母，数组的下标通常被称为关键字(key)。值和关键字都存储在内部的一张针对key/value应用hash的表格里。由于hash不是顺序存储，因此在显示数组内容时会发现，它们并不是按照你预料的顺序显示出来的。数组和变量一样，都是在使用时自动创建的，awk也同样会自动判断其存储的是数字还是字符串。一般而言，awk中的数组用来从记录中收集信息，可以用于计算总和、统计单词以及跟踪模板被匹配的次数等等。 显示/etc/passwd的账户 12345678awk -F &apos;:&apos; &apos;BEGIN &#123;count=0;&#125; &#123;name[count] = $1;count++;&#125;; END&#123;for (i = 0; i &lt; NR; i++) print i, name[i]&#125;&apos; /etc/passwdrootdaemonbinsyssyncgames......]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux sed命令详解]]></title>
    <url>%2F2014%2F01%2F05%2Flinux-sed%2F</url>
    <content type="text"><![CDATA[功能说明：利用script来处理文本文件。语 法：1sed [-hnV][-e script][-f script文件][文本文件] 补充说明：sed可依照script的指令，来处理、编辑文本文件。 参数：12345-e script或—expression=script 以选项中指定的script来处理输入的文本文件。 -f script文件或—file=script文件 以选项中指定的script文件来处理输入的文本文件。 -h或—help 显示帮助。 -n或—quiet或--silent 仅显示script处理后的结果。 -V或—version 显示版本信息。 例子：1$ sed -e 's/123/1234/' a.txt 将a.txt文件中所有行中的123用1234替换（-e表示命令以命令行的方式执行；参数s，表示执行替换操作） 1$ sed -e '3,5 a4' a.txt 将a.txt文件中的3行到5行之间所有行的后面添加一行内容为4的行（参数a，表示添加行，参数a后面指定添加的内容） 1$ sed -e '1 s/12/45/' a.txt 把第一行的12替换成45 1$ sed -i "s/oldstring/newstring/g" `grep oldstring -rl yourdir` 批量处理通过grep搜索出来的所有文档，将这些文档中所有的oldstring用newstring替换（-i参数表示直接对目标文件操作） 1$ sed -n 's/^test/mytest/p' example.file (-n)选项和p标志一起使用表示只打印那些发生替换的行。也就是说，如果某一行开头的test被替换成mytest，就打印它。(^这是正则表达式中表示开头，该符号后面跟的就是开头的字符串)（参数p表示打印行） 1$ sed 's/^wangpan/&amp;19850715/' example.file 表示被替换换字符串被找到后，被替换的字符串通过＆符号连接给出的字符串组成新字符传替换被替换的字符串,所有以wangpan开头的行都会被替换成它自已加19850715，变成wangpan19850715 1$ sed -n 's/\(love\)able/\1rs/p' example.file love被标记为1，所有loveable会被替换成lovers，而且替换的行会被打印出来。需要将这条命令分解，s/是表示替换操作，(love)表示选中love字符串，(love)able/表示包含loveable的行，(love)able/\l表示love字符串标记为1，表示在替换过程中不变。rs/表示替换的目标字符串。这条命令的操作含义：只打印替换了的行 1$ sed 's#10#100#g' example.file 不论什么字符，紧跟着s命令的都被认为是新的分隔符，所以，“#”在这里是分隔符，代替了默认的“/”分隔符。表示把所有10替换成100。 1$ sed -n '/love/,/unlove/p' example.file 只打印包含love字符串行到包含unlove字符串行之间的所有行（确定行的范围就是通过逗号实现的） 1$ sed -n '5,/^wang/p' example 只打印从第五行开始到第一个包含以wang开始的行之间的所有行 1$ sed '/love/,/unlove/s/$/wangpan/' example.file 对于包含love字符串的行到包含unlove字符串之间的行，每行的末尾用字符串wangpan替换。字符串$/表示以字符串结尾的行，$/表示每一行的结尾，s/$/wangpan/表示每一行的结尾添加wangpan字符串 1$ sed -e '11,53d' -e 's/wang/pan/' example.file (-e)选项允许在同一行里执行多条命令。如例子所示，第一条命令删除11至53行，第二条命令用pan替换wang。命令的执行顺序对结果有影响。如果两个命令都是替换命令，那么第一个替换命令将影响第二个替换命令的结果。(参数d，表示删除指定的行) 1$ sed --expression='s/wang/pan/' --expression='/love/d' example.file 一个比-e更好的命令是–expression。它能给sed表达式赋值。 1$ sed '/wangpan/r file' example.file file里的内容被读进来，显示在与wangpan匹配的行后面，如果匹配多行，则file的内容将显示在所有匹配行的下面。参数r，表示读出文件，后面空格紧跟文件名称 1$ sed -n '/test/w file' example.file 在example.file中所有包含test的行都被写入file里。参数w，表示将匹配的行写入到指定的文件file中 1$ sed '/^test/a\oh! My god!' example.file ‘oh! My god!’被追加到以test开头的行的后面，sed要求参数a后面有一个反斜杠。 1$ sed '/test/i\oh! My god!' example.file ‘oh! My god!’被追加到包含test字符串行的前面，参数i表示添加指定内容到匹配行的前面，sed要求参数i后面有一个反斜杠 1$ sed '/test/&#123; n; s/aa/bb/; &#125;' example.file 如果test被匹配，则移动到匹配行的下一行，替换这一行的aa，变为bb。参数n，表示读取匹配行的下一个输入行，用下一个命令处理新的行而不是匹配行。Sed要求参数n后跟分号 1$ sed '1,10y/abcde/ABCDE/' example.file 把1—10行内所有abcde转变为大写，注意，正则表达式元字符不能使用这个命令。参数y，表示把一个字符翻译为另外的字符（但是不用于正则表达式） 1$ sed -i 's/now/right now/g' test_sed_command.txt 表示直接操作文件test_sed_command.txt，将文件test_sed_command.txt中所有的now用right now替换。参数-i，表示直接操作修改文件，不输出。 1$ sed '2q' test_sed_command.txt 在打印完第2行后，就直接退出sed。参数q，表示退出 1$ sed -e '/old/h' -e '/girl-friend/G' test_sed_command.txt 首先了解参数h，拷贝匹配成功行的内容到内存中的缓冲区。在了解参数G，获得内存缓冲区的内容，并追加到当前模板块文本的后面。上面命令行的含义：将包含old字符串的行的内容保存在缓冲区中，然后将缓冲区的内容拿出来添加到包含girl-friend字符串行的后面。隐含要求搜集到缓冲区的匹配行在需要添加行的前面。 1$ sed -e '/test/h' -e '/wangpan/x' example.file 将包含test字符串的行的内容保存在缓冲区中，然后再将缓冲区的内容替换包含wangpan字符串的行。参数x，表示行替换操作。隐含要求搜集到缓冲区的匹配行在需要被替换行的前面。]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>sed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux grep命令详解]]></title>
    <url>%2F2014%2F01%2F04%2Flinux-grep%2F</url>
    <content type="text"><![CDATA[简介grep (global search regular expression(RE) and print out the line,全面搜索正则表达式并把行打印出来)是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 Unix的grep家族包括grep、egrep和fgrep。egrep和fgrep的命令只跟grep有很小不同。egrep是grep的扩展，支持更多的re元字符， fgrep就是fixed grep或fast grep，它们把所有的字母都看作单词，也就是说，正则表达式中的元字符表示回其自身的字面意义，不再特殊。linux使用GNU版本的grep。它功能更强，可以通过-G、-E、-F命令行选项来使用egrep和fgrep的功能。 grep常用用法grep [-acinv] [–color=auto] ‘搜寻字符串’ filename选项与参数：-a ：将 binary 文件以 text 文件的方式搜寻数据-c ：计算找到 ‘搜寻字符串’ 的次数-i ：忽略大小写的不同，所以大小写视为相同-n ：顺便输出行号-v ：反向选择，亦即显示出没有 ‘搜寻字符串’ 内容的那一行！–color=auto ：可以将找到的关键词部分加上颜色的显示喔！ 将/etc/passwd，有出现 root 的行取出来1234567grep root /etc/passwdroot:x:0:0:root:/root:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologin或cat /etc/passwd | grep root root:x:0:0:root:/root:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologin 将/etc/passwd，有出现 root 的行取出来,同时显示这些行在/etc/passwd的行号123grep -n root /etc/passwd1:root:x:0:0:root:/root:/bin/bash30:operator:x:11:0:operator:/root:/sbin/nologin 在关键字的显示方面，grep 可以使用 –color=auto 来将关键字部分使用颜色显示。 这可是个很不错的功能啊！但是如果每次使用 grep 都得要自行加上 –color=auto 又显的很麻烦～ 此时那个好用的 alias 就得来处理一下啦！你可以在 ~/.bashrc 内加上这行：alias grep=&#39;grep --color=auto&#39;再以source ~/.bashrc来立即生效即可喔！ 这样每次运行 grep 他都会自动帮你加上颜色显示啦 将/etc/passwd，将没有出现 root 的行取出来123grep -v root /etc/passwdroot:x:0:0:root:/root:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologin 将/etc/passwd，将没有出现 root 和nologin的行取出来123grep -v root /etc/passwd | grep -v nologinroot:x:0:0:root:/root:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologin 用 dmesg 列出核心信息，再以 grep 找出内含 eth 那行,要将捉到的关键字显色，且加上行号来表示：12345[root@www ~]# dmesg | grep -n --color=auto 'eth'247:eth0: RealTek RTL8139 at 0xee846000, 00:90:cc:a6:34:84, IRQ 10248:eth0: Identified 8139 chip type 'RTL-8139C'294:eth0: link up, 100Mbps, full-duplex, lpa 0xC5E1305:eth0: no IPv6 routers present 你会发现除了 eth 会有特殊颜色来表示之外，最前面还有行号喔！在关键字的显示方面，grep 可以使用 –color=auto 来将关键字部分使用颜色显示。 这可是个很不错的功能啊！但是如果每次使用 grep 都得要自行加上 –color=auto 又显的很麻烦～ 此时那个好用的 alias 就得来处理一下啦！你可以在 ~/.bashrc 内加上这行：alias grep=&#39;grep --color=auto&#39;再以source ~/.bashrc来立即生效即可喔！ 这样每次运行 grep 他都会自动帮你加上颜色显示啦 用 dmesg 列出核心信息，再以 grep 找出内含 eth 那行,在关键字所在行的前两行与后三行也一起捉出来显示12345678[root@www ~]# dmesg | grep -n -A3 -B2 --color=auto 'eth'245-PCI: setting IRQ 10 as level-triggered246-ACPI: PCI Interrupt 0000:00:0e.0[A] -&gt; Link [LNKB] ...247:eth0: RealTek RTL8139 at 0xee846000, 00:90:cc:a6:34:84, IRQ 10248:eth0: Identified 8139 chip type 'RTL-8139C'249-input: PC Speaker as /class/input/input2250-ACPI: PCI Interrupt 0000:00:01.4[B] -&gt; Link [LNKB] ...251-hdb: ATAPI 48X DVD-ROM DVD-R-RAM CD-R/RW drive, 2048kB Cache, UDMA(66) 如上所示，你会发现关键字 247 所在的前两行及 248 后三行也都被显示出来！这样可以让你将关键字前后数据捉出来进行分析啦！ 根据文件内容递归查找目录123grep ‘energywise’ * #在当前目录搜索带'energywise'行的文件grep -r ‘energywise’ * #在当前目录及其子目录下搜索'energywise'行的文件grep -l -r ‘energywise’ * #在当前目录及其子目录下搜索'energywise'行的文件，但是不显示匹配的行，只显示匹配的文件 grep与正规表达式字符类 字符类的搜索：如果我想要搜寻 test 或 taste 这两个单字时，可以发现到，其实她们有共通的 ‘t?st’ 存在～这个时候，我可以这样来搜寻：123[root@www ~]# grep -n 't[ae]st' regular_express.txt8:I can't finish the test.9:Oh! The soup taste good. 其实 [] 里面不论有几个字节，他都谨代表某一个字节， 所以，上面的例子说明了，我需要的字串是tast或test两个字串而已！ 字符类的反向选择 [^] ：如果想要搜索到有 oo 的行，但不想要 oo 前面有 g，如下12345[root@www ~]# grep -n '[^g]oo' regular_express.txt2:apple is my favorite food.3:Football game is not use feet only.18:google is the best tools for search keyword.19:goooooogle yes! 第 2,3 行没有疑问，因为 foo 与 Foo 均可被接受！ 但是第 18 行明明有 google 的 goo 啊～别忘记了，因为该行后面出现了 tool 的 too 啊！所以该行也被列出来～ 也就是说， 18 行里面虽然出现了我们所不要的项目 (goo) 但是由於有需要的项目 (too) ， 因此，是符合字串搜寻的喔！ 至於第 19 行，同样的，因为 goooooogle 里面的 oo 前面可能是 o ，例如： go(ooo)oogle ，所以，这一行也是符合需求的！ 字符类的连续：再来，假设我 oo 前面不想要有小写字节，所以，我可以这样写 [^abcd….z]oo ， 但是这样似乎不怎么方便，由於小写字节的 ASCII 上编码的顺序是连续的， 因此，我们可以将之简化为底下这样：12[root@www ~]# grep -n '[^a-z]oo' regular_express.txt3:Football game is not use feet only. 也就是说，当我们在一组集合字节中，如果该字节组是连续的，例如大写英文/小写英文/数字等等， 就可以使用[a-z],[A-Z],[0-9]等方式来书写，那么如果我们的要求字串是数字与英文呢？ 呵呵！就将他全部写在一起，变成：[a-zA-Z0-9]。 我们要取得有数字的那一行，就这样：123[root@www ~]# grep -n '[0-9]' regular_express.txt5:However, this dress is about $ 3183 dollars.15:You are the best is mean you are the no. 1. 行首与行尾字节 ^ $行首字符：如果我想要让 the 只在行首列出呢？ 这个时候就得要使用定位字节了！我们可以这样做：12[root@www ~]# grep -n '^the' regular_express.txt12:the symbol '*' is represented as start. 此时，就只剩下第 12 行，因为只有第 12 行的行首是 the 开头啊～此外， 如果我想要开头是小写字节的那一行就列出呢？可以这样：12345678[root@www ~]# grep -n '^[a-z]' regular_express.txt2:apple is my favorite food.4:this dress doesn't fit me.10:motorcycle is cheap than car.12:the symbol '*' is represented as start.18:google is the best tools for search keyword.19:goooooogle yes!20:go! go! Let's go. 如果我不想要开头是英文字母，则可以是这样：123[root@www ~]# grep -n '^[^a-zA-Z]' regular_express.txt1:"Open Source" is a good mechanism to develop programs.21:# I am VBird ^ 符号，在字符类符号(括号[])之内与之外是不同的！ 在 [] 内代表反向选择，在 [] 之外则代表定位在行首的意义！ 那如果我想要找出来，行尾结束为小数点 (.) 的那一行：12345678910111213[root@www ~]# grep -n '\.$' regular_express.txt1:"Open Source" is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.4:this dress doesn't fit me.10:motorcycle is cheap than car.11:This window is clear.12:the symbol '*' is represented as start.15:You are the best is mean you are the no. 1.16:The world &lt;Happy&gt; is the same with "glad".17:I like dog.18:google is the best tools for search keyword.20:go! go! Let's go. 特别注意到，因为小数点具有其他意义(底下会介绍)，所以必须要使用转义字符(\)来加以解除其特殊意义！ 找出空白行：12[root@www ~]# grep -n '^$' regular_express.txt22: 因为只有行首跟行尾 (^$)，所以，这样就可以找出空白行啦！ 任意一个字节 . 与重复字节 *这两个符号在正则表达式的意义如下： . (小数点)：代表一定有一个任意字节的意思；* (星号)：代表重复前一个字符， 0 到无穷多次的意思，为组合形态 假设我需要找出 g??d 的字串，亦即共有四个字节， 起头是 g 而结束是 d ，我可以这样做：1234[root@www ~]# grep -n 'g..d' regular_express.txt1:"Open Source" is a good mechanism to develop programs.9:Oh! The soup taste good.16:The world &lt;Happy&gt; is the same with "glad". 因为强调 g 与 d 之间一定要存在两个字节，因此，第 13 行的 god 与第 14 行的 gd 就不会被列出来啦！ 如果我想要列出有 oo, ooo, oooo 等等的数据， 也就是说，至少要有两个(含) o 以上，该如何是好？ 因为 * 代表的是重复 0 个或多个前面的 RE 字符的意义， 因此 o* 代表的是：拥有空字节或一个 o 以上的字节，因此，grep -n &#39;o*&#39; regular_express.txt将会把所有的数据都列印出来终端上！ 当我们需要至少两个 o 以上的字串时，就需要 ooo* ，亦即是：1234567[root@www ~]# grep -n 'ooo*' regular_express.txt1:"Open Source" is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.9:Oh! The soup taste good.18:google is the best tools for search keyword.19:goooooogle yes! 如果我想要字串开头与结尾都是 g，但是两个 g 之间仅能存在至少一个 o ，亦即是 gog, goog, gooog…. 等等，那该如何？123[root@www ~]# grep -n 'goo*g' regular_express.txt18:google is the best tools for search keyword.19:goooooogle yes! 如果我想要找出 g 开头与 g 结尾的行，当中的字符可有可无123456[root@www ~]# grep -n 'g.*g' regular_express.txt1:"Open Source" is a good mechanism to develop programs.14:The gd software is a library for drafting programs.18:google is the best tools for search keyword.19:goooooogle yes!20:go! go! Let's go. 因为是代表 g 开头与 g 结尾，中间任意字节均可接受，所以，第 1, 14, 20 行是可接受的喔！ 这个 .* 的 RE 表示任意字符是很常见的. 如果我想要找出任意数字的行？因为仅有数字，所以就成为：123[root@www ~]# grep -n '[0-9][0-9]*' regular_express.txt5:However, this dress is about $ 3183 dollars.15:You are the best is mean you are the no. 1. 限定连续 RE 字符范围 {}我们可以利用 . 与 RE 字符及 * 来配置 0 个到无限多个重复字节， 那如果我想要限制一个范围区间内的重复字节数呢？ 举例来说，我想要找出两个到五个 o 的连续字串，该如何作？这时候就得要使用到限定范围的字符 {} 了。 但因为 { 与 } 的符号在 shell 是有特殊意义的，因此， 我们必须要使用字符 \ 来让他失去特殊意义才行。 至於 {} 的语法是这样的，假设我要找到两个 o 的字串，可以是：1234567[root@www ~]# grep -n 'o\&#123;2\&#125;' regular_express.txt1:"Open Source" is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.9:Oh! The soup taste good.18:google is the best tools for search ke19:goooooogle yes! 假设我们要找出 g 后面接 2 到 5 个 o ，然后再接一个 g 的字串，他会是这样：12[root@www ~]# grep -n 'go\&#123;2,5\&#125;g' regular_express.txt18:google is the best tools for search keyword. 如果我想要的是 2 个 o 以上的 goooo….g 呢？除了可以是 gooo*g ，也可以是：123[root@www ~]# grep -n 'go\&#123;2,\&#125;g' regular_express.txt18:google is the best tools for search keyword.19:goooooogle yes! 扩展grep(grep -E 或者 egrep)：使用扩展grep的主要好处是增加了额外的正则表达式元字符集。 打印所有包含NW或EA的行。如果不是使用egrep，而是grep，将不会有结果查出。123egrep 'NW|EA' testfile northwest NW Charles Main 3.0 .98 3 34 eastern EA TB Savage 4.4 .84 5 20 对于标准grep，如果在扩展元字符前面加\，grep会自动启用扩展选项-E。123grep 'NW\|EA' testfilenorthwest NW Charles Main 3.0 .98 3 34eastern EA TB Savage 4.4 .84 5 20 搜索所有包含一个或多个3的行。12345678egrep '3+' testfilegrep -E '3+' testfilegrep '3\+' testfile #这3条命令将会northwest NW Charles Main 3.0 .98 3 34western WE Sharon Gray 5.3 .97 5 23northeast NE AM Main Jr. 5.1 .94 3 13central CT Ann Stephens 5.7 .94 5 13 搜索所有包含0个或1个小数点字符的行。1234567egrep '2\.?[0-9]' testfile grep -E '2\.?[0-9]' testfilegrep '2\.\?[0-9]' testfile #首先含有2字符，其后紧跟着0个或1个点，后面再是0和9之间的数字。western WE Sharon Gray 5.3 .97 5 23southwest SW Lewis Dalsass 2.7 .8 2 18eastern EA TB Savage 4.4 .84 5 20 搜索一个或者多个连续的no的行。123456egrep '(no)+' testfilegrep -E '(no)+' testfilegrep '\(no\)\+' testfile #3个命令返回相同结果，northwest NW Charles Main 3.0 .98 3 34northeast NE AM Main Jr. 5.1 .94 3 13north NO Margot Weber 4.5 .89 5 9 不使用正则表达式 fgrep 查询速度比grep命令快，但是不够灵活：它只能找固定的文本，而不是规则表达式。 如果你想在一个文件或者输出中找到包含星号字符的行12345fgrep '*' /etc/profilefor i in /etc/profile.d/*.sh ; do或grep -F '*' /etc/profilefor i in /etc/profile.d/*.sh ; do]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jQuery的.bind()、.live()和.delegate()之间区别]]></title>
    <url>%2F2014%2F01%2F04%2Fjquery-bind-delegate%2F</url>
    <content type="text"><![CDATA[摘要: jQuery的.bind()、.live()和.delegate()之间的区别并非总是那么明显的，然而，如果我们对所有的不同之处都有清晰的理解的话，那么这将会有助于我们编写出更加简洁的代码，以及防止在交互应用中弹出错误。 DOM树 首先，可视化一个HMTL文档的DOM树是很有帮助的。一个简单的HTML页面看起来就像是这个样子： 事件冒泡(又称事件传播) 当我们点击一个链接时，其触发了链接元素的单击事件，该事件则引发任何我们已绑定到该元素的单击事件上的函数的执行。 1$('a').bind('click', function() &#123; alert("That tickles!") &#125;); 因此一个单击操作会触发alert函数的执行。 click事件接着会向树的根方向传播，广播到父元素，然后接着是每个祖先元素，只要是它的某个后代元素上的单击事件被触发，事件就会传给它。 在操纵DOM的语境中，document是根节点。 现在我们可以较容易地说明.bind()、.live()和.delegate()的不同之处了。 .bind() 1$('a').bind('click', function() &#123; alert("That tickles!") &#125;); 这是最简单的绑定方法了。JQuery扫描文档找出所有的$(‘a’)元素，并把alert函数绑定到每个元素的click事件上。 .live() 1$('a').live('click', function() &#123; alert("That tickles!") &#125;); JQuery把alert函数绑定到$(document)元素上，并使用’click’和’a’作为参数。任何时候只要有事件冒泡到document节点上，它就查看该事件是否是一个click事件，以及该事件的目标元素与’a’这一CSS选择器是否匹配，如果都是的话，则执行函数。 live方法还可以被绑定到具体的元素(或“context”)而不是document上，像这样： 1$('a', $('#container')[0]).live(...); .delegate() 1$('#container').delegate('a', 'click', function() &#123; alert("That tickles!") &#125;); JQuery扫描文档查找$(‘#container’)，并使用click事件和’a’这一CSS选择器作为参数把alert函数绑定到$(‘#container’)上。任何时候只要有事件冒泡到$(‘#container’)上，它就查看该事件是否是click事件，以及该事件的目标元素是否与CCS选择器相匹配。如果两种检查的结果都为真的话，它就执行函数。 可以注意到，这一过程与.live()类似，但是其把处理程序绑定到具体的元素而非document这一根上。精明的JS’er们可能会做出这样的结论，即$(‘a’).live() == $(document).delegate(‘a’)，是这样吗?嗯，不，不完全是。 为什么.delegate()要比.live()好用 基于几个原因，人们通常更愿意选用jQuery的delegate方法而不是live方法。考虑下面的例子： 123$('a').live('click', function() &#123; blah() &#125;); // 或者 $(document).delegate('a', 'click', function() &#123; blah() &#125;); 速度 后者实际上要快过前者，因为前者首先要扫描整个的文档查找所有的$(‘a’)元素，把它们存成jQuery对象。尽管live函数仅需要把’a’作为串参数传递以用做之后的判断，但是$()函数并未“知道”被链接的方法将会是.live()。 而另一方面，delegate方法仅需要查找并存储$(document)元素。 一种寻求避开这一问题的方法是调用在$(document).ready()之外绑定的live，这样它就会立即执行。在这种方式下，其会在DOM获得填充之前运行，因此就不会查找元素或是创建jQuery对象了。 灵活性和链能力 live函数也挺令人费解的。想想看，它被链到$(‘a’)对象集上，但其实际上是在$(document)对象上发生作用。由于这个原因，它能够试图以一种吓死人的方式来把方法链到自身上。实际上，我想说的是，以$.live(‘a’,…)这一形式作为一种全局性的jQuery方法，live方法会更具意义一些。 仅支持CSS选择器 最后一点，live方法有一个非常大的缺点，那就是它仅能针对直接的CSS选择器做操作，这使得它变得非常的不灵活。 欲了解更多关于CSS选择器的缺点，请参阅Exploring jQuery .live() and .die()一文。 更新：感谢Hacker News上的pedalpete和后面评论中的Ellsass提醒我加入接下来的这一节内容。 为什么选择.live()或.delegate()而不是.bind() 毕竟，bind看起来似乎更加的明确和直接，难道不是吗?嗯，有两个原因让我们更愿意选择delegate或live而不是bind： 为了把处理程序附加到可能还未存在于DOM中的DOM元素之上。因为bind是直接把处理程序绑定到各个元素上，它不能把处理程序绑定到还未存在于页面中的元素之上。 如果你运行了$(‘a’).bind(…)，而后新的链接经由AJAX加入到了页面中，则你的bind处理程序对于这些新加入的链接来说是无效的。而另一方面live和delegate则是被绑定到另一个祖先节点上，因此其对于任何目前或是将来存在于该祖先元素之内的元素都是有效的。 或者为了把处理程序附加到单个元素上或是一小组元素之上，监听后代元素上的事件而不是循环遍历并把同一个函数逐个附加到DOM中的100个元素上。把处理程序附加到一个(或是一小组)祖先元素上而不是直接把处理程序附加到页面中的所有元素上，这种做法带来了性能上的好处。 停止传播 最后一个我想做的提醒与事件传播有关。通常情况下，我们可以通过使用这样的事件方法来终止处理函数的执行： 12345$('a').bind('click', function(e) &#123; e.preventDefault(); // 或者 e.stopPropagation(); &#125;); 不过，当我们使用live或是delegate方法的时候，处理函数实际上并没有在运行，需要等到事件冒泡到处理程序实际绑定的元素上时函数才会运行。而到此时为止，我们的其他的来自.bind()的处理函数早已运行了。 原文地址：http://developer.51cto.com/art/201103/249694.htm]]></content>
      <categories>
        <category>jQuery</category>
      </categories>
      <tags>
        <tag>jQuery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正确理解ThreadLocal]]></title>
    <url>%2F2014%2F01%2F04%2Fthreadlocal-understanding%2F</url>
    <content type="text"><![CDATA[首先，ThreadLocal 不是用来解决共享对象的多线程访问问题的，一般情况下，通过ThreadLocal.set() 到线程中的对象是该线程自己使用的对象，其他线程是不需要访问的，也访问不到的。 各个线程中访问的是不同的对象。 另外，说ThreadLocal使得各线程能够保持各自独立的一个对象，并不是通过ThreadLocal.set()来实现的，而是通过每个线程中的new 对象 的操作来创建的对象，每个线程创建一个，不是什么对象的拷贝或副本。 通过ThreadLocal.set()将这个新创建的对象的引用保存到各线程的自己的一个map中，每个线程都有这样一个map，执行ThreadLocal.get()时，各线程从自己的map中取出放进去的对象，因此取出来的是各自自己线程中的对象，ThreadLocal实例是作为map的key来使用的。如果ThreadLocal.set()进去的东西本来就是多个线程共享的同一个对象，那么多个线程的ThreadLocal.get()取得的还是这个共享对象本身，还是有并发访问问题。 下面来看一个hibernate中典型的ThreadLocal的应用： 1234567891011121314private static final ThreadLocal threadSession = new ThreadLocal(); public static Session getSession() throws InfrastructureException &#123; Session s = (Session) threadSession.get(); try &#123; if (s == null) &#123; s = getSessionFactory().openSession(); threadSession.set(s); &#125; &#125; catch (HibernateException ex) &#123; throw new InfrastructureException(ex); &#125; return s; &#125; 可以看到在getSession()方法中，首先判断当前线程中有没有放进去session，如果还没有，那么通过sessionFactory().openSession()来创建一个session，再将session set到线程中，实际是放到当前线程的ThreadLocalMap这个map中，这时，对于这个session的唯一引用就是当前线程中的那个ThreadLocalMap（下面会讲到），而threadSession作为这个值的key，要取得这个session可以通过threadSession.get()来得到，里面执行的操作实际是先取得当前线程中的ThreadLocalMap，然后将threadSession作为key将对应的值取出。这个session相当于线程的私有变量，而不是public的。显然，其他线程中是取不到这个session的，他们也只能取到自己的ThreadLocalMap中的东西。要是session是多个线程共享使用的，那还不乱套了。试想如果不用ThreadLocal怎么来实现呢？可能就要在action中创建session，然后把session一个个传到service和dao中，这可够麻烦的。或者可以自己定义一个静态的map，将当前thread作为key，创建的session作为值，put到map中，应该也行，这也是一般人的想法，但事实上，ThreadLocal的实现刚好相反，它是在每个线程中有一个map，而将ThreadLocal实例作为key，这样每个map中的项数很少，而且当线程销毁时相应的东西也一起销毁了，不知道除了这些还有什么其他的好处。 总之，ThreadLocal不是用来解决对象共享访问问题的，而主要是提供了保持对象的方法和避免参数传递的方便的对象访问方式。归纳了两点：1。每个线程中都有一个自己的ThreadLocalMap类对象，可以将线程自己的对象保持到其中，各管各的，线程可以正确的访问到自己的对象。2。将一个共用的ThreadLocal静态实例作为key，将不同对象的引用保存到不同线程的ThreadLocalMap中，然后在线程执行的各处通过这个静态ThreadLocal实例的get()方法取得自己线程保存的那个对象，避免了将这个对象作为参数传递的麻烦。 当然如果要把本来线程共享的对象通过ThreadLocal.set()放到线程中也可以，可以实现避免参数传递的访问方式，但是要注意get()到的是那同一个共享对象，并发访问问题要靠其他手段来解决。但一般来说线程共享的对象通过设置为某类的静态变量就可以实现方便的访问了，似乎没必要放到线程中。 ThreadLocal的应用场合，我觉得最适合的是按线程多实例（每个线程对应一个实例）的对象的访问，并且这个对象很多地方都要用到。 下面来看看ThreadLocal的实现原理（jdk1.5源码）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124public class ThreadLocal&lt;T&gt; &#123; /** * ThreadLocals rely on per-thread hash maps attached to each thread * (Thread.threadLocals and inheritableThreadLocals). The ThreadLocal * objects act as keys, searched via threadLocalHashCode. This is a * custom hash code (useful only within ThreadLocalMaps) that eliminates * collisions in the common case where consecutively constructed * ThreadLocals are used by the same threads, while remaining well-behaved * in less common cases. */ private final int threadLocalHashCode = nextHashCode(); /** * The next hash code to be given out. Accessed only by like-named method. */ private static int nextHashCode = 0; /** * The difference between successively generated hash codes - turns * implicit sequential thread-local IDs into near-optimally spread * multiplicative hash values for power-of-two-sized tables. */ private static final int HASH_INCREMENT = 0x61c88647; /** * Compute the next hash code. The static synchronization used here * should not be a performance bottleneck. When ThreadLocals are * generated in different threads at a fast enough rate to regularly * contend on this lock, memory contention is by far a more serious * problem than lock contention. */ private static synchronized int nextHashCode() &#123; int h = nextHashCode; nextHashCode = h + HASH_INCREMENT; return h; &#125; /** * Creates a thread local variable. */ public ThreadLocal() &#123; &#125; /** * Returns the value in the current thread's copy of this thread-local * variable. Creates and initializes the copy if this is the first time * the thread has called this method. * * @return the current thread's value of this thread-local */ public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) return (T)map.get(this); // Maps are constructed lazily. if the map for this thread // doesn't exist, create it, with this ThreadLocal and its // initial value as its only entry. T value = initialValue(); createMap(t, value); return value; &#125; /** * Sets the current thread's copy of this thread-local variable * to the specified value. Many applications will have no need for * this functionality, relying solely on the &#123;@link #initialValue&#125; * method to set the values of thread-locals. * * @param value the value to be stored in the current threads' copy of * this thread-local. */ public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); &#125; /** * Get the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @return the map */ ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals; &#125; /** * Create the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @param firstValue value for the initial entry of the map * @param map the map to store. */ void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue); &#125; ....... /** * ThreadLocalMap is a customized hash map suitable only for * maintaining thread local values. No operations are exported * outside of the ThreadLocal class. The class is package private to * allow declaration of fields in class Thread. To help deal with * very large and long-lived usages, the hash table entries use * WeakReferences for keys. However, since reference queues are not * used, stale entries are guaranteed to be removed only when * the table starts running out of space. */ static class ThreadLocalMap &#123; ........ &#125;&#125; 可以看到ThreadLocal类中的变量只有这3个int型： 123private final int threadLocalHashCode = nextHashCode(); private static int nextHashCode = 0; private static final int HASH_INCREMENT = 0x61c88647; 而作为ThreadLocal实例的变量只有 threadLocalHashCode 这一个，nextHashCode 和HASH_INCREMENT 是ThreadLocal类的静态变量，实际上HASH_INCREMENT是一个常量，表示了连续分配的两个ThreadLocal实例的threadLocalHashCode值的增量，而nextHashCode 的表示了即将分配的下一个ThreadLocal实例的threadLocalHashCode 的值。 可以来看一下创建一个ThreadLocal实例即new ThreadLocal()时做了哪些操作，从上面看到构造函数ThreadLocal()里什么操作都没有，唯一的操作是这句： 1private final int threadLocalHashCode = nextHashCode(); 那么nextHashCode()做了什么呢： 12345private static synchronized int nextHashCode() &#123; int h = nextHashCode; nextHashCode = h + HASH_INCREMENT; return h; &#125; 就是将ThreadLocal类的下一个hashCode值即nextHashCode的值赋给实例的threadLocalHashCode，然后nextHashCode的值增加HASH_INCREMENT这个值。 因此ThreadLocal实例的变量只有这个threadLocalHashCode，而且是final的，用来区分不同的ThreadLocal实例，ThreadLocal类主要是作为工具类来使用，那么ThreadLocal.set()进去的对象是放在哪儿的呢？ 看一下上面的set()方法，两句合并一下成为 1ThreadLocalMap map = Thread.currentThread().threadLocals; 这个ThreadLocalMap 类是ThreadLocal中定义的内部类，但是它的实例却用在Thread类中： 12345678public class Thread implements Runnable &#123; ...... /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; ......&#125; 再看这句：12if (map != null) map.set(this, value); 也就是将该ThreadLocal实例作为key，要保持的对象作为值，设置到当前线程的ThreadLocalMap 中，get()方法同样大家看了代码也就明白了，ThreadLocalMap 类的代码太多了，我就不帖了，自己去看源码吧。 贴个例子吧： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class Student &#123; private int age = 0; //年龄 public int getAge() &#123; return this.age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; public class ThreadLocalDemo implements Runnable &#123; //创建线程局部变量studentLocal，在后面你会发现用来保存Student对象 private final static ThreadLocal studentLocal = new ThreadLocal(); public static void main(String[] agrs) &#123; ThreadLocalDemo td = new ThreadLocalDemo(); Thread t1 = new Thread(td, "a"); Thread t2 = new Thread(td, "b"); t1.start(); t2.start(); &#125; public void run() &#123; accessStudent(); &#125; /** * 示例业务方法，用来测试 */ public void accessStudent() &#123; //获取当前线程的名字 String currentThreadName = Thread.currentThread().getName(); System.out.println(currentThreadName + " is running!"); //产生一个随机数并打印 Random random = new Random(); int age = random.nextInt(100); System.out.println("thread " + currentThreadName + " set age to:" + age); //获取一个Student对象，并将随机数年龄插入到对象属性中 Student student = getStudent(); student.setAge(age); System.out.println("thread " + currentThreadName + " first read age is:" + student.getAge()); try &#123; Thread.sleep(500); &#125; catch (InterruptedException ex) &#123; ex.printStackTrace(); &#125; System.out.println("thread " + currentThreadName + " second read age is:" + student.getAge()); &#125; protected Student getStudent() &#123; //获取本地线程变量并强制转换为Student类型 Student student = (Student) studentLocal.get(); //线程首次执行此方法的时候，studentLocal.get()肯定为null if (student == null) &#123; //创建一个Student对象，并保存到本地线程变量studentLocal中 student = new Student(); studentLocal.set(student); &#125; return student; &#125;&#125; 结果：12345678a is running! thread a set age to:76 b is running! thread b set age to:27 thread a first read age is:76 thread b first read age is:27 thread a second read age is:76 thread b second read age is:27 原文地址：http://www.iteye.com/topic/103804]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>threadlocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vi/vim 编辑器命令]]></title>
    <url>%2F2014%2F01%2F03%2Flinux-vi%2F</url>
    <content type="text"><![CDATA[vi/vim 用法笔记。 按 i 进入编辑模式 （一开始是命令模式的） 按 esc 进入命令模式 在命令模式中输入:wq 保存 （write and quit） 在命令模式中输入:q 退出 在命令模式中输入:q! 强制退出 (不保存) 在命令模式中输入/后接关键字搜索，n下一个匹配，N上一个匹配 在命令模式中输入gg 回到顶部，GG到底部 在命令模式中输入dd 删除当前行 在命令模式中输入x 删除后一个字符，X删除前一个字符 在命令模式中输入u undo最后一次修改 在命令模式中输入yy 复制当前行 在命令模式中输入p 粘贴到光标后面，P粘贴到光标前面 未完待续…]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vi</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2014%2F01%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to my blog, This is my first post.so hello world]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>hello</tag>
        <tag>world</tag>
      </tags>
  </entry>
</search>
