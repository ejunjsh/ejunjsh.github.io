<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[openstack安装准备(一)]]></title>
    <url>%2F2017%2F08%2F18%2Fopenstack-install-prepare-1%2F</url>
    <content type="text"><![CDATA[正在写。。。 准备VMware由于我是习惯了mac上做实验，所以用VMware fusion，随便下个破解版即可。 准备UbuntuUbuntu去官网下载16.04的服务器版本的ISO即可。 准备网络这次实验用到两台虚拟机： controller,computehostname:controller 接口 ip 模式 ens33 192.168.199.10 桥接 ens34 192.168.5.10 私有 ens35 192.168.112.10 nat hostname:compute 接口 ip 模式 ens33 192.168.199.11 桥接 ens34 192.168.5.11 私有 PS: 桥接模式是虚拟机可以更物理机所在网络共享一套网络，例如跟物理机同一个WiFi里面的设备都可以访问物理机里面的虚拟机。一般用来做管理节点的网络。 私有模式代表虚拟机只能跟物理机作为一个网络，其他设备访问不了，一般可以用来做内部网络 nat模式用来给虚拟机访问互联网用 上面网络配置好后，可以开搞了，至于怎么安装虚拟机和配置网络，可以搜索相关文章😈。]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack安装索引]]></title>
    <url>%2F2017%2F08%2F16%2Fopenstack-install-index%2F</url>
    <content type="text"><![CDATA[索引页 openstack安装准备(一)]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一张图了解三色标记法]]></title>
    <url>%2F2017%2F08%2F16%2Fgc-three-color%2F</url>
    <content type="text"><![CDATA[三色标记法是传统 Mark-Sweep 的一个改进，它是一个并发的 GC 算法。原理如下， 首先创建三个集合：白、灰、黑。 将所有对象放入白色集合中。 然后从根节点开始遍历所有对象（注意这里并不递归遍历），把遍历到的对象从白色集合放入灰色集合。 之后遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，之后将此灰色对象放入黑色集合 重复 4 直到灰色中无任何对象 通过write-barrier检测对象有变化，重复以上操作 收集所有白色对象（垃圾） 这个算法可以实现 “on-the-fly”，也就是在程序执行的同时进行收集，并不需要暂停整个程序。但是也会有一个缺陷，可能程序中的垃圾产生的速度会大于垃圾收集的速度，这样会导致程序中的垃圾越来越多无法被收集掉。]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>gc</tag>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一张图了解标记清除算法]]></title>
    <url>%2F2017%2F08%2F16%2Fgc-mark-sweep%2F</url>
    <content type="text"><![CDATA[这个算法分为两步，标记和清除。 标记：从程序的根节点开始， 递归地 遍历所有对象，将能遍历到的对象打上标记。 清除：讲所有未标记的的对象当作垃圾销毁。 但是这个算法也有一个缺陷，就是人们常常说的 STW 问题（Stop The World）。因为算法在标记时必须暂停整个程序，否则其他线程的代码可能会改变对象状态，从而可能把不应该回收的对象当做垃圾收集掉。当程序中的对象逐渐增多时，递归遍历整个对象树会消耗很多的时间，在大型程序中这个时间可能会是毫秒级别的。让所有的用户等待几百毫秒的 GC 时间这是不能容忍的。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postman的几种body的使用介绍]]></title>
    <url>%2F2017%2F08%2F10%2Fpostman%2F</url>
    <content type="text"><![CDATA[postman,用来模拟发送http请求的工具，里面涉及的请求body有以下几个类型，所以记下，而且也能理解http body的几种格式，分享之。。 form-data就是http请求中的multipart/form-data,它会将表单的数据处理为一条消息，以标签为单元，用分隔符分开。既可以上传键值对，也可以上传文件。当上传的字段是文件时，会有Content-Type来表名文件类型；content-disposition，用来说明字段的一些信息；由于有boundary隔离，所以multipart/form-data既可以上传文件，也可以上传键值对，它采用了键值对的方式，所以可以上传多个文件。 内容为12345678910111213141516171819202122232425POST /login HTTP/1.1Host: 10.170.56.67Content-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gWCache-Control: no-cachePostman-Token: 9843651a-5bf9-0544-03c1-fcc2a16f484b------WebKitFormBoundary7MA4YWxkTrZu0gWContent-Disposition: form-data; name=&quot;username&quot;admin------WebKitFormBoundary7MA4YWxkTrZu0gWContent-Disposition: form-data; name=&quot;password&quot;admin123------WebKitFormBoundary7MA4YWxkTrZu0gWContent-Disposition: form-data; name=&quot;abc&quot;; filename=&quot;&quot;Content-Type: ------WebKitFormBoundary7MA4YWxkTrZu0gWContent-Disposition: form-data; name=&quot;tttt&quot;; filename=&quot;&quot;Content-Type: ------WebKitFormBoundary7MA4YWxkTrZu0gW-- x-www-form-urlencoded就是application/x-www-from-urlencoded,会将表单内的数据转换为键值对，并以urlencode为格式内容为1234567POST /login HTTP/1.1Host: 10.170.56.67Content-Type: application/x-www-form-urlencodedCache-Control: no-cachePostman-Token: e6887900-a46e-2ff4-8232-de878b75f5fdusername=admin&amp;password=admin123 raw可以上传任意格式的文本，可以上传text、json、xml、html等内容为12345678910POST /login HTTP/1.1Host: 10.170.56.67Content-Type: application/jsonCache-Control: no-cachePostman-Token: 233df0e0-c6d9-98c7-4d7e-736329322683&#123; &quot;abc&quot;:&quot;cba&quot;, &quot;cba&quot;:&quot;abc&quot;&#125; 从图片和内容对比，可以发现，基本，粘什么，就发什么，不会进行任何转意。 binary相当于Content-Type:application/octet-stream,从字面意思得知，只可以上传二进制数据，通常用来上传文件，由于没有键值，所以，一次只能上传一个文件。 multipart/form-data与x-www-form-urlencoded区别 multipart/form-data：既可以上传文件等二进制数据，也可以上传表单键值对，只是最后会转化为一条信息； x-www-form-urlencoded：只能上传键值对，并且键值对都是间隔分开的]]></content>
      <categories>
        <category>http</category>
      </categories>
      <tags>
        <tag>postman</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go的是否需要用goroutine pool？]]></title>
    <url>%2F2017%2F08%2F03%2Fgo-worker-pool-if-need%2F</url>
    <content type="text"><![CDATA[这几天无聊，想到java有自己的线程池，是否对应go也有它的goroutine pool呢，所以搜了下，标准库没有，github有，但都大同小异，所以自己实现了一个。 一个简单的goroutine pool12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package workerpoolimport ( "sync" "fmt")type task func()type worker struct &#123; stopC chan bool&#125;type WorkerPool struct &#123; num int sync.Mutex taskQ chan task workers []*worker&#125;func NewWorkerPool(workerNum int,queueCap int) *WorkerPool&#123; return &amp;WorkerPool&#123;num:workerNum,taskQ:make(chan task,queueCap),workers:make([]*worker,workerNum)&#125;&#125;func (wp *WorkerPool) Execute(t task)&#123; wp.taskQ&lt;-t&#125;func (wp *WorkerPool) Start() *WorkerPool&#123; for i:=0;i&lt;wp.num;i++&#123; wp.workers[i]=&amp;worker&#123; make(chan bool)&#125; w:=wp.workers[i] go func(i int) &#123; for &#123; stop:=false select &#123; case f:=&lt;-wp.taskQ: f() case stop=&lt;-w.stopC: break &#125; if stop&#123; break &#125; &#125; fmt.Println("stop") &#125;(i) &#125; return wp&#125;func (wp *WorkerPool) Stop()&#123; for _,w:=range wp.workers&#123; w.stopC&lt;- true &#125;&#125; 代码很简单，就是NewWorkerPool一个池子的时候设置goroutine的数量和任务队列的大小。Start后就创建那么多goroutine去任务队列取任务执行，取不到任务就自循。Execute方法是把任务压进队列，如果队列满了就阻塞。 性能要测试性能，肯定要有对比，以下是没有使用pool:1234567891011121314func nopool() &#123; wg := new(sync.WaitGroup) //执行1000000次，每次都启动一个goroutine for i := 0; i &lt; 1000000; i++ &#123; wg.Add(1) go func(n int) &#123; for j := 0; j &lt; 100000; j++ &#123; &#125; defer wg.Done() &#125;(i) &#125; wg.Wait()&#125; 以下是简单版的只是单纯限制goroutine数量和任务队列的代码，没有任何封装的:12345678910111213141516171819202122232425262728func gopool() &#123; wg := new(sync.WaitGroup) //队列100 data := make(chan int, 100) //goroutine 数量10个 for i := 0; i &lt; 10; i++ &#123; wg.Add(1) go func(n int) &#123; defer wg.Done() for _ = range data &#123; func()&#123; for j := 0; j &lt; 100000; j++ &#123; &#125; &#125;() &#125; &#125;(i) &#125; //执行1000000个任务 for i := 0; i &lt; 1000000; i++ &#123; data &lt;- i &#125; close(data) wg.Wait()&#125; 然后是主角:123456789101112131415161718func workerpool() &#123; wg := new(sync.WaitGroup) //十个goroutine，队列容量100 wp:=NewWorkerPool(10,100) wp.Start() //提交1000000任务 for i := 0; i &lt; 1000000; i++ &#123; wg.Add(1) wp.Execute(func() &#123; for j := 0; j &lt; 100000; j++ &#123; &#125; wg.Done() &#125;) &#125; wg.Wait()&#125; 上面代码基本都是做同样一件事，但是后两个只开10个goroutine，第一个就开了1000000个，结果：123BenchmarkNopool-8 1 7966900091 ns/opBenchmarkGopool-8 1 7949844269 ns/opBenchmarkWorkerPool-8 1 7997732135 ns/op 可以看出来，没有区别，重新run几次基本没有多大变化。 总结由于go本身有对goroutine有调度，所以自己实现的池子来调度其实好像没有什么用。还有可能我自己能力实现不好，没发挥池子的作用😀。但是用更少的goroutine能完成同样的事情，应该是一种优化，而且这里的goroutine执行都是简单的循环，没有复杂的业务，一旦业务复杂，更少goroutine能够减少内存和goroutine切换时的cpu资源，有可能上面性能的比较会拉开。]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用树莓派搭建一个简易的NAS]]></title>
    <url>%2F2017%2F07%2F20%2Fraspberry-nas%2F</url>
    <content type="text"><![CDATA[准备 raspberry pi 3 硬盘（格式化过ext4的） 连接raspberry用的终端 安装samba12sudo apt-get updatesudo apt-get install samba samba-common-bin 配置samba12sudo cp /etc/samba/smb.conf /etc/samba/smb.conf.backsudo vim /etc/samba/smb.conf 1234567891011121314151617# 在末尾加入如下内容# 分享名称[MyNAS]# 说明信息comment = NAS Storage# 可以访问的用户valid users = pi,root# 共享文件的路径,raspberry pi 会自动将连接到其上的外接存储设备挂载到/media/pi/目录下。path = /media/pi/# 可被其他人看到资源名称（非内容）browseable = yes# 可写writable = yes# 新建文件的权限为 664create mask = 0664# 新建目录的权限为 775directory mask = 0775 可以把配置文件中你不需要的分享名称删除，例如 [homes], [printers] 等。测试配置文件是否有错误，根据提示做相应修改1testparm 添加登陆账户并创建密码，必须是 linux 已存在的用户1sudo smbpasswd -a pi 重启 samba 服务1sudo /etc/init.d/samba restart 连接一般树莓派跟你的WiFi相连的话，你的网络就能看到跟上面配置一样的分享名称，如mac上面这样的显示：如果显示没权限，可以断开连接，用你上面添加的账号登录。]]></content>
      <categories>
        <category>raspberrypi</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>raspberrypi</tag>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用树莓派实现一个能播放天气的闹钟]]></title>
    <url>%2F2017%2F07%2F18%2Fraspberry-weather-clock%2F</url>
    <content type="text"><![CDATA[前提你要有个pi😄 获取天气接口这里我是用图灵机器人来获取天气的接口，你可以自己上去注册一个，下面代码URL的Key是我注册的机器人给的。1234567891011def getWeatherText(): try: response = requests.get( "http://www.tuling123.com/openapi/api?key=652ae4a714794fe6b01faa990d7a981f&amp;info=%s" % "广州今日天气") json = response.json() if json["code"] == 100000: return json["text"] else: return "" except: return "" 播放文字利用百度的接口可以转换文本为语音。默认只有女声12345def text2voice(text): url = 'http://tts.baidu.com/text2audio?idx=1&amp;tex=&#123;0&#125;&amp;cuid=baidu_speech_' \ 'demo&amp;cod=2&amp;lan=zh&amp;ctp=1&amp;pdt=1&amp;spd=4&amp;per=4&amp;vol=5&amp;pit=5'.format(text) # 用mplayer播放语音 os.system('mplayer "%s"' % url) 安装播放媒体软件上面代码你看到的mplayer,就是用来播放语音的，传个url作为参数12sudo apt-get install mplayerusage: mplayer [url] 播放音乐有了上面这个神器，你可以给播报语音前后加一首音乐😄12def playMusic(path): os.system('mplayer %s' % path) 总结利用上面的东东，可以组合些好玩的东西了，至于闹钟的唤醒，可以crob job 做，也可以代码里面实现，enjoy…😄全部代码地址 https://github.com/ejunjsh/raspberrypi-code/blob/master/clock/weather.py]]></content>
      <categories>
        <category>raspberrypi</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>raspberrypi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用go实现一个简单的restful接口]]></title>
    <url>%2F2017%2F07%2F18%2Fgo-first-rest%2F</url>
    <content type="text"><![CDATA[前言go的标准库http已经封装好很多接口，可以很简单实现一个web服务器。12345678910111213// 定义 handlerfunc HelloServer(w http.ResponseWriter, req *http.Request) &#123; io.WriteString(w, "hello, world!\n")&#125;func main() &#123; //绑定pattern和handler http.HandleFunc("/hello", HelloServer) err := http.ListenAndServe(":12345", nil) if err != nil &#123; log.Fatal("ListenAndServe: ", err) &#125;&#125; 基于上面例子可以封装一个restful接口，不是难事。 实现从上面例子可以看到，一个url pattern对应一个handler，即对应一个处理，就可以处理http请求了，所以下面的实现是基于对这两个东西的封装开始 封装一个restful app 结构12345678910111213type App struct &#123; //一个map，key是pattern，value是handler handlers map[string]func(r *HttpRequest,w HttpResponse)error //pattern数组，用来保证加入pattern的顺序，因为上面的map是无顺序的 patterns []string //一个map，key是pattern，value是http method methods map[string]string //用来实现在url path取出参数的。 regexps map[string]*regexp.Regexp pathparamanmes map[string][]string //用来处理异常的handler errHandler func( err error, r *HttpRequest,w HttpResponse)&#125; 初始化函数12345678910111213func NewApp() *App &#123; return &amp;App&#123; handlers: make(map[string]func(r *HttpRequest,w HttpResponse)error), patterns:make([]string,0), methods:make(map[string]string), regexps:make(map[string]*regexp.Regexp), pathparamanmes:make(map[string][]string), //一个默认的异常处理，直接返回异常内容 errHandler: func(err error, r *HttpRequest, w HttpResponse) &#123; w.Write( []byte(err.Error())) &#125;, &#125;&#125; 映射绑定123456789101112131415161718192021222324252627282930313233343536func(a *App) handle(method string,pattern string, handler func(r *HttpRequest,w HttpResponse) error)&#123; //绑定pattern和handler a.handlers[pattern]=handler //绑定pattern和method a.methods[pattern]=method //绑定pattern 正则，用来匹配url pattern,和获取url path 参数 a.regexps[pattern],a.pathparamanmes[pattern]=convertPatterntoRegex(pattern) for _,s:=range a.patterns&#123; if s==pattern&#123; return &#125; &#125; //加入数组，方便用此数组确定顺序 a.patterns=append(a.patterns,pattern)&#125;//绑定GETfunc (a *App) Get(pattern string, handler func(r *HttpRequest,w HttpResponse)error) &#123; a.handle("GET",pattern,handler)&#125;//绑定POSTfunc (a *App) Post(pattern string, handler func(r *HttpRequest,w HttpResponse)error) &#123; a.handle("POST",pattern,handler)&#125;//绑定DELETEfunc (a *App) Delete(pattern string, handler func(r *HttpRequest,w HttpResponse)error) &#123; a.handle("DELETE",pattern,handler)&#125;//绑定PUTfunc (a *App) Put(pattern string, handler func(r *HttpRequest,w HttpResponse) error) &#123; a.handle("PUT",pattern,handler)&#125;func (a *App) Error(handler func(err error,r *HttpRequest,w HttpResponse)) &#123; a.errHandler=handler&#125; 有了Restful接口的四个方法映射绑定，剩下的就要请求能进到来，所以接下来要写个入口才行。 编写http入口12345678910111213141516171819//http 入口func(a *App) Run(address string) error&#123; fmt.Printf("Server listens on %s",address) err:=http.ListenAndServe(address,&amp;hodler&#123;app:a&#125;) if err!=nil&#123; return err &#125; return nil&#125;//https 入口func(a *App) RunTls(address string,cert string,key string) error&#123; fmt.Printf("Server listens on %s",address) err:=http.ListenAndServeTLS(address,cert,key,&amp;hodler&#123;app:a&#125;) if err!=nil&#123; return err &#125; return nil&#125; 入口函数主要调用http库来启动http服务，然后把请求处理函数作为ListenAndServe第二个参数传入。这里由holder来实现这个处理函数。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func (h *hodler) ServeHTTP(w http.ResponseWriter, r *http.Request)&#123; //封装一下，附加更多功能 request:= newHttpRequest(r) response:=newHttpResponse(w) //捕获panic，并让errhandler处理返回。 defer func() &#123; if err:=recover();err!=nil&#123; if e,ok:=err.(error);ok&#123; h.app.errHandler(InternalError&#123;e,""&#125;,request,response) &#125; if e,ok:=err.(string);ok&#123; h.app.errHandler(InternalError&#123;nil,e&#125;,request,response) &#125; &#125; &#125;() //根据pattern的添加顺序，循环判断 for _,p:=range h.app.patterns&#123; if reg,ok:= h.app.regexps[p];ok&#123; //匹配method if method,ok:=h.app.methods[p];ok&amp;&amp;r.Method==method&#123; //匹配pattern if reg.Match([]byte(r.URL.Path)) &#123; //抽取url path parameters matchers:=reg.FindSubmatch([]byte(r.URL.Path)) pathParamMap:=make(map[string]string) if len(matchers)&gt;1&#123; if pathParamNames,ok:=h.app.pathparamanmes[p];ok&#123; for i:=1;i&lt;len(matchers);i++&#123; pathParamMap[pathParamNames[i]]=string(matchers[i]) &#125; &#125; &#125; //PathParams是封装后的request独有的属性 request.PathParams=pathParamMap if handler,ok:=h.app.handlers[p];ok&#123; //执行handler err:=handler(request,response) if err!=nil&#123; //执行errhandler h.app.errHandler(err,request,response) &#125; return &#125; &#125; &#125; &#125; &#125; //执行no found errhandler h.app.errHandler(NoFoundError&#123;&#125;,request,response)&#125; 基本一个请求的流程如下：requset-&gt;ServeHTTP()-&gt;匹配url pattern-&gt;匹配method-&gt;匹配到你的handler-&gt;执行你的handler-&gt;你的handler返回结果 返回结果由于返回结果可以有很多，所以封装了http库的http.ResponseWriter来实现WriteString,WriteJson,WriteXml,WriteFile等方法。1234567891011121314151617181920212223242526272829//封装request，附件一个PathParams来保存url path parameters.type HttpRequest struct &#123; *http.Request PathParams map[string] string&#125;type HttpResponse struct &#123; http.ResponseWriter&#125;//用来返回字符func (response *HttpResponse) WriteString(str string) error &#123; ...&#125;//返回JSONfunc (response *HttpResponse) WriteJson(jsonObj interface&#123;&#125;) error &#123; ...&#125;//返回XMLfunc (response *HttpResponse) WriteXml(xmlObj interface&#123;&#125;) error &#123; ...&#125;//返回文件func (response *HttpResponse) WriteFile(filepath string) error &#123; ...&#125;//返回一个模板htmlfunc (response *HttpResponse) WriteTemplates(data interface&#123;&#125;,tplPath ...string) error &#123; ...&#125; 例子123456789101112131415161718192021222324func main()&#123; //new 一个restful接口 app:=gorest.NewApp() //绑定 app.Get("/json", func(r *gorest.HttpRequest, w gorest.HttpResponse) error &#123; a:= struct &#123; Abc string `json:"abc"` Cba string `json:"cba"` &#125;&#123;"123","321"&#125; //返回json作为结果 return w.WriteJson(a) &#125;) app.Error(func(err error, r *gorest.HttpRequest, w gorest.HttpResponse)&#123; if e,ok:=err.(gorest.NoFoundError);ok &#123; w.Write([]byte(e.Error())) &#125; if e,ok:=err.(gorest.InternalError);ok &#123; w.Write([]byte(e.Error())) &#125; &#125;) //启动 app.Run(":8081")&#125; 收工😄 总结go的标准库封装了很多了，所以实现这个其实还是比较轻松的😄详细代码见https://github.com/ejunjsh/gorest]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一张图了解一致性hash]]></title>
    <url>%2F2017%2F07%2F16%2Fconsistent-hash%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一张图了解hashmap]]></title>
    <url>%2F2017%2F07%2F15%2Fhashmap%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SELECT * ...... FOR UPDATE 锁机制]]></title>
    <url>%2F2016%2F12%2F19%2Fmysql-select-for-update%2F</url>
    <content type="text"><![CDATA[由于InnoDB预设是Row-Level Lock，InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！ 举个例子:假设有个表单products ，里面有id跟name二个栏位，id是主键。例1: (明确指定主键，并且有此笔资料，row lock)12SELECT * FROM products WHERE id='3' FOR UPDATE;SELECT * FROM products WHERE id='3' and type=1 FOR UPDATE; 例2: (明确指定主键，若查无此笔资料，无lock)1SELECT * FROM products WHERE id='-1' FOR UPDATE; 例3: (无主键，table lock)1SELECT * FROM products WHERE name='Mouse' FOR UPDATE; 例4: (主键不明确，table lock)1SELECT * FROM products WHERE id&lt;&gt;'3' FOR UPDATE; 例5: (主键不明确，table lock)1SELECT * FROM products WHERE id LIKE '3' FOR UPDATE; 注1: FOR UPDATE仅适用于InnoDB，且必须在交易区块(BEGIN/COMMIT)中才能生效。注2: 要测试锁定的状况，可以利用mysql的Command Mode ，开二个视窗来做测试。 在MySql 5.0中测试确实是这样的另外：MyAsim 只支持表级锁，InnerDB支持行级锁添加了(行级锁/表级锁)锁的数据不能被其它事务再锁定，也不被其它事务修改（修改、删除）是表级锁时，不管是否查询到记录，都会锁定表 此外，如果A与B都对表id进行查询但查询不到记录，则A与B在查询上不会进行row锁，但A与B都会获取排它锁，此时A再插入一条记录的话则会因为B已经有锁而处于等待中，此时B再插入一条同样的数据则会抛出Deadlock found when trying to get lock; try restarting transaction然后释放锁，此时A就获得了锁而插入成功 上面介绍过SELECT … FOR UPDATE 的用法，不过锁定(Lock)的数据是判别就得要注意一下了。由于InnoDB 预设是Row-Level Lock，所以只有「明确」的指定主键，MySQL 才会执行Row lock (只锁住被选取的数据) ，否则MySQL 将会执行Table Lock (将整个数据表单给锁住)。 转载 http://www.cnblogs.com/chenwenbiao/archive/2012/06/06/2537508.html]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单理解Java GC与幽灵引用]]></title>
    <url>%2F2016%2F09%2F11%2Fjava-gc-reference%2F</url>
    <content type="text"><![CDATA[Java中一共有4种类型的引用:StrongReference、SoftReference、WeakReference以及PhantomReference (幽灵引用), 这 4 种类型的引用与Java GC有着密切的关系, 让我们逐一来看它们的定义和使用场景。 Strong ReferenceStrongReference 是 Java 的默认引用实现,它会尽可能长时间的存活于 JVM 内， 当没有任何对象指向它时Java GC 执行后将会被回收12345678910111213141516171819@Test public void strongReference() &#123; Object referent = new Object(); /** * 通过赋值创建 StrongReference */ Object strongReference = referent; assertSame(referent, strongReference); referent = null; System.gc(); /** * StrongReference 在 GC 后不会被回收 */ assertNotNull(strongReference); &#125; WeakReference &amp; WeakHashMapWeakReference， 顾名思义,是一个弱引用,当所引用的对象在 JVM 内不再有强引用时, Java GC 后 weak reference 将会被自动回收123456789101112131415@Test public void weakReference() &#123; Object referent = new Object(); WeakReference&lt;Object&gt; weakRerference = new WeakReference&lt;Object&gt;(referent); assertSame(referent, weakRerference.get()); referent = null; System.gc(); /** * 一旦没有指向 referent 的强引用, weak reference 在 GC 后会被自动回收 */ assertNull(weakRerference.get()); &#125; WeakHashMap 使用 WeakReference 作为 key， 一旦没有指向 key 的强引用, WeakHashMap 在Java GC 后将自动删除相关的 entry12345678910111213141516171819202122@Test public void weakHashMap() throws InterruptedException &#123; Map&lt;Object, Object&gt; weakHashMap = new WeakHashMap&lt;Object, Object&gt;(); Object key = new Object(); Object value = new Object(); weakHashMap.put(key, value); assertTrue(weakHashMap.containsValue(value)); key = null; System.gc(); /** * 等待无效 entries 进入 ReferenceQueue 以便下一次调用 getTable 时被清理 */ Thread.sleep(1000); /** * 一旦没有指向 key 的强引用, WeakHashMap 在 GC 后将自动删除相关的 entry */ assertFalse(weakHashMap.containsValue(value)); &#125; SoftReferenceSoftReference 于 WeakReference 的特性基本一致， 最大的区别在于 SoftReference 会尽可能长的保留引用直到 JVM 内存不足时才会被回收(虚拟机保证), 这一特性使得 SoftReference 非常适合缓存应用123456789101112131415@Test public void softReference() &#123; Object referent = new Object(); SoftReference&lt;Object&gt; softRerference = new SoftReference&lt;Object&gt;(referent); assertNotNull(softRerference.get()); referent = null; System.gc(); /** *soft references 只有在 jvm OutOfMemory 之前才会被回收, 所以它非常适合缓存应用 */ assertNotNull(softRerference.get()); &#125; Phantom Reference作为本文主角， Phantom Reference(幽灵引用) 与 WeakReference 和 SoftReference 有很大的不同,因为它的 get() 方法永远返回 null, 这也正是它名字的由来12345678910@Test public void phantomReferenceAlwaysNull() &#123; Object referent = new Object(); PhantomReference&lt;Object&gt; phantomReference = new PhantomReference&lt;Object&gt;(referent, new ReferenceQueue&lt;Object&gt;()); /** * phantom reference 的 get 方法永远返回 null */ assertNull(phantomReference.get()); &#125; 诸位可能要问, 一个永远返回 null 的 reference 要来何用,请注意构造 PhantomReference 时的第二个参数 ReferenceQueue(事实上 WeakReference &amp; SoftReference 也可以有这个参数)，PhantomReference 唯一的用处就是跟踪 referent何时被 enqueue 到 ReferenceQueue 中. RererenceQueue当一个 WeakReference 开始返回 null 时， 它所指向的对象已经准备被回收， 这时可以做一些合适的清理工作. 将一个 ReferenceQueue 传给一个 Reference 的构造函数， 当对象被回收时， 虚拟机会自动将这个对象插入到 ReferenceQueue 中， WeakHashMap 就是利用 ReferenceQueue 来清除 key 已经没有强引用的 entries.1234567891011121314151617@Test public void referenceQueue() throws InterruptedException &#123; Object referent = new Object(); ReferenceQueue&lt;Object&gt; referenceQueue = new ReferenceQueue&lt;Object&gt;(); WeakReference&lt;Object&gt; weakReference = new WeakReference&lt;Object&gt;(referent, referenceQueue); assertFalse(weakReference.isEnqueued()); Reference&lt;? extends Object&gt; polled = referenceQueue.poll(); assertNull(polled); referent = null; System.gc(); assertTrue(weakReference.isEnqueued()); Reference&lt;? extends Object&gt; removed = referenceQueue.remove(); assertNotNull(removed); &#125; Phantom Reference vs Weak ReferencePhantomReference有两个好处， 其一， 它可以让我们准确地知道对象何时被从内存中删除， 这个特性可以被用于一些特殊的需求中(例如 Distributed GC，XWork 和 google-guice 中也使用 PhantomReference 做了一些清理性工作). 其二， 它可以避免 finalization 带来的一些根本性问题, 上文提到 PhantomReference 的唯一作用就是跟踪 referent 何时被 enqueue 到 ReferenceQueue 中,但是 WeakReference 也有对应的功能, 两者的区别到底在哪呢 ?这就要说到 Object 的 finalize 方法, 此方法将在 gc 执行前被调用, 如果某个对象重载了 finalize 方法并故意在方法内创建本身的强引用,这将导致这一轮的 GC 无法回收这个对象并有可能引起任意次 GC， 最后的结果就是明明 JVM 内有很多 Garbage 却 OutOfMemory， 使用 PhantomReference 就可以避免这个问题， 因为 PhantomReference 是在 finalize 方法执行后回收的，也就意味着此时已经不可能拿到原来的引用,也就不会出现上述问题,当然这是一个很极端的例子, 一般不会出现. 对比Soft vs Weak vs Phantom References Type Purpose Use When GCed Implementing Class Strong Reference An ordinary reference. Keeps objects alive as long as they are referenced. normal reference. Any object not pointed to can be reclaimed. default Soft Reference Keeps objects alive provided there’s enough memory. to keep objects alive even after clients have removed their references (memory-sensitive caches), in case clients start asking for them again by key. After a first gc pass, the JVM decides it still needs to reclaim more space. java.lang.ref.SoftReference Weak Reference Keeps objects alive only while they’re in use (reachable) by clients. Containers that automatically delete objects no longer in use. After gc determines the object is only weakly reachable java.lang.ref.WeakReference java.util.WeakHashMap Phantom Reference Lets you clean up after finalization but before the space is reclaimed (replaces or augments the use offinalize()) Special clean up processing After finalization. java.lang.ref.PhantomReference Java GC小结一般的应用程序不会涉及到 Reference 编程， 但是了解这些知识会对理解Java GC 的工作原理以及性能调优有一定帮助, 在实现一些基础性设施比如缓存时也可能会用到， 希望本文能有所帮助.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis的事务和watch]]></title>
    <url>%2F2016%2F09%2F03%2Fredis-transaction-watch%2F</url>
    <content type="text"><![CDATA[redis的事务严格意义来讲,redis的事务和我们理解的传统数据库(如mysql)的事务是不一样的。 redis中的事务定义Redis中的事务（transaction）是一组命令的集合。 事务同命令一样都是Redis的最小执行单位，一个事务中的命令要么都执行，要么都不执行。事务的原理是先将属于一个事务的命令发送给Redis，然后再让Redis依次执行这些命令。 Redis保证一个事务中的所有命令要么都执行，要么都不执行。如果在发送EXEC命令前客户端断线了，则Redis会清空事务队列，事务中的所有命令都不会执行。而一旦客户端发送了EXEC命令，所有的命令就都会被执行，即使此后客户端断线也没关系，因为Redis中已经记录了所有要执行的命令。 除此之外，Redis的事务还能保证一个事务内的命令依次执行而不被其他命令插入。试想客户端A需要执行几条命令，同时客户端B发送了一条命令，如果不使用事务，则客户端B的命令可能会插入到客户端A的几条命令中执行。如果不希望发生这种情况，也可以使用事务。 事务的应用 事务的应用非常普遍，如银行转账过程中A给B汇款，首先系统从A的账户中将钱划走，然后向B的账户增加相应的金额。这两个步骤必须属于同一个事务，要么全执行，要么全不执行。否则只执行第一步，钱就凭空消失了，这显然让人无法接受。 和传统的事务不同 和传统的mysql事务不同的事，即使我们的加钱操作失败,我们也无法在这一组命令中让整个状态回滚到操作之前 事务的错误处理如果一个事务中的某个命令执行出错，Redis会怎样处理呢？要回答这个问题，首先需要知道什么原因会导致命令执行出错。 语法错误语法错误指命令不存在或者命令参数的个数不对。比如：12345678910redis＞MULTIOKredis＞SET key valueQUEUEDredis＞SET key(error)ERR wrong number of arguments for 'set' commandredis＞ errorCOMMAND key(error) ERR unknown command 'errorCOMMAND'redis＞ EXEC(error) EXECABORT Transaction discarded because of previous errors. 跟在MULTI命令后执行了3个命令：一个是正确的命令，成功地加入事务队列；其余两个命令都有语法错误。而只要有一个命令有语法错误，执行EXEC命令后Redis就会直接返回错误，连语法正确的命令也不会执行。 这里需要注意一点：Redis 2.6.5之前的版本会忽略有语法错误的命令，然后执行事务中其他语法正确的命令。就此例而言，SET key value会被执行，EXEC命令会返回一个结果：1) OK。 运行错误运行错误指在命令执行时出现的错误，比如使用散列类型的命令操作集合类型的键，这种错误在实际执行之前Redis是无法发现的，所以在事务里这样的命令是会被Redis接受并执行的。如果事务里的一条命令出现了运行错误，事务里其他的命令依然会继续执行（包括出错命令之后的命令），示例如下：1234567891011121314redis＞MULTIOKredis＞SET key 1QUEUEDredis＞SADD key 2QUEUEDredis＞SET key 3QUEUEDredis＞EXEC1) OK2) (error) ERR Operation against a key holding the wrong kind of value3) OKredis＞GET key"3" 可见虽然SADD key 2出现了错误，但是SET key 3依然执行了。 Redis的事务没有关系数据库事务提供的回滚（rollback）功能。为此开发者必须在事务执行出错后自己收拾剩下的摊子（将数据库复原回事务执行前的状态等,这里我们一般采取日志记录然后业务补偿的方式来处理，但是一般情况下，在redis做的操作不应该有这种强一致性要求的需求，我们认为这种需求为不合理的设计）。 Watch命令大家可能知道redis提供了基于incr命令来操作一个整数型数值的原子递增，那么我们假设如果redis没有这个incr命令，我们该怎么实现这个incr的操作呢？ 那么我们下面的正主watch就要上场了。 如何使用watch命令正常情况下我们想要对一个整形数值做修改是这么做的(伪代码实现)：123val = GET mykeyval = val + 1SET mykey $val 但是上述的代码会出现一个问题,因为上面吧正常的一个incr(原子递增操作)分为了两部分,那么在多线程(分布式)环境中，这个操作就有可能不再具有原子性了。 研究过java的juc包的人应该都知道cas，那么redis也提供了这样的一个机制，就是利用watch命令来实现的。 watch命令描述 WATCH命令可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行。监控一直持续到EXEC命令（事务中的命令是在EXEC之后才执行的，所以在MULTI命令后可以修改WATCH监控的键值） 利用watch实现incr具体做法如下:123456WATCH mykeyval = GET mykeyval = val + 1MULTISET mykey $valEXEC 和此前代码不同的是，新代码在获取mykey的值之前先通过WATCH命令监控了该键，此后又将set命令包围在事务中，这样就可以有效的保证每个连接在执行EXEC之前，如果当前连接获取的mykey的值被其它连接的客户端修改，那么当前连接的EXEC命令将执行失败。这样调用者在判断返回值后就可以获悉val是否被重新设置成功。 注意点由于WATCH命令的作用只是当被监控的键值被修改后阻止之后一个事务的执行，而不能保证其他客户端不修改这一键值，所以在一般的情况下我们需要在EXEC执行失败后重新执行整个函数。 执行EXEC命令后会取消对所有键的监控，如果不想执行事务中的命令也可以使用UNWATCH命令来取消监控。 实现一个hsetNX函数我们实现的hsetNX这个功能是：仅当字段存在时才赋值。 为了避免竞态条件我们使用watch和事务来完成这一功能（伪代码）：123456789WATCH key isFieldExists = HEXISTS key, field if isFieldExists is 1 MULTI HSET key, field, value EXEC else UNWATCH return isFieldExists 在代码中会判断要赋值的字段是否存在，如果字段不存在的话就不执行事务中的命令，但需要使用UNWATCH命令来保证下一个事务的执行不会受到影响。 原文地址 http://www.jianshu.com/p/361cb9cd13d5]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程(二) 同步和锁]]></title>
    <url>%2F2016%2F08%2F20%2Fjava-thread-2-md%2F</url>
    <content type="text"><![CDATA[记录，汇总 线程同步问题的产生什么是线程同步问题，我们先来看一段卖票系统的代码，然后再分析这个问题：1234567891011121314151617181920212223242526package com.sky.code.thread;public class TicketSeller implements Runnable &#123; private int num = 100; public void run() &#123; while(true) &#123; if(num&gt;0) &#123; try&#123; Thread.sleep(10); &#125;catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //输出卖票信息 System.out.println(Thread.currentThread().getName()+".....sale...."+num--); &#125; else &#123; break; &#125; &#125; &#125;&#125; 上面是卖票线程类，下来再来看看执行类：1234567891011121314151617181920package com.sky.code.thread;public class TickeDemo &#123; public static void main(String[] args) &#123; TicketSeller t = new TicketSeller();//创建一个线程任务对象。 //创建4个线程同时卖票 Thread t1 = new Thread(t); Thread t2 = new Thread(t); Thread t3 = new Thread(t); Thread t4 = new Thread(t); //启动线程 t1.start(); t2.start(); t3.start(); t4.start(); &#125;&#125; 运行程序结果如下（仅截取部分数据）：从运行结果，我们就可以看出我们3个售票窗口同时卖出了96号票，这显然是不合逻辑的，其实这个问题就是我们前面所说的线程同步问题。不同的线程都对同一个数据进了操作这就容易导致数据错乱的问题，也就是线程不同步。那么这个问题该怎么解决呢？在给出解决思路之前我们先来分析一下这个问题是怎么产生的？我们声明一个线程类TicketSeller，在这个类中我们又声明了一个成员变量num也就是票的数量，然后我们通过run方法不断的去获取票数并输出，最后我们在外部类TicketDemo中创建了四个线程同时操作这个数据，运行后就出现我们刚才所说的线程同步问题，从这里我们可以看出产生线程同步(线程安全)问题的条件有两个：1.多个线程在操作共享的数据（num），2.操作共享数据的线程代码有多条（4条线程）；既然原因知道了，那该怎么解决？ 解决思路：将多条操作共享数据的线程代码封装起来，当有线程在执行这些代码的时候，其他线程时不可以参与运算的。必须要当前线程把这些代码都执行完毕后，其他线程才可以参与运算。 好了，思路知道了，我们就用java代码的方式来解决这个问题。 解决线程同步的两种典型方案在java中有两种机制可以防止线程安全的发生，Java语言提供了一个synchronized关键字来解决这问题，同时在Java SE5.0引入了Lock锁对象的相关类，接下来我们分别介绍这两种方法 通过锁（Lock）对象的方式解决线程安全问题在给出解决代码前我们先来介绍一个知识点：Lock，锁对象。在java中锁是用来控制多个线程访问共享资源的方式，一般来说，一个锁能够防止多个线程同时访问共享资源（但有的锁可以允许多个线程并发访问共享资源，比如读写锁，后面我们会分析）。在Lock接口出现之前，java程序是靠synchronized关键字（后面分析）实现锁功能的，而JAVA SE5.0之后并发包中新增了Lock接口用来实现锁的功能，它提供了与synchronized关键字类似的同步功能，只是在使用时需要显式地获取和释放锁，缺点就是缺少像synchronized那样隐式获取释放锁的便捷性，但是却拥有了锁获取与释放的可操作性，可中断的获取锁以及超时获取锁等多种synchronized关键字所不具备的同步特性。接下来我们就来介绍Lock接口的主要API方便我们学习 方法 相关描述内容 void lock() 获取锁，调用该方法当前线程会获取锁，当获取锁后。从该方法返回 void lockInterruptibly() throws InterruptedException 可中断获取锁和lock()方法不同的是该方法会响应中断，即在获取锁中可以中断当前线程。例如某个线程在等待一个锁的控制权的这段时间需要中断。 boolean tryLock() 尝试非阻塞获取锁，调用该方法后立即返回，如果能够获取锁则返回true，否则返回false。 boolean tryLock(long time,TimeUnit unit) throws InterruptedException 超时获取锁，当前线程在以下3种情况返回：1.当前线程在超时时间内获取了锁2.当前线程在超时时间被中断3.当前线程超时时间结束，返回false void unlock() 释放锁 Condition newCondition() 条件对象，获取等待通知组件。该组件和当前的锁绑定，当前线程只有获取了锁，才能调用该组件的await()方法，而调用后，当前线程将释放锁。 这里先介绍一下API，接下来我们将结合Lock接口的实现子类ReentrantLock来讲解下他的几个方法。 ReentrantLock（重入锁)重入锁，顾名思义就是支持重新进入的锁，它表示该锁能够支持一个线程对资源的重复加锁，也就是说在调用lock()方法时，已经获取到锁的线程，能够再次调用lock()方法获取锁而不被阻塞，同时还支持获取锁的公平性和非公平性。这里的公平是在绝对时间上，先对锁进行获取的请求一定先被满足，那么这个锁是公平锁，反之，是不公平的(但是如果不是需要，建议不要用公平锁，因为会造成一些资源的没必要等待，浪费性能)。那么该如何使用呢？看范例代码：1.同步执行的代码跟synchronized类似功能：123456789ReentrantLock lock = new ReentrantLock(); //参数默认false，不公平锁 ReentrantLock lock = new ReentrantLock(true); //公平锁 lock.lock(); //如果被其它资源锁定，会在此等待锁释放，达到暂停的效果 try &#123; //操作 &#125; finally &#123; lock.unlock(); //释放锁 &#125; 2.防止重复执行代码：12345678ReentrantLock lock = new ReentrantLock(); if (lock.tryLock()) &#123; //如果已经被lock，则立即返回false不会等待，达到忽略操作的效果 try &#123; //操作 &#125; finally &#123; lock.unlock(); &#125; &#125; 3.尝试等待执行的代码：12345678910111213ReentrantLock lock = new ReentrantLock(true); //公平锁 try &#123; if (lock.tryLock(5, TimeUnit.SECONDS)) &#123; //如果已经被lock，尝试等待5s，看是否可以获得锁，如果5s后仍然无法获得锁则返回false继续执行 try &#123; //操作 &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); //当前线程被中断时(interrupt)，会抛InterruptedException &#125; 这里有点需要特别注意的，把解锁操作放在finally代码块内这个十分重要。如果在临界区的代码抛出异常，锁必须被释放。否则，其他线程将永远阻塞。好了，ReentrantLock我们就简单介绍到这里，接下来我们通过ReentrantLock来解决前面卖票线程的线程同步（安全）问题，代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041package com.sky.code.thread;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class TicketSellerWithLock implements Runnable &#123; //创建锁对象 private Lock ticketLock = new ReentrantLock(); //创建锁对象(公平锁) //private Lock ticketLock = new ReentrantLock(true); private int num = 100; public void run() &#123; while(true) &#123; ticketLock.lock();//获取锁 if(num&gt;0) &#123; try&#123; Thread.sleep(10); //输出卖票信息 System.out.println(Thread.currentThread().getName()+".....sale...."+num--); &#125;catch (InterruptedException e) &#123; Thread.currentThread().interrupt();//继续中断异常 &#125;finally &#123; ticketLock.unlock();//释放锁 &#125; &#125; else &#123; ticketLock.unlock();//释放锁 break; &#125; &#125; &#125;&#125; TicketDemo类无需变化，线程安全问题就此解决。但是还是要说一下公平锁的问题，上面例子，不开公平锁的结果如下：开公平锁的结果如下：你会发现不开公平锁，cpu钟爱用第一个线程做事情，而开了公平锁后，基本是各个线程交替执行。上面提到公平锁是会消耗性能的，如果CPU调度的时候选择的不是公平调度的那个线程，CPU会放弃本次调度，干别的事情，如果老是调度不到的话，是浪费CPU调度的。 通过synchronied关键字的方式解决线程安全问题在Java中内置了语言级的同步原语－synchronized，这个可以大大简化了Java中多线程同步的使用。从JAVA SE1.0开始，java中的每一个对象都有一个内部锁，如果一个方法使用synchronized关键字进行声明，那么这个对象将保护整个方法，也就是说调用该方法线程必须获得内部的对象锁。123public synchronized void method&#123; //method body &#125; 等价于123456789private Lock ticketLock = new ReentrantLock(); public void method&#123; ticketLock.lock(); try&#123; //....... &#125;finally&#123; ticketLock.unlock(); &#125; &#125; 从这里可以看出使用synchronized关键字来编写代码要简洁得多了。当然，要理解这一代码，我们必须知道每个对象有一个内部锁，并且该锁有一个内部条件。由锁来管理那些试图进入synchronized方法的线程，由条件来管那些调用wait的线程(wait()/notifyAll/notify())。同时我们必须明白一旦有一个线程通过synchronied方法获取到内部锁，该类的所有synchronied方法或者代码块都无法被其他线程访问直到当前线程释放了内部锁。刚才上面说的是同步方法，synchronized还有一种同步代码块的实现方式：1234Object obj = new Object(); synchronized(obj)&#123; //需要同步的代码 &#125; 其中obj是对象锁，可以是任意对象。那么我们就通过其中的一个方法来解决售票系统的线程同步问题：1234567891011121314151617181920class Ticket implements Runnable &#123; private int num = 100; Object obj = new Object(); public void run() &#123; while(true) &#123; synchronized(obj) &#123; if(num&gt;0) &#123; try&#123;Thread.sleep(10);&#125;catch (InterruptedException e)&#123;&#125; System.out.println(Thread.currentThread().getName()+".....sale...."+num--); &#125; &#125; &#125; &#125; &#125; 嗯，同步代码块解决，运行结果也正常。到此同步问题也就解决了，当然代码同步也是要牺牲效率为前提的：同步的好处：解决了线程的安全问题。同步的弊端：相对降低了效率，因为同步外的线程的都会判断同步锁。同步的前提：同步中必须有多个线程并使用同一个锁。 线程间的通信机制线程开始运行，拥有自己的栈空间，但是如果每个运行中的线程，如果仅仅是孤立地运行，那么没有一点儿价值，或者是价值很小，如果多线程能够相互配合完成工作的话，这将带来巨大的价值，这也就是线程间的通信啦。在java中多线程间的通信使用的是等待/通知机制来实现的。 synchronied关键字等待/通知机制是指一个线程A调用了对象O的wait()方法进入等待状态，而另一个线程B调用了对象O的notify()或者notifyAll()方法，线程A收到通知后从对象O的wait()方法返回，进而执行后续操作。上述的两个线程通过对象O来完成交互，而对象上的wait()和notify()/notifyAll()的关系就如同开关信号一样，用来完成等待方和通知方之间的交互工作。等待/通知机制主要是用到的函数方法是notify()/notifyAll(),wait()/wait(long),wait(long,int),这些方法在上一篇文章都有说明过，这里就不重复了。当然这是针对synchronied关键字修饰的函数或代码块，因为要使用notify()/notifyAll(),wait()/wait(long),wait(long,int)这些方法的前提是对调用对象加锁，也就是说只能在同步函数或者同步代码块中使用。 条件对象的等待/通知机制所谓的条件对象也就是配合前面我们分析的Lock锁对象，通过锁对象的条件对象来实现等待/通知机制。那么条件对象是怎么创建的呢？12//创建条件对象 Condition conditionObj=ticketLock.newCondition(); 就这样我们创建了一个条件对象。注意这里返回的对象是与该锁（ticketLock）相关的条件对象。下面是条件对象的API： 方法 函数方法对应的描述 void await() 将该线程放到条件等待池中（对应wait()方法） void signalAll() 解除该条件等待池中所有线程的阻塞状态（对应notifyAll()方法） void signal() 从该条件的等待池中随机地选择一个线程，解除其阻塞状态（对应notify()方法） 上述方法的过程分析：一个线程A调用了条件对象的await()方法进入等待状态，而另一个线程B调用了条件对象的signal()或者signalAll()方法，线程A收到通知后从条件对象的await()方法返回，进而执行后续操作。上述的两个线程通过条件对象来完成交互，而对象上的await()和signal()/signalAll()的关系就如同开关信号一样，用来完成等待方和通知方之间的交互工作。当然这样的操作都是必须基于条件对象的锁的，当前线程只有获取了锁，才能调用该条件对象的await()方法，而调用后，当前线程将释放锁。 这里有点要特别注意的是，上述两种等待/通知机制中，无论是调用了signal()/signalAll()方法还是调用了notify()/notifyAll()方法并不会立即激活一个等待线程。它们仅仅都只是解除等待线程的阻塞状态，以便这些线程可以在当前线程解锁或者退出同步方法后，通过争夺CPU执行权实现对对象的访问。到此，线程通信机制的概念分析完，我们下面通过生产者消费者模式来实现等待/通知机制。 生产者消费者模式单生产者单消费者模式顾名思义，就是一个线程消费，一个线程生产。我们先来看看等待/通知机制下的生产者消费者模式：我们假设这样一个场景，我们是卖北京烤鸭店铺，我们现在只有一条生产线也只有一条消费线，也就是说只能生产线程生产完了，再通知消费线程才能去卖，如果消费线程没烤鸭了，就必须通知生产线程去生产，此时消费线程进入等待状态。在这样的场景下，我们不仅要保证共享数据（烤鸭数量）的线程安全，而且还要保证烤鸭数量在消费之前必须有烤鸭。下面我们通过java代码来实现：北京烤鸭生产资源类KaoYaResource：1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.sky.code.thread;public class KaoYaResource &#123; private String name; private int count = 1;//烤鸭的初始数量 private boolean flag = false;//判断是否有需要线程等待的标志 /** * 生产烤鸭 */ public synchronized void product(String name)&#123; if(flag)&#123; //此时有烤鸭，等待 try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace() ; &#125; &#125; this.name=name+count;//设置烤鸭的名称 count++; System.out.println(Thread.currentThread().getName()+"...生产者..."+this.name); flag=true;//有烤鸭后改变标志 notifyAll();//通知消费线程可以消费了 &#125; /** * 消费烤鸭 */ public synchronized void consume()&#123; if(!flag)&#123;//如果没有烤鸭就等待 try&#123; this.wait(); &#125;catch(InterruptedException e)&#123; &#125; &#125; System.out.println(Thread.currentThread().getName()+"...消费者........"+this.name);//消费烤鸭1 flag = false; notifyAll();//通知生产者生产烤鸭 &#125;&#125; 在这个类中我们有两个synchronized的同步方法，一个是生产烤鸭的，一个是消费烤鸭的，之所以需要同步是因为我们操作了共享数据count，同时为了保证生产烤鸭后才能消费也就是生产一只烤鸭后才能消费一只烤鸭，我们使用了等待/通知机制，wait()和notify()。当第一次运行生产现场时调用生产的方法，此时有一只烤鸭，即flag=false，无需等待，因此我们设置可消费的烤鸭名称然后改变flag=true，同时通知消费线程可以消费烤鸭了，即使此时生产线程再次抢到执行权，因为flag=true，所以生产线程会进入等待阻塞状态，消费线程被唤醒后就进入消费方法，消费完成后，又改变标志flag=false，通知生产线程可以生产烤鸭了………以此循环。生产消费执行类Single_Producer_Consumer.java:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.sky.code.thread;public class Single_Producer_Consumer &#123; public static void main(String[] args) &#123; KaoYaResource r = new KaoYaResource(); Producer pro = new Producer(r); Consumer con = new Consumer(r); //生产者线程 Thread t0 = new Thread(pro); //消费者线程 Thread t2 = new Thread(con); //启动线程 t0.start(); t2.start(); &#125;&#125;class Producer implements Runnable&#123; private KaoYaResource r; Producer(KaoYaResource r) &#123; this.r = r; &#125; public void run() &#123; while(true) &#123; r.product("北京烤鸭"); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;class Consumer implements Runnable&#123; private KaoYaResource r; Consumer(KaoYaResource r) &#123; this.r = r; &#125; public void run() &#123; while(true) &#123; r.consume(); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 在这个类中我们创建两个线程，一个是消费者线程，一个是生产者线程，我们分别开启这两个线程用于不断的生产消费，运行结果如下：12345678910111213141516171819hread-0...生产者...北京烤鸭1Thread-1...消费者........北京烤鸭1Thread-0...生产者...北京烤鸭2Thread-1...消费者........北京烤鸭2Thread-0...生产者...北京烤鸭3Thread-1...消费者........北京烤鸭3Thread-0...生产者...北京烤鸭4Thread-1...消费者........北京烤鸭4Thread-0...生产者...北京烤鸭5Thread-1...消费者........北京烤鸭5Thread-0...生产者...北京烤鸭6Thread-1...消费者........北京烤鸭6Thread-0...生产者...北京烤鸭7Thread-1...消费者........北京烤鸭7Thread-0...生产者...北京烤鸭8Thread-1...消费者........北京烤鸭8Thread-0...生产者...北京烤鸭9Thread-1...消费者........北京烤鸭9..... 很显然的情况就是生产一只烤鸭然后就消费一只烤鸭。运行情况完全正常，嗯，这就是单生产者单消费者模式。上面使用的是synchronized关键字的方式实现的，那么接下来我们使用对象锁的方式实现：KaoYaResourceByLock.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.sky.code.thread;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class KaoYaResourceByLock &#123; private String name; private int count = 1;//烤鸭的初始数量 private boolean flag = false;//判断是否有需要线程等待的标志 //创建一个锁对象 private Lock resourceLock=new ReentrantLock(); //创建条件对象 private Condition condition= resourceLock.newCondition(); /** * 生产烤鸭 */ public void product(String name)&#123; resourceLock.lock();//先获取锁 try&#123; if(flag)&#123; try &#123; condition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; this.name=name+count;//设置烤鸭的名称 count++; System.out.println(Thread.currentThread().getName()+"...生产者..."+this.name); flag=true;//有烤鸭后改变标志 condition.signalAll();//通知消费线程可以消费了 &#125;finally&#123; resourceLock.unlock(); &#125; &#125; /** * 消费烤鸭 */ public void consume()&#123; resourceLock.lock(); try&#123; if(!flag)&#123;//如果没有烤鸭就等待 try&#123; condition.await(); &#125;catch(InterruptedException e)&#123; &#125; &#125; System.out.println(Thread.currentThread().getName()+"...消费者........"+this.name);//消费烤鸭1 flag = false; condition.signalAll();//通知生产者生产烤鸭 &#125;finally&#123; resourceLock.unlock(); &#125; &#125;&#125; 代码变化不大，我们通过对象锁的方式去实现，首先要创建一个对象锁，我们这里使用的重入锁ReestrantLock类，然后通过手动设置lock()和unlock()的方式去获取锁以及释放锁。为了实现等待/通知机制，我们还必须通过锁对象去创建一个条件对象Condition，然后通过锁对象的await()和signalAll()方法去实现等待以及通知操作。Single_Producer_Consumer.java代码替换一下资源类即可,运行结果一样。 多生产者多消费者模式分析完了单生产者单消费者模式，我们再来聊聊多生产者多消费者模式，也就是多条生产线程配合多条消费线程。既然这样的话我们先把上面的代码Single_Producer_Consumer.java类修改成新类，大部分代码不变，仅新增2条线程去跑，一条t1的生产 共享资源类KaoYaResource不作更改，代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.sky.code.thread;public class Mutil_Producer_Consumer &#123; public static void main(String[] args) &#123; KaoYaResource r = new KaoYaResource(); Mutil_Producer pro = new Mutil_Producer(r); Mutil_Consumer con = new Mutil_Consumer(r); //生产者线程 Thread t0 = new Thread(pro); Thread t1 = new Thread(pro); //消费者线程 Thread t2 = new Thread(con); Thread t3 = new Thread(con); //启动线程 t0.start(); t1.start(); t2.start(); t3.start(); &#125;&#125;class Mutil_Producer implements Runnable&#123; private KaoYaResource r; Mutil_Producer(KaoYaResource r) &#123; this.r = r; &#125; public void run() &#123; while(true) &#123; r.product("北京烤鸭"); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;class Mutil_Consumer implements Runnable&#123; private KaoYaResource r; Mutil_Consumer(KaoYaResource r) &#123; this.r = r; &#125; public void run() &#123; while(true) &#123; r.consume(); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 就多了两条线程，我们运行代码看看，结果如下：123456789Thread-0...生产者...北京烤鸭63Thread-2...消费者........北京烤鸭63Thread-3...消费者........北京烤鸭63 #消费两次了............Thread-0...生产者...北京烤鸭67 #没有被消费Thread-1...生产者...北京烤鸭68 Thread-2...消费者........北京烤鸭68Thread-0...生产者...北京烤鸭69 不对呀，我们才生产一只烤鸭，怎么就被消费了2次啊，有的烤鸭生产了也没有被消费啊？难道共享数据源没有进行线程同步？回顾下KaoYaResource.java共享数据count的获取方法都进行synchronized关键字同步了呀！那怎么还会出现数据混乱的现象啊？分析：确实，我们对共享数据也采用了同步措施，而且也应用了等待/通知机制，但是这样的措施只在单生产者单消费者的情况下才能正确应用，但从运行结果来看，我们之前的单生产者单消费者安全处理措施就不太适合多生产者多消费者的情况了。那么问题出在哪里？可以明确的告诉大家，肯定是在资源共享类，下面我们就来分析问题是如何出现，又该如何解决？直接上图 解决后的资源代码如下只将if改为了while：1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.sky.code.thread;public class KaoYaResourceByMulti &#123; private String name; private int count = 1;//烤鸭的初始数量 private boolean flag = false;//判断是否有需要线程等待的标志 /** * 生产烤鸭 */ public synchronized void product(String name)&#123; while (flag)&#123; //此时有烤鸭，等待 try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace() ; &#125; &#125; this.name=name+count;//设置烤鸭的名称 count++; System.out.println(Thread.currentThread().getName()+"...生产者..."+this.name); flag=true;//有烤鸭后改变标志 notifyAll();//通知消费线程可以消费了 &#125; /** * 消费烤鸭 */ public synchronized void consume()&#123; while (!flag)&#123;//如果没有烤鸭就等待 try&#123; this.wait(); &#125;catch(InterruptedException e)&#123; &#125; &#125; System.out.println(Thread.currentThread().getName()+"...消费者........"+this.name);//消费烤鸭1 flag = false; notifyAll();//通知生产者生产烤鸭 &#125;&#125; 运行结果跟单线程那个一致，就不贴了。到此，多消费者多生产者模式也完成，不过上面用的是synchronied关键字实现的，而锁对象的解决方法也一样将之前单消费者单生产者的资源类中的if判断改为while判断即可代码就不贴了哈。不过下面我们将介绍一种更有效的锁对象解决方法，我们准备使用两组条件对象（Condition也称为监视器）来实现等待/通知机制，也就是说通过已有的锁获取两组监视器，一组监视生产者，一组监视消费者。有了前面的分析这里我们直接上代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.sky.code.thread;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class ResourceBy2Condition &#123; private String name; private int count = 1; private boolean flag = false; //创建一个锁对象。 Lock lock = new ReentrantLock(); //通过已有的锁获取两组监视器，一组监视生产者，一组监视消费者。 Condition producer_con = lock.newCondition(); Condition consumer_con = lock.newCondition(); /** * 生产 * @param name */ public void product(String name) &#123; lock.lock(); try &#123; while(flag)&#123; try&#123;producer_con.await();&#125;catch(InterruptedException e)&#123;&#125; &#125; this.name = name + count; count++; System.out.println(Thread.currentThread().getName()+"...生产者5.0..."+this.name); flag = true;// notifyAll(); // con.signalAll(); consumer_con.signal();//直接唤醒消费线程 &#125; finally &#123; lock.unlock(); &#125; &#125; /** * 消费 */ public void consume() &#123; lock.lock(); try &#123; while(!flag)&#123; try&#123;consumer_con.await();&#125;catch(InterruptedException e)&#123;&#125; &#125; System.out.println(Thread.currentThread().getName()+"...消费者.5.0......."+this.name);//消费烤鸭1 flag = false;// notifyAll(); // con.signalAll(); producer_con.signal();//直接唤醒生产线程 &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 从代码中可以看到，我们创建了producer_con 和consumer_con两个条件对象，分别用于监听生产者线程和消费者线程，在product()方法中，我们获取到锁后，如果此时flag为true的话，也就是此时还有烤鸭未被消费，因此生产线程需要等待，所以我们调用生产线程的监控器producer_con的await()的方法进入阻塞等待池；但如果此时的flag为false的话，就说明烤鸭已经消费完，需要生产线程去生产烤鸭，那么生产线程将进行烤鸭生产并通过消费线程的监控器consumer_con的signal()方法去通知消费线程对烤鸭进行消费。consume()方法也是同样的道理，这里就不过多分析了。我们可以发现这种方法比我们之前的synchronized同步方法或者是单监视器的锁对象都来得高效和方便些，之前都是使用notifyAll()和signalAll()方法去唤醒池中的线程，然后让池中的线程又进入 竞争队列去抢占CPU资源，这样不仅唤醒了无关的线程而且又让全部线程进入了竞争队列中，而我们最后使用两种监听器分别监听生产者线程和消费者线程，这样的方式恰好解决前面两种方式的问题所在，我们每次唤醒都只是生产者线程或者是消费者线程而不会让两者同时唤醒，这样不就能更高效得去执行程序了吗？好了，到此多生产者多消费者模式也分析完毕。 线程死锁现在我们再来讨论一下线程死锁问题，从上面的分析，我们知道锁是个非常有用的工具，运用的场景非常多，因为它使用起来非常简单，而且易于理解。但同时它也会带来一些不必要的麻烦，那就是可能会引起死锁，一旦产生死锁，就会造成系统功能不可用。我们先通过一个例子来分析，这个例子会引起死锁，使得线程t1和线程t2互相等待对方释放锁。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.sky.code.thread;public class DeadLockDemo &#123; private static String A="A"; private static String B="B"; public static void main(String[] args) &#123; DeadLockDemo deadLock=new DeadLockDemo(); deadLock.deadLock(); &#125; private void deadLock()&#123; Thread t1=new Thread(new Runnable()&#123; @SuppressWarnings("static-access") @Override public void run() &#123; synchronized (A) &#123; try &#123; Thread.currentThread().sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (B) &#123; System.out.println("1"); &#125; &#125; &#125; &#125;); Thread t2 =new Thread(new Runnable() &#123; @Override public void run() &#123; synchronized (B) &#123; synchronized (A) &#123; System.out.println("2"); &#125; &#125; &#125; &#125;); //启动线程 t1.start(); t2.start(); &#125;&#125; 上面代码运行基本没有输出，一直卡着。同步嵌套是产生死锁的常见情景，从上面的代码中我们可以看出，当t1线程拿到锁A后，睡眠2秒，此时线程t2刚好拿到了B锁，接着要获取A锁，但是此时A锁正好被t1线程持有，因此只能等待t1线程释放锁A，但遗憾的是在t1线程内又要求获取到B锁，而B锁此时又被t2线程持有，到此结果就是t1线程拿到了锁A同时在等待t2线程释放锁B，而t2线程获取到了锁B也同时在等待t1线程释放锁A，彼此等待也就造成了线程死锁问题。虽然我们现实中一般不会向上面那么写出那样的代码，但是有些更为复杂的场景中，我们可能会遇到这样的问题，比如t1拿了锁之后，因为一些异常情况没有释放锁（死循环），也可能t1拿到一个数据库锁，释放锁的时候抛出了异常，没有释放等等，所以我们应该在写代码的时候多考虑死锁的情况，这样才能有效预防死锁程序的出现。下面我们介绍一下避免死锁的几个常见方法： 避免一个线程同时获取多个锁。 避免在一个资源内占用多个 资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用tryLock(timeout)来代替使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 避免同步嵌套的发生 Thread.join()如果一个线程A执行了thread.join()语句，其含义是：当前线程A等待thread线程终止之后才能从thread.join()返回。线程Thread除了提供join()方法之外，还提供了join(long millis)和join(long millis,int nanos)两个具备超时特性的方法。这两个超时的方法表示，如果线程在给定的超时时间里没有终止，那么将会从该超时方法中返回。下面给出一个例子，创建10个线程，编号0~9，每个线程调用前一个线程的join()方法，也就是线程0结束了，线程1才能从join()方法中返回，而0需要等待main线程结束。12345678910111213141516171819202122232425262728293031package com.sky.code.thread;public class JoinDemo &#123; public static void main(String[] args) &#123; Thread previous = Thread.currentThread(); for(int i=0;i&lt;10;i++)&#123; //每个线程拥有前一个线程的引用。需要等待前一个线程终止，才能从等待中返回 Thread thread=new Thread(new Domino(previous),String.valueOf(i)); thread.start(); previous=thread; &#125; System.out.println(Thread.currentThread().getName()+" 线程结束"); &#125;&#125;class Domino implements Runnable&#123; private Thread thread; public Domino(Thread thread)&#123; this.thread=thread; &#125; @Override public void run() &#123; try &#123; thread.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+" 线程结束"); &#125;&#125; 运行结果：1234567891011main 线程结束0 线程结束1 线程结束2 线程结束3 线程结束4 线程结束5 线程结束6 线程结束7 线程结束8 线程结束9 线程结束 结束 参考 http://blog.csdn.net/javazejian/article/details/50878665所有代码在 https://github.com/ejunjsh/java-code/tree/master/com/sky/code/thread]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程(一)基础]]></title>
    <url>%2F2016%2F08%2F08%2Fjava-thread-1-md%2F</url>
    <content type="text"><![CDATA[对多线程复习下，汇总一下 什么是线程以及多线程与进程的区别在现代操作在运行一个程序时，会为其创建一个进程。例如启动一个QQ程序，操作系统就会为其创建一个进程。而操作系统中调度的最小单位元是线程，也叫轻量级进程，在一个进程里可以创建多个线程，这些线程都拥有各自的计数器，堆栈和局部变量等属性，并且能够访问共享的内存变量。处理器在这些线程上高速切换，让使用者感觉到这些线程在同时执行。因此我们可以这样理解：进程：正在运行的程序，是系统进行资源分配和调用的独立单位。每一个进程都有它自己的内存空间和系统资源。线程：是进程中的单个顺序控制流，是一条执行路径一个进程如果只有一条执行路径，则称为单线程程序。一个进程如果有多条执行路径，则称为多线程程序。 多线程的创建与启动创建多线程有两种方法，一种是继承Thread类重写run方法，另一种是实现Runnable接口重写run方法。下面我们分别给出代码示例，继承Thread类重写run方法：123456789package com.sky.code.thread;public class NewThread extends Thread &#123; @Override public void run()&#123; System.out.println("I'm a thread that extends Thread!"); &#125;&#125; 实现Runnable接口重写run方法:12345678910package com.sky.code.thread;public class NewRunnable implements Runnable&#123; @Override public void run() &#123; System.out.println("I'm a thread that implements Runnable !"); &#125;&#125; 怎么启动线程？12345678910111213package com.sky.code.thread;public class StartThread &#123; public static void main(String[] args)&#123; NewThread t1=new NewThread(); t1.start(); NewRunnable r=new NewRunnable(); Thread t2=new Thread(r); t2.start(); &#125;&#125; 运行结果：12I&apos;m a thread that extends Thread!I&apos;m a thread that implements Runnable ! 代码相当简单，不过多解释。这里有点需要注意的是调用start()方法后并不是是立即的执行多线程的代码，而是使该线程变为可运行态，什么时候运行多线程代码是由操作系统决定的。 中断线程和守护线程以及线程优先级什么是中断线程？我们先来看看中断线程是什么？(该解释来自java核心技术一书，我对其进行稍微简化)，当线程的run()方法执行方法体中的最后一条语句后，并经由执行return语句返回时，或者出现在方法中没有捕获的异常时线程将终止。在java早期版本中有一个stop方法，其他线程可以调用它终止线程，但是这个方法现在已经被弃用了，因为这个方法会造成一些线程不安全的问题。我们可以把中断理解为一个标识位的属性，它表示一个运行中的线程是否被其他线程进行了中断操作，而中断就好比其他线程对该线程打可个招呼，其他线程通过调用该线程的interrupt方法对其进行中断操作，当一个线程调用interrupt方法时，线程的中断状态（标识位）将被置位（改变），这是每个线程都具有的boolean标志，每个线程都应该不时的检查这个标志，来判断线程是否被中断。而要判断线程是否被中断，我们可以使用如下代码1Thread.currentThread().isInterrupted() 123while(!Thread.currentThread().isInterrupted())&#123; do something &#125; 但是如果此时线程处于阻塞状态（sleep或者wait），就无法检查中断状态，此时会抛出InterruptedException异常。如果每次迭代之后都调用sleep方法（或者其他可中断的方法），isInterrupted检测就没必要也没用处了，如果在中断状态被置位时调用sleep方法，它不会休眠反而会清除这一休眠状态并抛出InterruptedException。所以如果在循环中调用sleep,不要去检测中断状态，只需捕获InterruptedException。代码范例如下：1234567891011public void run()&#123; while(more work to do )&#123; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; //thread was interrupted during sleep e.printStackTrace(); &#125;finally&#123; //clean up , if required &#125; &#125; 不妥的处理方式：12345678void myTask()&#123; ... try&#123; sleep(50) &#125;catch(InterruptedException e)&#123; ... &#125; &#125; 正确的处理方式：123void myTask()throw InterruptedException&#123; sleep(50) &#125; 或者12345678void myTask()&#123; ... try&#123; sleep(50) &#125;catch(InterruptedException e)&#123; Thread.currentThread().interrupt(); &#125; &#125; 最后关于中断线程，我们这里给出中断线程的一些主要方法：void interrupt()：向线程发送中断请求，线程的中断状态将会被设置为true，如果当前线程被一个sleep调用阻塞，那么将会抛出interrupedException异常。static boolean interrupted()：测试当前线程（当前正在执行命令的这个线程）是否被中断。注意这是个静态方法，调用这个方法会产生一个副作用那就是它会将当前线程的中断状态重置为false。boolean isInterrupted()：判断线程是否被中断，这个方法的调用不会产生副作用即不改变线程的当前中断状态。static Thread currentThread() : 返回代表当前执行线程的Thread对象。 这里要注意下，为啥上面的代码，在catch之后还要在中断一次，因为catch会把当前线程的中断标志重置为false，这里不重新中断一次，上层代码就不知道中断了，程序就不知道有中断的发生，下面代码可以说明这个123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.sky.code.thread;/** * Created by ejunjsh on 8/10/2017. */public class TestInterrupt &#123; public static void main(String[] args) &#123; Thread t= new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e)&#123; //catch 异常之后，输出是false System.out.println("1.current interrupted flag is " +Thread.currentThread().isInterrupted()); Thread.currentThread().interrupt(); System.out.println("2.current interrupted flag is " +Thread.currentThread().isInterrupted()); &#125; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e)&#123; System.out.println("3.current interrupted flag is " +Thread.currentThread().isInterrupted()); Thread.currentThread().interrupt(); System.out.println("4.current interrupted flag is " +Thread.currentThread().isInterrupted()); &#125; &#125; &#125;); t.start(); //开始中断 t.interrupt(); try &#123; t.join(); &#125; catch (InterruptedException e) &#123; &#125; System.out.println("5.current state is " +t.getState()); &#125;&#125; 输出结果:123451.current interrupted flag is false2.current interrupted flag is true3.current interrupted flag is false4.current interrupted flag is true5.current state is TERMINATED 很明显，开始中断后，catch的标志位被重置了。 什么是守护线程？首先我们可以通过t.setDaemon(true)的方法将线程转化为守护线程。而守护线程的唯一作用就是为其他线程提供服务。计时线程就是一个典型的例子，它定时地发送“计时器滴答”信号告诉其他线程去执行某项任务。当只剩下守护线程时，虚拟机就退出了，因为如果只剩下守护线程，程序就没有必要执行了。另外JVM的垃圾回收、内存管理等线程都是守护线程。还有就是在做数据库应用时候，使用的数据库连接池，连接池本身也包含着很多后台线程，监控连接个数、超时时间、状态等等。最后还有一点需要特别注意的是在java虚拟机退出时Daemon线程中的finally代码块并不一定会执行哦，代码示例：12345678910111213141516171819202122232425package com.sky.code.thread;public class Deamon &#123; public static void main(String[] args) &#123; Thread deamon = new Thread(new DaemonRunner(),"DaemonRunner"); //设置为守护线程 deamon.setDaemon(true); deamon.start();//启动线程 &#125; static class DaemonRunner implements Runnable&#123; @Override public void run() &#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally&#123; System.out.println("这里的代码在java虚拟机退出时并不一定会执行哦！"); &#125; &#125; &#125;&#125; 因此在构建Daemon线程时，不能依靠finally代码块中的内容来确保执行关闭或清理资源的逻辑。 什么是线程优先级在现代操作系统中基本采用时分的形式调度运行的线程，操作系统会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了就会发生线程调度，并等待着下一次分配。线程分配到的时间片多少也决定了线程使用处理器资源的多少，而线程优先级就是决定线程需要多或者少分配一些处理器资源的线程属性。在java线程中，通过一个整型的成员变量Priority来控制线程优先级，每一个线程有一个优先级，默认情况下，一个线程继承它父类的优先级。可以用setPriority方法提高或降低任何一个线程优先级。可以将优先级设置在MIN_PRIORITY（在Thread类定义为1）与MAX_PRIORITY（在Thread类定义为10）之间的任何值。线程的默认优先级为NORM_PRIORITY（在Thread类定义为5）。尽量不要依赖优先级，如果确实要用，应该避免初学者常犯的一个错误。如果有几个高优先级的线程没有进入非活动状态，低优先级线程可能永远也不能执行。每当调度器决定运行一个新线程时，首先会在具有高优先级的线程中进行选择，尽管这样会使低优先级的线程可能永远不会被执行到。因此我们在设置优先级时，针对频繁阻塞（休眠或者I/O操作）的线程需要设置较高的优先级，而偏重计算（需要较多CPU时间或者运算）的线程则设置较低的优先级，这样才能确保处理器不会被长久独占。当然还有要注意就是在不同的JVM以及操作系统上线程的规划存在差异，有些操作系统甚至会忽略对线程优先级的设定，如mac os系统或者Ubuntu系统…….. 线程的状态转化关系1.新建状态（New）：新创建了一个线程对象。2.就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的start()方法。该状态的线程位于可运行线程池中，变得可运行，等待获取CPU的使用权。3.运行状态（Running）：就绪状态的线程获取了CPU，执行程序代码。4.阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种： 等待阻塞（WAITING）：运行的线程执行wait()方法，JVM会把该线程放入等待池中。 同步阻塞（Blocked）：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。 超时阻塞（TIME_WAITING）：运行的线程执行sleep(long)或join(long)方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。 5.死亡状态（Dead）：线程执行完了或者因异常退出了run()方法，该线程结束生命周期。图中的方法解析如下：Thread.sleep()：在指定时间内让当前正在执行的线程暂停执行，但不会释放”锁标志”。不推荐使用。Thread.sleep(long)：使当前线程进入阻塞状态，在指定时间内不会执行。Object.wait()和Object.wait(long)：在其他线程调用对象的notify或notifyAll方法前，导致当前线程等待。线程会释放掉它所占有的”锁标志”，从而使别的线程有机会抢占该锁。 当前线程必须拥有当前对象锁。如果当前线程不是此锁的拥有者，会抛出IllegalMonitorStateException异常。 唤醒当前对象锁的等待线程使用notify或notifyAll方法，也必须拥有相同的对象锁，否则也会抛出IllegalMonitorStateException异常，waite()和notify()必须在synchronized函数或synchronized中进行调用。如果在non-synchronized函数或non-synchronized中进行调用,虽然能编译通过，但在运行时会发生IllegalMonitorStateException的异常。Object.notifyAll()：则从对象等待池中唤醒所有等待等待线程Object.notify()：则从对象等待池中唤醒其中一个线程。Thread.yield()方法 暂停当前正在执行的线程对象，yield()只是使当前线程重新回到可执行状态，所以执行yield()的线程有可能在进入到可执行状态后马上又被执行，yield()只能使同优先级或更高优先级的线程有执行的机会。Thread.Join()：把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行的线程。比如在线程B中调用了线程A的Join()方法，直到线程A执行完毕后，才会继续执行线程B。好了。本篇线程基础知识介绍到此结束。 参考 http://blog.csdn.net/javazejian/article/details/50878598所有代码在 https://github.com/ejunjsh/java-code/tree/master/com/sky/code/thread]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过反编译深入理解Java String及intern]]></title>
    <url>%2F2016%2F07%2F29%2Fjava-string-intern%2F</url>
    <content type="text"><![CDATA[字符串问题字符串在我们平时的编码工作中其实用的非常多，并且用起来也比较简单，所以很少有人对其做特别深入的研究。倒是面试或者笔试的时候，往往会涉及比较深入和难度大一点的问题。我在招聘的时候也偶尔会问应聘者相关的问题，倒不是说一定要回答的特别正确和深入，通常问这些问题的目的有两个，第一是考察对 JAVA 基础知识的了解程度，第二是考察应聘者对技术的态度。 我们看看以下程序会输出什么结果？如果你能正确的回答每一道题，并且清楚其原因，那本文对你就没什么太大的意义。如果回答不正确或者不是很清楚其原理，那就仔细看看以下的分析，本文应该能帮助你清楚的理解每段程序的结果及输出该结果的深层次原因。 代码段一：1234567891011package com.paddx.test.string;public class StringTest &#123; public static void main(String[] args) &#123; String str1 = "string"; String str2 = new String("string"); String str3 = str2.intern(); System.out.println(str1==str2);//#1 System.out.println(str1==str3);//#2 &#125;&#125; 代码段二：12345678910111213141516171819 package com.paddx.test.string; public class StringTest01 &#123; public static void main(String[] args) &#123; String baseStr = "baseStr"; final String baseFinalStr = "baseStr"; String str1 = "baseStr01"; String str2 = "baseStr"+"01"; String str3 = baseStr + "01"; String str4 = baseFinalStr+"01"; String str5 = new String("baseStr01").intern(); System.out.println(str1 == str2);//#3 System.out.println(str1 == str3);//#4 System.out.println(str1 == str4);//#5 System.out.println(str1 == str5);//#6 &#125;&#125; 代码段三（1）：12345678910package com.paddx.test.string;&lt;br&gt; public class InternTest &#123; public static void main(String[] args) &#123; String str2 = new String("str")+new String("01"); str2.intern(); String str1 = "str01"; System.out.println(str2==str1);//#7 &#125;&#125; 代码段三（2）：12345678910 package com.paddx.test.string; public class InternTest01 &#123; public static void main(String[] args) &#123; String str1 = "str01"; String str2 = new String("str")+new String("01"); str2.intern(); System.out.println(str2 == str1);//#8 &#125;&#125; 为了方便描述，我对上述代码的输出结果由#1~#8进行了编码，下文中蓝色字体部分即为结果。 字符串深入分析代码段一分析字符串不属于基本类型，但是可以像基本类型一样，直接通过字面量赋值，当然也可以通过new来生成一个字符串对象。不过通过字面量赋值的方式和new的方式生成字符串有本质的区别： 通过字面量赋值创建字符串时，会优先在常量池中查找是否已经存在相同的字符串，倘若已经存在，栈中的引用直接指向该字符串；倘若不存在，则在常量池中生成一个字符串，再将栈中的引用指向该字符串。而通过new的方式创建字符串时，就直接在堆中生成一个字符串的对象（备注，JDK 7 以后，HotSpot 已将常量池从永久代转移到了堆中。详细信息可参考《JDK8内存模型-消失的PermGen》一文），栈中的引用指向该对象。对于堆中的字符串对象，可以通过 intern() 方法来将字符串添加的常量池中，并返回指向该常量的引用。现在我们应该能很清楚代码段一的结果了： 结果 #1：因为str1指向的是字符串中的常量，str2是在堆中生成的对象，所以str1==str2返回false。结果 #2：str2调用intern方法，会将str2中值（“string”）复制到常量池中，但是常量池中已经存在该字符串（即str1指向的字符串），所以直接返回该字符串的引用，因此str1==str2返回true。 以下运行代码段一的代码的结果： 代码段二分析对于代码段二的结果，还是通过反编译StringTest01.class文件比较容易理解：常量池内容（部分）：执行指令（部分，第二列#+序数对应常量池中的项）：在解释上述执行过程之前，先了解两条指令： ldc：Push item from run-time constant pool，从常量池中加载指定项的引用到栈。 astore_：Store reference into local variable，将引用赋值给第n个局部变量。 现在我们开始解释代码段二的执行过程： 0: ldc #2：加载常量池中的第二项（”baseStr”）到栈中。 2: astore_1 ：将1中的引用赋值给第一个局部变量，即String baseStr = “baseStr”； 3: ldc #2：加载常量池中的第二项（”baseStr”）到栈中。 5: astore_2 ：将3中的引用赋值给第二个局部变量，即 final String baseFinalStr=”baseStr”； 6: ldc #3：加载常量池中的第三项（”baseStr01”）到栈中。 8: astore_3 ：将6中的引用赋值给第三个局部变量，即String str1=”baseStr01”; 9: ldc #3：加载常量池中的第三项（”baseStr01”）到栈中。 11: astore 4：将9中的引用赋值给第四个局部变量：即String str2=”baseStr01”； 结果#3：str1==str2 肯定会返回true，因为str1和str2都指向常量池中的同一引用地址。所以其实在JAVA 1.6之后，常量字符串的“+”操作，编译阶段直接会合成为一个字符串。13: new #4：生成StringBuilder的实例。16: dup ：复制13生成对象的引用并压入栈中。17: invokespecial #5：调用常量池中的第五项，即StringBuilder.方法。以上三条指令的作用是生成一个StringBuilder的对象。20: aload_1 ：加载第一个参数的值，即”baseStr”21: invokevirtual #6 ：调用StringBuilder对象的append方法。24: ldc #7：加载常量池中的第七项（”01”）到栈中。26: invokevirtual #6：调用StringBuilder.append方法。29: invokevirtual #8：调用StringBuilder.toString方法。32: astore 5：将29中的结果引用赋值改第五个局部变量，即对变量str3的赋值。结果 #4：因为str3实际上是stringBuilder.append()生成的结果，所以与str1不相等，结果返回false。34: ldc #3：加载常量池中的第三项（”baseStr01”）到栈中。36: astore 6：将34中的引用赋值给第六个局部变量，即str4=”baseStr01”;结果 #5 ：因为str1和str4指向的都是常量池中的第三项，所以str1==str4返回true。这里我们还能发现一个现象，对于final字段，编译期直接进行了常量替换，而对于非final字段则是在运行期进行赋值处理的。38: new #9：创建String对象41: dup ：复制引用并压如栈中。42: ldc #3：加载常量池中的第三项（”baseStr01”）到栈中。44: invokespecial #10：调用String.”“方法，并传42步骤中的引用作为参数传入该方法。47: invokevirtual #11：调用String.intern方法。从38到41的对应的源码就是new String(“baseStr01”).intern()。50: astore 7：将47步返回的结果赋值给变量7，即str5指向baseStr01在常量池中的位置。结果 #6 ：因为str5和str1都指向的都是常量池中的同一个字符串，所以str1==str5返回true。运行代码段二，输出结果如下：## 代码段三解析：对于代码段三，在 JDK 1.6 和 JDK 1.7中的运行结果不同。我们先看一下运行结果，然后再来解释其原因：JDK 1.6 下的运行结果：JDK 1.7 下的运行结果：根据对代码段一的分析，应该可以很简单得出 JDK 1.6 的结果，因为 str2 和 str1本来就是指向不同的位置，理应返回false。比较奇怪的问题在于JDK 1.7后，对于第一种情况返回true，但是调换了一下位置返回的结果就变成了false。这个原因主要是从JDK 1.7后，HotSpot 将常量池从永久代移到了元空间，正因为如此，JDK 1.7 后的intern方法在实现上发生了比较大的改变，JDK 1.7后，intern方法还是会先去查询常量池中是否有已经存在，如果存在，则返回常量池中的引用，这一点与之前没有区别，区别在于，如果在常量池找不到对应的字符串，则不会再将字符串拷贝到常量池，而只是在常量池中生成一个对原字符串的引用。所以:结果 #7：在第一种情况下，因为常量池中没有“str01”这个字符串，所以会在常量池中生成一个对堆中的“str01”的引用，而在进行字面量赋值的时候，常量池中已经存在，所以直接返回该引用即可，因此str1和str2都指向堆中的字符串，返回true。结果 #8：调换位置以后，因为在进行字面量赋值（String str1 = “str01”）的时候，常量池中不存在，所以str1指向的常量池中的位置，而str2指向的是堆中的对象，再进行intern方法时，对str1和str2已经没有影响了，所以返回false。 常见面试题解答有了对以上的知识的了解，我们现在再来看常见的面试或笔试题就很简单了： Q：String s = new String(“xyz”)，创建了几个String Object? A：两个，常量池中的”xyz”和堆中对象。 Q：下列程序的输出结果： String s1 = “abc”;String s2 = “abc”;System.out.println(s1 == s2); A：true，均指向常量池中对象。 Q：下列程序的输出结果： String s1 = new String(“abc”);String s2 = new String(“abc”);System.out.println(s1 == s2); A：false，两个引用指向堆中的不同对象。 Q：下列程序的输出结果： String s1 = “abc”;String s2 = “a”;String s3 = “bc”;String s4 = s2 + s3;System.out.println(s1 == s4); A：false，因为s2+s3实际上是使用StringBuilder.append来完成，会生成不同的对象。 Q：下列程序的输出结果： String s1 = “abc”;final String s2 = “a”;final String s3 = “bc”;String s4 = s2 + s3;System.out.println(s1 == s4); A：true，因为final变量在编译后会直接替换成对应的值，所以实际上等于s4=”a”+”bc”，而这种情况下，编译器会直接合并为s4=”abc”，所以最终s1==s4。 Q：下列程序的输出结果： String s = new String(“abc”);String s1 = “abc”;String s2 = new String(“abc”); System.out.println(s == s1.intern());System.out.println(s == s2.intern());System.out.println(s1 == s2.intern()); A：false，false，true，具体原因参考第二部分内容。 原文地址 http://www.cnblogs.com/paddix/p/5326863.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8内存模型—永久代(PermGen)和元空间(Metaspace)]]></title>
    <url>%2F2016%2F07%2F26%2Fjava8-permgen-metaspace%2F</url>
    <content type="text"><![CDATA[JVM 内存模型根据 JVM 规范，JVM 内存共分为虚拟机栈、堆、方法区、程序计数器、本地方法栈五个部分。 虚拟机栈每个线程有一个私有的栈，随着线程的创建而创建。栈里面存着的是一种叫“栈帧”的东西，每个方法会创建一个栈帧，栈帧中存放了局部变量表（基本数据类型和对象引用）、操作数栈、方法出口等信息。栈的大小可以固定也可以动态扩展。当栈调用深度大于JVM所允许的范围，会抛出StackOverflowError的错误，不过这个深度范围不是一个恒定的值，我们通过下面这段程序可以测试一下这个结果：栈溢出测试源码：1234567891011121314151617181920package com.paddx.test.memory; public class StackErrorMock &#123; private static int index = 1; public void call()&#123; index++; call(); &#125; public static void main(String[] args) &#123; StackErrorMock mock = new StackErrorMock(); try &#123; mock.call(); &#125;catch (Throwable e)&#123; System.out.println("Stack deep : "+index); e.printStackTrace(); &#125; &#125;&#125; 代码段1 运行三次，可以看出每次栈的深度都是不一样的，输出结果如下。至于红色框里的值是怎么出来的，就需要深入到 JVM 的源码中才能探讨，这里不作详细阐述。 虚拟机栈除了上述错误外，还有另一种错误，那就是当申请不到空间时，会抛出 OutOfMemoryError。这里有一个小细节需要注意，catch 捕获的是 Throwable，而不是 Exception。因为 StackOverflowError 和 OutOfMemoryError 都不属于 Exception 的子类。 本地方法栈这部分主要与虚拟机用到的 Native 方法相关，一般情况下， Java 应用程序员并不需要关心这部分的内容 PC 寄存器PC 寄存器，也叫程序计数器。JVM支持多个线程同时运行，每个线程都有自己的程序计数器。倘若当前执行的是 JVM 的方法，则该寄存器中保存当前执行指令的地址；倘若执行的是native 方法，则PC寄存器中为空。 堆堆内存是 JVM 所有线程共享的部分，在虚拟机启动的时候就已经创建。所有的对象和数组都在堆上进行分配。这部分空间可通过 GC 进行回收。当申请不到空间时会抛出 OutOfMemoryError。下面我们简单的模拟一个堆内存溢出的情况：12345678910111213141516171819202122package com.paddx.test.memory; import java.util.ArrayList;import java.util.List; public class HeapOomMock &#123; public static void main(String[] args) &#123; List&lt;byte[]&gt; list = new ArrayList&lt;byte[]&gt;(); int i = 0; boolean flag = true; while (flag)&#123; try &#123; i++; list.add(new byte[1024 * 1024]);//每次增加一个1M大小的数组对象 &#125;catch (Throwable e)&#123; e.printStackTrace(); flag = false; System.out.println("count="+i);//记录运行的次数 &#125; &#125; &#125;&#125; 代码段2 运行上述代码，输出结果如下： 注意，这里我指定了堆内存的大小为16M，所以这个地方显示的count=14（这个数字不是固定的），至于为什么会是14或其他数字，需要根据 GC 日志来判断 方法区 方法区也是所有线程共享。主要用于存储类的信息、常量池、方法数据、方法代码等。方法区逻辑上属于堆的一部分，但是为了与堆进行区分，通常又叫“非堆”。 关于方法区内存溢出的问题会在下文中详细探讨。 PermGen（永久代）绝大部分 Java 程序员应该都见过 “java.lang.OutOfMemoryError: PermGen space “这个异常。这里的 “PermGen space”其实指的就是方法区。不过方法区和“PermGen space”又有着本质的区别。前者是 JVM 的规范，而后者则是 JVM 规范的一种实现，并且只有 HotSpot 才有 “PermGen space”，而对于其他类型的虚拟机，如 JRockit（Oracle）、J9（IBM） 并没有“PermGen space”。由于方法区主要存储类的相关信息，所以对于动态生成类的情况比较容易出现永久代的内存溢出。最典型的场景就是，在 jsp 页面比较多的情况，容易出现永久代内存溢出。我们现在通过动态生成类来模拟 “PermGen space”的内存溢出：1234package com.paddx.test.memory; public class Test &#123;&#125; 12345678910111213141516171819202122232425package com.paddx.test.memory; import java.io.File;import java.net.URL;import java.net.URLClassLoader;import java.util.ArrayList;import java.util.List; public class PermGenOomMock&#123; public static void main(String[] args) &#123; URL url = null; List&lt;ClassLoader&gt; classLoaderList = new ArrayList&lt;ClassLoader&gt;(); try &#123; url = new File("/tmp").toURI().toURL(); URL[] urls = &#123;url&#125;; while (true)&#123; ClassLoader loader = new URLClassLoader(urls); classLoaderList.add(loader); loader.loadClass("com.paddx.test.memory.Test"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 代码段3 运行结果如下： 本例中使用的 JDK 版本是 1.7，指定的 PermGen 区的大小为 8M。通过每次生成不同URLClassLoader对象来加载Test类，从而生成不同的类对象，这样就能看到我们熟悉的 “java.lang.OutOfMemoryError: PermGen space “ 异常了。这里之所以采用 JDK 1.7，是因为在 JDK 1.8 中， HotSpot 已经没有 “PermGen space”这个区间了，取而代之是一个叫做 Metaspace（元空间） 的东西。下面我们就来看看 Metaspace 与 PermGen space 的区别。 Metaspace（元空间）其实，移除永久代的工作从JDK1.7就开始了。JDK1.7中，存储在永久代的部分数据就已经转移到了Java Heap或者是 Native Heap。但永久代仍存在于JDK1.7中，并没完全移除，譬如符号引用(Symbols)转移到了native heap；字面量(interned strings)转移到了java heap；类的静态变量(class statics)转移到了java heap。我们可以通过一段程序来比较 JDK 1.6 与 JDK 1.7及 JDK 1.8 的区别，以字符串常量为例：12345678910111213141516package com.paddx.test.memory; import java.util.ArrayList;import java.util.List; public class StringOomMock &#123; static String base = "string"; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); for (int i=0;i&lt; Integer.MAX_VALUE;i++)&#123; String str = base + base; base = str; list.add(str.intern()); &#125; &#125;&#125; 代码段4 这段程序以2的指数级不断的生成新的字符串，这样可以比较快速的消耗内存。我们通过 JDK 1.6、JDK 1.7 和 JDK 1.8 分别运行：JDK 1.6 的运行结果：JDK 1.7的运行结果：JDK 1.8的运行结果：从上述结果可以看出，JDK 1.6下，会出现“PermGen Space”的内存溢出，而在 JDK 1.7和 JDK 1.8 中，会出现堆内存溢出，并且 JDK 1.8中 PermSize 和 MaxPermGen 已经无效。因此，可以大致验证 JDK 1.7 和 1.8 将字符串常量由永久代转移到堆中，并且 JDK 1.8 中已经不存在永久代的结论。现在我们看看元空间到底是一个什么东西？ 元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制，但可以通过以下参数来指定元空间的大小： -XX:MetaspaceSize，初始空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。 -XX:MaxMetaspaceSize，最大空间，默认是没有限制的。 除了上面两个指定大小的选项以外，还有两个与 GC 相关的属性： -XX:MinMetaspaceFreeRatio，在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集 -XX:MaxMetaspaceFreeRatio，在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集 现在我们在 JDK 8下重新运行一下代码段3，不过这次不再指定 PermSize 和 MaxPermSize。而是指定 MetaSpaceSize 和 MaxMetaSpaceSize的大小。输出结果如下：从输出结果，我们可以看出，这次不再出现永久代溢出，而是出现了元空间的溢出。 总结 通过上面分析，大家应该大致了解了 JVM 的内存划分，也清楚了 JDK 8 中永久代向元空间的转换。不过大家应该都有一个疑问，就是为什么要做这个转换？所以，最后给大家总结以下几点原因： 字符串存在永久代中，容易出现性能问题和内存溢出。 类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代溢出。 永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。 Oracle 可能会将HotSpot 与 JRockit 合二为一。 原文地址 http://www.cnblogs.com/paddix/p/5309550.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从字节码层面看“HelloWorld”]]></title>
    <url>%2F2016%2F07%2F23%2Fbytecode-hello-world%2F</url>
    <content type="text"><![CDATA[HelloWorld 字节码生成众所周知，Java 程序是在 JVM 上运行的，不过 JVM 运行的其实不是 Java 语言本身，而是 Java 程序编译成的字节码文件。可能一开始 JVM 是为 Java 语言服务的，不过随着编译技术和 JVM 自身的不断发展和成熟，JVM 已经不仅仅只运行 Java 程序。任何能编译成为符合 JVM 字节码规范的语言都可以在 JVM 上运行，比较常见的 Scala、Groove、JRuby等。今天，我就从大家最熟悉的程序“HelloWorld”程序入手，分析整个 Class 文件的结构。虽然这个程序比较简单，但是基本上包含了字节码规范中的所有内容，因此即使以后要分析更复杂的程序，那也只是“量”上的变化，本质上没有区别。 我们先直观的看下源码与字节码之间的对应关系:HelloWorld的源码：1234567package com.paddx.test.asm; public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println("Hello,World!"); &#125;&#125; 编译器采用JDK 1.7：12345678&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; 编译以后的字节码文件（使用UltraEdit的16进制模式打开）：红色框内的部分就是HelloWorld.class的内容，其他部分是UltraEdit自动生成的：红色框顶部的0~f代表列号，左边部分代表行号，右侧部分是二进制码对应的字符（utf-8编码）。 字节码解析要弄明白 HelloWorld.java 和 HelloWorld.class 文件是如何对应的，我们必须对 JVM 的字节码规范有所了解。字节码文件的结构非常紧凑，没有任何冗余的信息，连分隔符都没有，它采用的是固定的文件结构和数据类型来实现对内容的分割的。字节码中包括两种数据类型：无符号数和表。无符号数又包括 u1，u2，u4，u8四种，分别代表1个字节、2个字节、4个字节和8个字节。而表结构则是由无符号数据组成的。 字节码文件的格式固定如下： type descriptor u4 magic u2 minor_version u2 major_version u2 constant_pool_count cp_info constant_pool[cosntant_pool_count – 1] u2 access_flags u2 this_class u2 super_class u2 interfaces_count u2 interfaces[interfaces_count] u2 fields_count field_info fields[fields_count] u2 methods_count method_info methods[methods_count] u2 attributes_count attribute_info attributes[attributes_count] 现在，我们就按这个格式对上述HelloWorld.class文件进行分析： magic（u4）：CA FE BA BE ，代表该文件是一个字节码文件，我们平时区分文件类型都是通过后缀名来区分的，不过后缀名是可以随便修改的，所以仅靠后缀名不能真正区分一个文件的类型。区分文件类型的另个办法就是magic数字，JVM 就是通过 CA FE BA BE 来判断该文件是不是class文件。 minor_version（u2）：00 00，小版本号，因为我这里采用的1.7，所以小版本号为0. major_version（u2）：00 33，大版本号，x033转换为十进制为51，下表是jdk 1.6 以后对应支持的 Class 文件版本号： 编译器版本 -target参数 十六进制版本 十进制版本 JDK 1.6.0_01 不带（默认 -target 1.6） 00 00 00 32 50.0 JDK 1.6.0_01 -target 1.5 00 00 00 31 49.0 JDK 1.6.0_01 -target 1.4 -source 1.4 00 00 00 30 48.0 JDK 1.7.0 不带（默认 -target 1.7） 00 00 00 33 51.0 JDK 1.7.0 -target 1.6 00 00 00 32 50.0 JDK 1.7.0 -target 1.4 -source 1.4 00 00 00 30 48.0 JDK 1.8.0 不带（默认 -target 1.8） 00 00 00 34 52.0 constant_pool_count（u2）：00 22，常量池数量，转换为十进制后为34，这里需要注意的是，字节码的常量池是从1开始计数的，所以34表示为（34-1）=33项。 TAG（u1）：0A，常量池的数据类型是表，每一项的开始都有一个tag（u1），表示常量的类型，常量池的表的类型包括如下14种，这里A（10）表示CONSTANT_Methodref，代表方法引用。 常量类型 值 CONSTANT_Utf8_info 1 CONSTANT_Integer_info 3 CONSTANT_Float_info 4 CONSTANT_Long_info 5 CONSTANT_Double_info 6 CONSTANT_Class_info 7 CONSTANT_String_info 8 CONSTANT_Fieldref_info 9 CONSTANT_Methodref_info 10 CONSTANT_InterfaceMethodref_info 11 CONSTANT_NameAndType_info 12 CONSTANT_MethodHandle_info 15 CONSTANT_MethodType_info 16 CONSTANT_InvokeDynamic_info 18 每种常量类型对应表结构： 常量 项目 类型 描述 CONSTANT_Utf8_info tag u1 1 length u2 字节数 bytes u1 utf-8编码的字符串 CONSTANT_Integer_info tag u1 3 bytes u4 int值 CONSTANT_Float_info tag u4 4 bytes u1 float值 CONSTANT_Long_info tag u1 5 bytes u8 long值 CONSTANT_Double_info tag u1 6 bytes u8 double值 CONSTANT_Class_info tag u1 7 index u2 指向全限定名常量项的索引 CONSTANT_String_info tag u1 8 index u2 指向字符串常量的索引 CONSTANT_Fieldref_info tag u1 9 index u2 指向声明字段的类或接口描述符CONSTANT_Class_info的索引值 index u2 指向CONSTANT_NameAndType_info的索引值 CONSTANT_Methodref_info tag u1 10 index u2 指向声明方法的类描述符CONSTANT_Class_info的索引值 index u2 指向CONSTANT_NameAndType_info的索引值 CONSTANT_InterfaceMethodref_info tag u1 11 index u2 指向声明方法的接口描述符CONSTANT_Class_info的索引值 index u2 指向CONSTANT_NameAndType_info的索引值 CONSTANT_NameAndType_info tag u1 12 index u2 指向该字段或方法名称常量的索引值 index u2 指向该字段或方法描述符常量的索引值 CONSTANT_MethodHandle_info tag u1 15 reference_kind u1 值必须1~9，它决定了方法句柄的的类型 reference_index u2 对常量池的索引 CONSTANT_MethodType_info tag u1 16 description_index u2 对常量池中方法描述符的索引 CONSTANT_InvokeDynamic_info tag u1 18 bootstap_method_attr_index u2 对引导方法表的索引 name_and_type_index u2 对CONSTANT_NameAndType_info的索引 CONSTANT_Methodref_info（u2):00 06，因为tag为A，代表一个方法引用表（CONSTANT_Methodref_info），所以第二项（u2）应该是指向常量池的位置，即常量池的第六项，表示一个CONSTANT_Class_info表的索引，用类似的方法往下分析，可以发现常量池的第六项如下，tag类型为07，查询上表可知道其即为CONSTANT_Class_info。 07之后的00 1B表示对常量池地27项（CONSTANT_Utf8_info）的引用，查看第27项如下图，即（java/lang/Object）： CONSTANT_NameAndType_info（u2）：00 14,方法引用表的第三项（u2），常量池索引，指向第20项。 CONSTANT_Fieldref_info（u1）：tag为09。 ….. 常量池的分析都类似，其他的分析由于篇幅问题就不在此一一讲述了。跳过常量池就到了访问标识（u2）： JVM 对访问标示符的规范如下： Flag Name Value Remarks ACC_PUBLIC 0x0001 pubilc ACC_FINAL 0x0010 final ACC_SUPER 0x0020 用于兼容早期编译器，新编译器都设置该标记，以在使用 invokespecial指令时对子类方法做特定处理。 ACC_INTERFACE 0x0200 接口，同时需要设置：ACC_ABSTRACT。不可同时设置：ACC_FINAL、ACC_SUPER、ACC_ENUM ACC_ABSTRACT 0x0400 抽象类，无法实例化。不可与ACC_FINAL同时设置。 ACC_SYNTHETIC 0x1000 synthetic，由编译器产生，不存在于源代码中。 ACC_ANNOTATION 0x2000 注解类型（annotation），需同时设置：ACC_INTERFACE、ACC_ABSTRACT ACC_ENUM 0x4000 枚举类型 这个表里面无法直接查询到0021这个值，原因是0021=0020+0001，即public+invokespecial指令，源码中的方法main是public的，而invokespecial是现在的版本都有的，所以值为0021。 接着往下是this_class（u2）：是指向constant pool的索引值，该值必须是CONSTANT_Class_info类型，值为00 05，即指向常量池中的第五项，第五项指向常量池中的第26项，即com/paddx/test/asm/HelloWorld： super_class(u2)）：super_class是指向constant pool的索引值，该值必须是CONSTANT_Class_info类型，指定当前字节码定义的类或接口的直接父类。这里的取值为00 06，根据上面的分析，对应的指向的全限定性类名为java/lang/object，即当前类的父类为Object类。 interfaces_count（u2）：接口的数量，因为这里没有实现接口，所以值为 00 00。 interfaces[interfaces_count]：因为没有接口，所以就不存在interfces选项。 field_count：属性数量，00 00。 field_info：因为没有属性，所以不存在这个选项。 method_count：00 02，为什么会有两个方法呢？我们明明只写了一个方法，这是因为JVM 会自动生成一个 的方法。 method_info：方法表，其结构如下： Type Descriptor u2 access_flag u2 name_index u2 descriptor_index u2 attributes_count attribute_info attribute_info[attributes_count] HelloWorld.class文件中对应的数据： access_flag（u2）: 00 01 name_index（u2）:00 07 descriptor_index（u2）:00 08 可以看看 07、08对应的常量池里面的值： 即 07 对应的是 &#60;init&#62;，08 对应的是()； attributes_count:00 01，表示包含一个属性 attribute_info：属性表，该表的结构如下： Type Descriptor u2 attribute_name_index u4 attribute_length u1 bytes attribute_name_index（u2）: 00 09，指向常量池中的索引。 attribute_length（u4）：00 00 00 2F，属性的长度47。 attribute_info:具体属性的分析与上面类似，大家可以对着JVM的规范自己尝试分析一下。 第一个方法结束后，接着进入第二个方法： 第二个方法的属性长度为x037，转换为十进制为55个字节。两个方法之后紧跟着的是attribute_count和attributes： attribute_count（u2）:值为 00 01，即有一个属性。 attribute_name_index（u2）：指向常量池中的第十二项。 attribute_length（u4）：00 00 00 02，长度为2。 分析完毕！ 基于字节码的操作 通过对HelloWorld这个程序的字节码分析，我们应该能够比较清楚的认识到整个字节码的结构。那我们通过字节码，可以做些什么呢？其实通过字节码能做很多平时我们无法完成的工作。比如，在类加载之前添加某些操作或者直接动态的生成字节码，CGlib就是通过这种方式来实现动态代理的。现在，我们就来完成另一个版本的HelloWorld：1234567package com.paddx.test.asm; public class HelloWorld2 &#123; public static void sayHello()&#123; &#125;&#125; 我们有个空的方法 sayHello()，现在要实现调该方法的时候打印出“HelloWorld”，怎么处理？如果我们手动去修改字节码文件，将打印“HelloWorld”的代码插入到sayHello方法中，原理上肯定没问题，不过操作过程还是比较复杂的。Java 的最大优势就在于只要你能想到的功能，基本上就有第三方开源的库实现过。字节码操作的开源库也比较多，这里我就用 ASM 4.0来实现该功能：12345678910111213141516171819202122232425262728293031323334353637383940package com.paddx.test.asm; import org.objectweb.asm.*; import java.io.IOException;import java.lang.reflect.InvocationTargetException; public class AsmDemo extends ClassLoader&#123; public static void main(String[] args) throws IOException, IllegalAccessException, InstantiationException, InvocationTargetException &#123; ClassReader classReader = new ClassReader("com.paddx.test.asm.HelloWorld2"); ClassWriter cw=new ClassWriter(ClassWriter.COMPUTE_MAXS); CustomVisitor myv=new CustomVisitor(Opcodes.ASM4,cw); classReader.accept(myv, 0); byte[] code=cw.toByteArray(); AsmDemo loader=new AsmDemo(); Class&lt;?&gt; appClass=loader.defineClass(null, code, 0,code.length); appClass.getMethods()[0].invoke(appClass.newInstance(), new Object[]&#123;&#125;); &#125; &#125; class CustomVisitor extends ClassVisitor implements Opcodes &#123; public CustomVisitor(int api, ClassVisitor cv) &#123; super(api, cv); &#125; @Override public MethodVisitor visitMethod(int access, String name, String desc, String signature, String[] exceptions) &#123; MethodVisitor mv = super.visitMethod(access, name, desc, signature, exceptions); if (name.equals("sayHello")) &#123; mv.visitFieldInsn(GETSTATIC, "java/lang/System", "out", "Ljava/io/PrintStream;"); mv.visitLdcInsn("HelloWorld!"); mv.visitMethodInsn(INVOKEVIRTUAL, "java/io/PrintStream", "println", "(Ljava/lang/String;)V"); &#125; return mv; &#125;&#125; 运行结果如下： 关于 ASM 4的操作在这就不细说了。有兴趣的朋友可以自己去研究一下，有机会，我也可以再后续的博文中跟大家分享。 总结 本文通过HelloWorld这样一个大家都非常熟悉的例子，深入的分析了字节码文件的结构。利用这些特性，我们可以完成一些相对高级的功能，如动态代理等。这些例子虽然都很简单，但是“麻雀虽小五脏俱全”，即使再复杂的程序也逃离不了这些最基本的东西。技术层面的东西就是这样子，只要你能了解一个简单的程序的原理，举一反三，就能很容易的理解更复杂的程序，这就是技术“易”的方面。同时，反过来说，即使“HelloWorld”这样一个简单的程序，如果我们深入探究，也不一定能特别理解其原理，这就是技术“难”的方面。总之，技术这种东西只要你用心深入地去研究，总是能带给你意想不到的惊喜~ 原文地址 http://www.cnblogs.com/paddix/p/5282004.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis cluster管理工具redis-trib.rb详解]]></title>
    <url>%2F2016%2F04%2F22%2Fredis-trib%2F</url>
    <content type="text"><![CDATA[redis-trib.rb是redis官方推出的管理redis集群的工具，集成在redis的源码src目录下，是基于redis提供的集群命令封装成简单、便捷、实用的操作工具。redis-trib.rb是redis作者用ruby完成的。为了看懂redis-trib.rb，我特意花了一个星期学习了ruby，也被ruby的简洁、明了所吸引。ruby是门非常灵活的语言，redis-trib.rb只用了1600行左右的代码，就实现了强大的集群操作。本文对redis-trib.rb的介绍是基于redis 3.0.6版本的源码。阅读本文需要对redis集群功能有一定的了解。关于redis集群功能的介绍，可以参考本人的另一篇文章《redis3.0 cluster功能介绍》。 help 信息先从redis-trib.rb的help信息，看下redis-trib.rb提供了哪些功能。12345678910111213141516171819202122232425262728293031323334353637$ruby redis-trib.rb helpUsage: redis-trib &lt;command&gt; &lt;options&gt; &lt;arguments ...&gt; create host1:port1 ... hostN:portN --replicas &lt;arg&gt; check host:port info host:port fix host:port --timeout &lt;arg&gt; reshard host:port --from &lt;arg&gt; --to &lt;arg&gt; --slots &lt;arg&gt; --yes --timeout &lt;arg&gt; --pipeline &lt;arg&gt; rebalance host:port --weight &lt;arg&gt; --auto-weights --threshold &lt;arg&gt; --use-empty-masters --timeout &lt;arg&gt; --simulate --pipeline &lt;arg&gt; add-node new_host:new_port existing_host:existing_port --slave --master-id &lt;arg&gt; del-node host:port node_id set-timeout host:port milliseconds call host:port command arg arg .. arg import host:port --from &lt;arg&gt; --copy --replace help (show this help)For check, fix, reshard, del-node, set-timeout you can specify the host and port of any working node in the cluster. 可以看到redis-trib.rb具有以下功能：1、create：创建集群2、check：检查集群3、info：查看集群信息4、fix：修复集群5、reshard：在线迁移slot6、rebalance：平衡集群节点slot数量7、add-node：将新节点加入集群8、del-node：从集群中删除节点9、set-timeout：设置集群节点间心跳连接的超时时间10、call：在集群全部节点上执行命令11、import：将外部redis数据导入集群下面从redis-trib.rb使用和源码的角度详细介绍redis-trib.rb的每个功能。 redis-trib.rb主要有两个类：ClusterNode和RedisTrib。ClusterNode保存了每个节点的信息，RedisTrib则是redis-trib.rb各个功能的实现。 ClusterNode对象先分析ClusterNode源码。ClusterNode有下面几个成员变量（ruby的类成员变量是以@开头的）：@r：执行redis命令的客户端对象。@info：保存了该节点的详细信息，包括cluster nodes命令中自己这行的信息和cluster info的信息。@dirty：节点信息是否需要更新，如果为true，我们需要把内存的节点更新信息到节点上。@friends：保存了集群其他节点的info信息。其信息为通过cluster nodes命令获得的其他节点信息。ClusterNode有下面一些成员方法：initialize：ClusterNode的构造方法，需要传入节点的地址信息。friends：返回@friends对象。slots：返回该节点负责的slots信息。has_flag?：判断节点info信息的的flags中是否有给定的flag。to_s：类似java的toString方法，返回节点的地址信息。connect：连接redis节点。assert_cluster：判断节点开启了集群配置。assert_empty：确定节点目前没有跟任何其他节点握手，同时自己的db数据为空。load_info：通过cluster info和cluster nodes导入节点信息。add_slots：给节点增加slot，该操作只是在内存中修改，并把dirty设置成true，等待flush_node_config将内存中的数据同步在节点执行。set_as_replica：slave设置复制的master地址。dirty设置成true。flush_node_config：将内存的数据修改同步在集群节点中执行。info_string：简单的info信息。get_config_signature：用来验证集群节点见的cluster nodes信息是否一致。该方法返回节点的签名信息。info：返回@info对象，包含详细的info信息。is_dirty?：判断@dirty。r：返回执行redis命令的客户端对象。 有了ClusterNode对象，在处理集群操作的时候，就获得了集群的信息，可以进行集群相关操作。在此先简单介绍下redis-trib.rb脚本的使用，以create为例：12create host1:port1 ... hostN:portN --replicas &lt;arg&gt; host1:port1 … hostN:portN表示子参数，这个必须在可选参数之后，–replicas 是可选参数，带的表示后面必须填写一个参数，像–slave这样，后面就不带参数，掌握了这个基本规则，就能从help命令中获得redis-trib.rb的使用方法。 其他命令大都需要传递host:port，这是redis-trib.rb为了连接集群，需要选择集群中的一个节点，然后通过该节点获得整个集群的信息。 下面就一一详细介绍redis-trib.rb的每个功能。 create创建集群create命令可选replicas参数，replicas表示需要有几个slave。最简单命令使用如下：1$ruby redis-trib.rb create 10.180.157.199:6379 10.180.157.200:6379 10.180.157.201:6379 有一个slave的创建命令如下：1$ruby redis-trib.rb create --replicas 1 10.180.157.199:6379 10.180.157.200:6379 10.180.157.201:6379 10.180.157.202:6379 10.180.157.205:6379 10.180.157.208:6379 创建流程如下：1、首先为每个节点创建ClusterNode对象，包括连接每个节点。检查每个节点是否为独立且db为空的节点。执行load_info方法导入节点信息。2、检查传入的master节点数量是否大于等于3个。只有大于3个节点才能组成集群。3、计算每个master需要分配的slot数量，以及给master分配slave。分配的算法大致如下：先把节点按照host分类，这样保证master节点能分配到更多的主机中。不停遍历遍历host列表，从每个host列表中弹出一个节点，放入interleaved数组。直到所有的节点都弹出为止。master节点列表就是interleaved前面的master数量的节点列表。保存在masters数组。计算每个master节点负责的slot数量，保存在slots_per_node对象，用slot总数除以master数量取整即可。遍历masters数组，每个master分配slots_per_node个slot，最后一个master，分配到16384个slot为止。接下来为master分配slave，分配算法会尽量保证master和slave节点不在同一台主机上。对于分配完指定slave数量的节点，还有多余的节点，也会为这些节点寻找master。分配算法会遍历两次masters数组。第一次遍历masters数组，在余下的节点列表找到replicas数量个slave。每个slave为第一个和master节点host不一样的节点，如果没有不一样的节点，则直接取出余下列表的第一个节点。第二次遍历是在对于节点数除以replicas不为整数，则会多余一部分节点。遍历的方式跟第一次一样，只是第一次会一次性给master分配replicas数量个slave，而第二次遍历只分配一个，直到余下的节点被全部分配出去。4、打印出分配信息，并提示用户输入“yes”确认是否按照打印出来的分配方式创建集群。5、输入“yes”后，会执行flush_nodes_config操作，该操作执行前面的分配结果，给master分配slot，让slave复制master，对于还没有握手（cluster meet）的节点，slave复制操作无法完成，不过没关系，flush_nodes_config操作出现异常会很快返回，后续握手后会再次执行flush_nodes_config。6、给每个节点分配epoch，遍历节点，每个节点分配的epoch比之前节点大1。7、节点间开始相互握手，握手的方式为节点列表的其他节点跟第一个节点握手。8、然后每隔1秒检查一次各个节点是否已经消息同步完成，使用ClusterNode的get_config_signature方法，检查的算法为获取每个节点cluster nodes信息，排序每个节点，组装成node_id1:slots|node_id2:slot2|…的字符串。如果每个节点获得字符串都相同，即认为握手成功。9、此后会再执行一次flush_nodes_config，这次主要是为了完成slave复制操作。10、最后再执行check_cluster，全面检查一次集群状态。包括和前面握手时检查一样的方式再检查一遍。确认没有迁移的节点。确认所有的slot都被分配出去了。11、至此完成了整个创建流程，返回[OK] All 16384 slots covered.。 check检查集群检查集群状态的命令，没有其他参数，只需要选择一个集群中的一个节点即可。执行命令以及结果如下：123456789101112131415161718192021222324$ruby redis-trib.rb check 10.180.157.199:6379&gt;&gt;&gt; Performing Cluster Check (using node 10.180.157.199:6379)M: b2506515b38e6bbd3034d540599f4cd2a5279ad1 10.180.157.199:6379 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d376aaf80de0e01dde1f8cd4647d5ac3317a8641 10.180.157.205:6379 slots: (0 slots) slave replicates e36c46dbe90960f30861af00786d4c2064e63df2M: 15126fb33796c2c26ea89e553418946f7443d5a5 10.180.157.201:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 59fa6ee455f58a5076f6d6f83ddd74161fd7fb55 10.180.157.208:6379 slots: (0 slots) slave replicates 15126fb33796c2c26ea89e553418946f7443d5a5S: 460b3a11e296aafb2615043291b7dd98274bb351 10.180.157.202:6379 slots: (0 slots) slave replicates b2506515b38e6bbd3034d540599f4cd2a5279ad1M: e36c46dbe90960f30861af00786d4c2064e63df2 10.180.157.200:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 检查前会先执行load_cluster_info_from_node方法，把所有节点数据load进来。load的方式为通过自己的cluster nodes发现其他节点，然后连接每个节点，并加入nodes数组。接着生成节点间的复制关系。 load完数据后，开始检查数据，检查的方式也是调用创建时候使用的check_cluster。 info查看集群信息info命令用来查看集群的信息。info命令也是先执行load_cluster_info_from_node获取完整的集群信息。然后显示ClusterNode的info_string结果，示例如下：123456$ruby redis-trib.rb info 10.180.157.199:637910.180.157.199:6379 (b2506515...) -&gt; 0 keys | 5461 slots | 1 slaves.10.180.157.201:6379 (15126fb3...) -&gt; 0 keys | 5461 slots | 1 slaves.10.180.157.200:6379 (e36c46db...) -&gt; 0 keys | 5462 slots | 1 slaves.[OK] 0 keys in 3 masters.0.00 keys per slot on average. fix修复集群fix命令的流程跟check的流程很像，显示加载集群信息，然后在check_cluster方法内传入fix为true的变量，会在集群检查出现异常的时候执行修复流程。目前fix命令能修复两种异常，一种是集群有处于迁移中的slot的节点，一种是slot未完全分配的异常。 fix_open_slot方法是修复集群有处于迁移中的slot的节点异常。1、先检查该slot是谁负责的，迁移的源节点如果没完成迁移，owner还是该节点。没有owner的slot无法完成修复功能。2、遍历每个节点，获取哪些节点标记该slot为migrating状态，哪些节点标记该slot为importing状态。对于owner不是该节点，但是通过cluster countkeysinslot获取到该节点有数据的情况，也认为该节点为importing状态。3、如果migrating和importing状态的节点均只有1个，这可能是迁移过程中redis-trib.rb被中断所致，直接执行move_slot继续完成迁移任务即可。传递dots和fix为true。4、如果migrating为空，importing状态的节点大于0，那么这种情况执行回滚流程，将importing状态的节点数据通过move_slot方法导给slot的owner节点，传递dots、fix和cold为true。接着对importing的节点执行cluster stable命令恢复稳定。5、如果importing状态的节点为空，有一个migrating状态的节点，而且该节点在当前slot没有数据，那么可以直接把这个slot设为stable。6、如果migrating和importing状态不是上述情况，目前redis-trib.rb工具无法修复，上述的三种情况也已经覆盖了通过redis-trib.rb工具迁移出现异常的各个方面，人为的异常情形太多，很难考虑完全。 fix_slots_coverage方法能修复slot未完全分配的异常。未分配的slot有三种状态。1、所有节点的该slot都没有数据。该状态redis-trib.rb工具直接采用随机分配的方式，并没有考虑节点的均衡。本人尝试对没有分配slot的集群通过fix修复集群，结果slot还是能比较平均的分配，但是没有了连续性，打印的slot信息非常离散。2、有一个节点的该slot有数据。该状态下，直接把slot分配给该slot有数据的节点。3、有多个节点的该slot有数据。此种情况目前还处于TODO状态，不过redis作者列出了修复的步骤，对这些节点，除第一个节点，执行cluster migrating命令，然后把这些节点的数据迁移到第一个节点上。清除migrating状态，然后把slot分配给第一个节点。 reshard在线迁移slotreshard命令可以在线把集群的一些slot从集群原来slot负责节点迁移到新的节点，利用reshard可以完成集群的在线横向扩容和缩容。reshard的参数很多，下面来一一解释一番：1234567reshard host:port --from &lt;arg&gt; --to &lt;arg&gt; --slots &lt;arg&gt; --yes --timeout &lt;arg&gt; --pipeline &lt;arg&gt; host:port：这个是必传参数，用来从一个节点获取整个集群信息，相当于获取集群信息的入口。–from ：需要从哪些源节点上迁移slot，可从多个源节点完成迁移，以逗号隔开，传递的是节点的node id，还可以直接传递–from all，这样源节点就是集群的所有节点，不传递该参数的话，则会在迁移过程中提示用户输入。–to ：slot需要迁移的目的节点的node id，目的节点只能填写一个，不传递该参数的话，则会在迁移过程中提示用户输入。–slots ：需要迁移的slot数量，不传递该参数的话，则会在迁移过程中提示用户输入。–yes：设置该参数，可以在打印执行reshard计划的时候，提示用户输入yes确认后再执行reshard。–timeout ：设置migrate命令的超时时间。–pipeline ：定义cluster getkeysinslot命令一次取出的key数量，不传的话使用默认值为10。 迁移的流程如下：1、通过load_cluster_info_from_node方法装载集群信息。2、执行check_cluster方法检查集群是否健康。只有健康的集群才能进行迁移。3、获取需要迁移的slot数量，用户没传递–slots参数，则提示用户手动输入。4、获取迁移的目的节点，用户没传递–to参数，则提示用户手动输入。此处会检查目的节点必须为master节点。5、获取迁移的源节点，用户没传递–from参数，则提示用户手动输入。此处会检查源节点必须为master节点。–from all的话，源节点就是除了目的节点外的全部master节点。这里为了保证集群slot分配的平均，建议传递–from all。6、执行compute_reshard_table方法，计算需要迁移的slot数量如何分配到源节点列表，采用的算法是按照节点负责slot数量由多到少排序，计算每个节点需要迁移的slot的方法为：迁移slot数量 * (该源节点负责的slot数量 / 源节点列表负责的slot总数)。这样算出的数量可能不为整数，这里代码用了下面的方式处理：12345n = (numslots/source_tot_slots*s.slots.length)if i == 0 n = n.ceilelse n = n.floor 这样的处理方式会带来最终分配的slot与请求迁移的slot数量不一致，这个BUG已经在github上提给作者，https://github.com/antirez/redis/issues/2990。7、打印出reshard计划，如果用户没传–yes，就提示用户确认计划。8、根据reshard计划，一个个slot的迁移到新节点上，迁移使用move_slot方法，该方法被很多命令使用，具体可以参见下面的迁移流程。move_slot方法传递dots为true和pipeline数量。9、至此，就完成了全部的迁移任务。下面看下一次reshard的执行结果：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465$ruby redis-trib.rb reshard --from all --to 80b661ecca260c89e3d8ea9b98f77edaeef43dcd --slots 11 10.180.157.199:6379&gt;&gt;&gt; Performing Cluster Check (using node 10.180.157.199:6379)S: b2506515b38e6bbd3034d540599f4cd2a5279ad1 10.180.157.199:6379 slots: (0 slots) slave replicates 460b3a11e296aafb2615043291b7dd98274bb351S: d376aaf80de0e01dde1f8cd4647d5ac3317a8641 10.180.157.205:6379 slots: (0 slots) slave replicates e36c46dbe90960f30861af00786d4c2064e63df2M: 15126fb33796c2c26ea89e553418946f7443d5a5 10.180.157.201:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 59fa6ee455f58a5076f6d6f83ddd74161fd7fb55 10.180.157.208:6379 slots: (0 slots) slave replicates 15126fb33796c2c26ea89e553418946f7443d5a5M: 460b3a11e296aafb2615043291b7dd98274bb351 10.180.157.202:6379 slots:0-5460 (5461 slots) master 1 additional replica(s)M: 80b661ecca260c89e3d8ea9b98f77edaeef43dcd 10.180.157.200:6380 slots: (0 slots) master 0 additional replica(s)M: e36c46dbe90960f30861af00786d4c2064e63df2 10.180.157.200:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.Ready to move 11 slots. Source nodes: M: 15126fb33796c2c26ea89e553418946f7443d5a5 10.180.157.201:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s) M: 460b3a11e296aafb2615043291b7dd98274bb351 10.180.157.202:6379 slots:0-5460 (5461 slots) master 1 additional replica(s) M: e36c46dbe90960f30861af00786d4c2064e63df2 10.180.157.200:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s) Destination node: M: 80b661ecca260c89e3d8ea9b98f77edaeef43dcd 10.180.157.200:6380 slots: (0 slots) master 0 additional replica(s) Resharding plan: Moving slot 5461 from e36c46dbe90960f30861af00786d4c2064e63df2 Moving slot 5462 from e36c46dbe90960f30861af00786d4c2064e63df2 Moving slot 5463 from e36c46dbe90960f30861af00786d4c2064e63df2 Moving slot 5464 from e36c46dbe90960f30861af00786d4c2064e63df2 Moving slot 0 from 460b3a11e296aafb2615043291b7dd98274bb351 Moving slot 1 from 460b3a11e296aafb2615043291b7dd98274bb351 Moving slot 2 from 460b3a11e296aafb2615043291b7dd98274bb351 Moving slot 10923 from 15126fb33796c2c26ea89e553418946f7443d5a5 Moving slot 10924 from 15126fb33796c2c26ea89e553418946f7443d5a5 Moving slot 10925 from 15126fb33796c2c26ea89e553418946f7443d5a5Do you want to proceed with the proposed reshard plan (yes/no)? yesMoving slot 5461 from 10.180.157.200:6379 to 10.180.157.200:6380:Moving slot 5462 from 10.180.157.200:6379 to 10.180.157.200:6380:Moving slot 5463 from 10.180.157.200:6379 to 10.180.157.200:6380:Moving slot 5464 from 10.180.157.200:6379 to 10.180.157.200:6380:Moving slot 0 from 10.180.157.202:6379 to 10.180.157.200:6380:Moving slot 1 from 10.180.157.202:6379 to 10.180.157.200:6380:Moving slot 2 from 10.180.157.202:6379 to 10.180.157.200:6380:Moving slot 10923 from 10.180.157.201:6379 to 10.180.157.200:6380:Moving slot 10924 from 10.180.157.201:6379 to 10.180.157.200:6380:Moving slot 10925 from 10.180.157.201:6379 to 10.180.157.200:6380: move_slot方法可以在线将一个slot的全部数据从源节点迁移到目的节点，fix、reshard、rebalance都需要调用该方法迁移slot。move_slot接受下面几个参数，1、pipeline：设置一次从slot上获取多少个key。2、quiet：迁移会打印相关信息，设置quiet参数，可以不用打印这些信息。3、cold：设置cold，会忽略执行importing和migrating。4、dots：设置dots，则会在迁移过程打印迁移key数量的进度。5、update：设置update，则会更新内存信息，方便以后的操作。 move_slot流程如下：1、如果没有设置cold，则对源节点执行cluster importing命令，对目的节点执行migrating命令。fix的时候有可能importing和migrating已经执行过来，所以此种场景会设置cold。2、通过cluster getkeysinslot命令，一次性获取远节点迁移slot的pipeline个key的数量.3、对这些key执行migrate命令，将数据从源节点迁移到目的节点。4、如果migrate出现异常，在fix模式下，BUSYKEY的异常，会使用migrate的replace模式再执行一次，BUSYKEY表示目的节点已经有该key了，replace模式可以强制替换目的节点的key。不是fix模式就直接返回错误了。5、循环执行cluster getkeysinslot命令，直到返回的key数量为0，就退出循环。6、如果没有设置cold，对每个节点执行cluster setslot命令，把slot赋给目的节点。7、如果设置update，则修改源节点和目的节点的slot信息。8、至此完成了迁移slot的流程。 rebalance平衡集群节点slot数量rebalance命令可以根据用户传入的参数平衡集群节点的slot数量，rebalance功能非常强大，可以传入的参数很多，以下是rebalance的参数列表和命令示例。12345678rebalance host:port --weight &lt;arg&gt; --auto-weights --threshold &lt;arg&gt; --use-empty-masters --timeout &lt;arg&gt; --simulate --pipeline &lt;arg&gt; 1$ruby redis-trib.rb rebalance --threshold 1 --weight b31e3a2e=5 --weight 60b8e3a1=5 --use-empty-masters --simulate 10.180.157.199:6379 下面也先一一解释下每个参数的用法：host:port：这个是必传参数，用来从一个节点获取整个集群信息，相当于获取集群信息的入口。–weight ：节点的权重，格式为node_id=weight，如果需要为多个节点分配权重的话，需要添加多个–weight 参数，即–weight b31e3a2e=5 –weight 60b8e3a1=5，node_id可为节点名称的前缀，只要保证前缀位数能唯一区分该节点即可。没有传递–weight的节点的权重默认为1。–auto-weights：这个参数在rebalance流程中并未用到。–threshold ：只有节点需要迁移的slot阈值超过threshold，才会执行rebalance操作。具体计算方法可以参考下面的rebalance命令流程的第四步。–use-empty-masters：rebalance是否考虑没有节点的master，默认没有分配slot节点的master是不参与rebalance的，设置–use-empty-masters可以让没有分配slot的节点参与rebalance。–timeout ：设置migrate命令的超时时间。–simulate：设置该参数，可以模拟rebalance操作，提示用户会迁移哪些slots，而不会真正执行迁移操作。–pipeline ：与reshar的pipeline参数一样，定义cluster getkeysinslot命令一次取出的key数量，不传的话使用默认值为10。 rebalance命令流程如下：1、load_cluster_info_from_node方法先加载集群信息。2、计算每个master的权重，根据参数–weight ，为每个设置的节点分配权重，没有设置的节点，则权重默认为1。3、根据每个master的权重，以及总的权重，计算自己期望被分配多少个slot。计算的方式为：总slot数量 （自己的权重 / 总权重）。4、计算每个master期望分配的slot是否超过设置的阈值，即–threshold 设置的阈值或者默认的阈值。计算的方式为：先计算期望移动节点的阈值，算法为：(100-(100.0expected/n.slots.length)).abs，如果计算出的阈值没有超出设置阈值，则不需要为该节点移动slot。只要有一个master的移动节点超过阈值，就会触发rebalance操作。5、如果触发了rebalance操作。那么就开始执行rebalance操作，先将每个节点当前分配的slots数量减去期望分配的slot数量获得balance值。将每个节点的balance从小到大进行排序获得sn数组。6、用dst_idx和src_idx游标分别从sn数组的头部和尾部开始遍历。目的是为了把尾部节点的slot分配给头部节点。 sn数组保存的balance列表排序后，负数在前面，正数在后面。负数表示需要有slot迁入，所以使用dst_idx游标，正数表示需要有slot迁出，所以使用src_idx游标。理论上sn数组各节点的balance值加起来应该为0，不过由于在计算期望分配的slot的时候只是使用直接取整的方式，所以可能出现balance值之和不为0的情况，balance值之和不为0即为节点不平衡的slot数量，由于slot总数有16384个，不平衡数量相对于总数，基数很小，所以对rebalance流程影响不大。 7、获取sn[dst_idx]和sn[src_idx]的balance值较小的那个值，该值即为需要从sn[src_idx]节点迁移到sn[dst_idx]节点的slot数量。8、接着通过compute_reshard_table方法计算源节点的slot如何分配到源节点列表。这个方法在reshard流程中也有调用，具体步骤可以参考reshard流程的第六步。9、如果是simulate模式，则只是打印出迁移列表。10、如果没有设置simulate，则执行move_slot操作，迁移slot，传入的参数为:quiet=&gt;true,:dots=&gt;false,:update=&gt;true。11、迁移完成后更新sn[dst_idx]和sn[src_idx]的balance值。如果balance值为0后，游标向前进1。12、直到dst_idx到达src_idx游标，完成整个rebalance操作。 add-node将新节点加入集群add-node命令可以将新节点加入集群，节点可以为master，也可以为某个master节点的slave。123add-node new_host:new_port existing_host:existing_port --slave --master-id &lt;arg&gt; add-node有两个可选参数：–slave：设置该参数，则新节点以slave的角色加入集群–master-id：这个参数需要设置了–slave才能生效，–master-id用来指定新节点的master节点。如果不设置该参数，则会随机为节点选择master节点。可以看下add-node命令的执行示例：1234567891011121314151617181920$ruby redis-trib.rb add-node --slave --master-id dcb792b3e85726f012e83061bf237072dfc45f99 10.180.157.202:6379 10.180.157.199:6379&gt;&gt;&gt; Adding node 10.180.157.202:6379 to cluster 10.180.157.199:6379&gt;&gt;&gt; Performing Cluster Check (using node 10.180.157.199:6379)M: dcb792b3e85726f012e83061bf237072dfc45f99 10.180.157.199:6379 slots:0-5460 (5461 slots) master 0 additional replica(s)M: 464d740bf48953ebcf826f4113c86f9db3a9baf3 10.180.157.201:6379 slots:10923-16383 (5461 slots) master 0 additional replica(s)M: befa7e17b4e5f239e519bc74bfef3264a40f96ae 10.180.157.200:6379 slots:5461-10922 (5462 slots) master 0 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 10.180.157.202:6379 to make it join the cluster.Waiting for the cluster to join.&gt;&gt;&gt; Configure node as replica of 10.180.157.199:6379.[OK] New node added correctly. add-node流程如下：1、通过load_cluster_info_from_node方法转载集群信息，check_cluster方法检查集群是否健康。2、如果设置了–slave，则需要为该节点寻找master节点。设置了–master-id，则以该节点作为新节点的master，如果没有设置–master-id，则调用get_master_with_least_replicas方法，寻找slave数量最少的master节点。如果slave数量一致，则选取load_cluster_info_from_node顺序发现的第一个节点。load_cluster_info_from_node顺序的第一个节点是add-node设置的existing_host:existing_port节点，后面的顺序根据在该节点执行cluster nodes返回的结果返回的节点顺序。3、连接新的节点并与集群第一个节点握手。4、如果没设置–slave就直接返回ok，设置了–slave，则需要等待确认新节点加入集群，然后执行cluster replicate命令复制master节点。5、至此，完成了全部的增加节点的流程。 del-node从集群中删除节点del-node可以把某个节点从集群中删除。del-node只能删除没有分配slot的节点。删除命令传递两个参数：host:port：从该节点获取集群信息。node_id：需要删除的节点id。del-node执行结果示例如下：1234$ruby redis-trib.rb del-node 10.180.157.199:6379 d5f6d1d17426bd564a6e309f32d0f5b96962fe53&gt;&gt;&gt; Removing node d5f6d1d17426bd564a6e309f32d0f5b96962fe53 from cluster 10.180.157.199:6379&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; SHUTDOWN the node. del-node流程如下：1、通过load_cluster_info_from_node方法转载集群信息。2、根据传入的node id获取节点，如果节点没找到，则直接提示错误并退出。3、如果节点分配的slot不为空，则直接提示错误并退出。4、遍历集群内的其他节点，执行cluster forget命令，从每个节点中去除该节点。如果删除的节点是master，而且它有slave的话，这些slave会去复制其他master，调用的方法是get_master_with_least_replicas，与add-node没设置–master-id寻找master的方法一样。5、然后关闭该节点 set-timeout设置集群节点间心跳连接的超时时间set-timeout用来设置集群节点间心跳连接的超时时间，单位是毫秒，不得小于100毫秒，因为100毫秒对于心跳时间来说太短了。该命令修改是节点配置参数cluster-node-timeout，默认是15000毫秒。通过该命令，可以给每个节点设置超时时间，设置的方式使用config set命令动态设置，然后执行config rewrite命令将配置持久化保存到硬盘。以下是示例：12345678ruby redis-trib.rb set-timeout 10.180.157.199:6379 30000&gt;&gt;&gt; Reconfiguring node timeout in every cluster node...*** New timeout set for 10.180.157.199:6379*** New timeout set for 10.180.157.205:6379*** New timeout set for 10.180.157.201:6379*** New timeout set for 10.180.157.200:6379*** New timeout set for 10.180.157.208:6379&gt;&gt;&gt; New node timeout set. 5 OK, 0 ERR. call在集群全部节点上执行命令call命令可以用来在集群的全部节点执行相同的命令。call命令也是需要通过集群的一个节点地址，连上整个集群，然后在集群的每个节点执行该命令。1234567$ruby redis-trib.rb call 10.180.157.199:6379 get key&gt;&gt;&gt; Calling GET key10.180.157.199:6379: MOVED 12539 10.180.157.201:637910.180.157.205:6379: MOVED 12539 10.180.157.201:637910.180.157.201:6379:10.180.157.200:6379: MOVED 12539 10.180.157.201:637910.180.157.208:6379: MOVED 12539 10.180.157.201:6379 import将外部redis数据导入集群import命令可以把外部的redis节点数据导入集群。导入的流程如下：1、通过load_cluster_info_from_node方法转载集群信息，check_cluster方法检查集群是否健康。2、连接外部redis节点，如果外部节点开启了cluster_enabled，则提示错误。3、通过scan命令遍历外部节点，一次获取1000条数据。4、遍历这些key，计算出key对应的slot。5、执行migrate命令,源节点是外部节点,目的节点是集群slot对应的节点，如果设置了–copy参数，则传递copy参数，如果设置了–replace，则传递replace参数。6、不停执行scan命令，直到遍历完全部的key。7、至此完成整个迁移流程这中间如果出现异常，程序就会停止。没使用–copy模式，则可以重新执行import命令，使用–copy的话，最好清空新的集群再导入一次。 import命令更适合离线的把外部Redis数据导入，在线导入的话最好使用更专业的导入工具，以slave的方式连接redis节点去同步节点数据应该是更好的方式。 下面是一个例子12./redis-trib.rb import --from 10.0.10.1:6379 10.10.10.1:7000上面的命令是把 10.0.10.1:6379（redis 2.8）上的数据导入到 10.10.10.1:7000这个节点所在的集群 原文地址http://blog.csdn.net/huwei2003/article/details/50973967]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM监控与调优]]></title>
    <url>%2F2015%2F09%2F12%2Fjava-jvm-monitor-optimization%2F</url>
    <content type="text"><![CDATA[JVM监控与调优主要的着眼点在于如何配置、如何监控、如何优化3点上。下面就将针对这3点进行学习 参数设置在Java虚拟机的参数中，有3种表示方法,用ps -ef |grep java命令，可以得到当前Java进程的所有启动参数和配置参数： 标准参数（-），所有的JVM实现都必须实现这些参数的功能，而且向后兼容； 非标准参数（-X），默认jvm实现这些参数的功能，但是并不保证所有jvm实现都满足，且不保证向后兼容； 非Stable参数（-XX），此类参数各个jvm实现会有所不同，将来可能会随时取消，需要慎重使用（但是，这些参数往往是非常有用的）; 标准参数其实标准参数是用过Java的人都最熟悉的，就是你在运行java命令时后面加上的参数，如java -version, java -jar 等，输入命令java -help或java -?就能获得当前机器所有java的标准参数列表。-client设置jvm使用client模式，这是一般在pc机器上使用的模式，启动很快，但性能和内存管理效率并不高；多用于桌面应用；-server使用server模式，启动速度虽然慢（比client模式慢10%左右），但是性能和内存管理效率很高，适用于服务器，用于生成环境、开发环境或测试环境的服务端；如果没有指定-server或-client，JVM启动的时候会自动检测当前主机是否为服务器，如果是就以server模式启动，64位的JVM只有server模式，所以无法使用-client参数；默认情况下，不同的启动模式，执行GC的方式有所区别： 启动模式 新生代GC方式 旧生代和持久代GC的方式 client 串行 串行 server 并行 并发 如果没有指定-server或-client模式，则判断方法如下：-classpath / -cpJVM加载和搜索文件的目录路径，多个路径用;分隔。注意，如果使用了-classpath，JVM就不会再搜索环境变量中定义的CLASSPATH路径。JVM搜索路径的顺序为：1.先搜索JVM自带的jar或zip包（Bootstrat，搜索路径可以用System.getProperty(“sun.boot.class.path”)获得）；2.搜索JRE_HOME/lib/ext下的jar包（Extension，搜索路径可以用System.getProperty(“java.ext.dirs”)获得）；3.搜索用户自定义目录，顺序为：当前目录（.），CLASSPATH，-cp；（搜索路径用System.getProperty(“java.class.path”)获得） -DpropertyName=value定义系统的全局属性值，如配置文件地址等，如果value有空格，可以用-Dname=”space string”这样的形式来定义，用System.getProperty(“propertyName”)可以获得这些定义的属性值，在代码中也可以用System.setProperty(“propertyName”,”value”)的形式来定义属性。 -verbose这是查询GC问题最常用的命令之一，具体参数如：-verbose:class 输出jvm载入类的相关信息，当jvm报告说找不到类或者类冲突时可此进行诊断。-verbose:gc 输出每次GC的相关情况，后面会有更详细的介绍。-verbose:jni 输出native方法调用的相关情况，一般用于诊断jni调用错误信息。 非标准参数非标准参数，是在标准参数的基础上进行扩展的参数，输入java -X命令，能够获得当前JVM支持的所有非标准参数列表（你会发现，其实并不多哦）。在不同类型的JVM中，采用的参数有所不同，在讲解非标准参数时，请参考下面的图，对内存区域的大小有个形象的了解-Xmn新生代内存大小的最大值，包括E区和两个S区的总和，使用方法如：-Xmn65535，-Xmn1024k，-Xmn512m，-Xmn1g (-Xms,-Xmx也是种写法)-Xmn只能使用在JDK1.4或之后的版本中，（之前的1.3/1.4版本中，可使用-XX:NewSize设置年轻代大小，用-XX:MaxNewSize设置年轻代最大值）；如果同时设置了-Xmn和-XX:NewSize，-XX:MaxNewSize，则谁设置在后面，谁就生效；如果同时设置了-XX:NewSize -XX:MaxNewSize与-XX:NewRatio则实际生效的值是：min(MaxNewSize,max(NewSize, heap/(NewRatio+1)))在开发、测试环境，可以-XX:NewSize和-XX:MaxNewSize来设置新生代大小，但在线上生产环境，使用-Xmn一个即可（推荐），或者将-XX:NewSize和-XX:MaxNewSize设置为同一个值，这样能够防止在每次GC之后都要调整堆的大小（即：抖动，抖动会严重影响性能） -Xms初始堆的大小，也是堆大小的最小值，默认值是总共的物理内存/64（且小于1G），默认情况下，当堆中可用内存小于40%(这个值可以用-XX: MinHeapFreeRatio调整，如-X:MinHeapFreeRatio=30)时，堆内存会开始增加，一直增加到-Xmx的大小； -Xmx堆的最大值，默认值是总共的物理内存/64（且小于1G），如果-Xms和-Xmx都不设置，则两者大小会相同，默认情况下，当堆中可用内存大于70%（这个值可以用-XX:MaxHeapFreeRatio调整，如-X:MaxHeapFreeRatio=60)时，堆内存会开始减少，一直减小到-Xms的大小；整个堆的大小=年轻代大小+年老代大小，堆的大小不包含持久代大小，如果增大了年轻代，年老代相应就会减小，官方默认的配置为年老代大小/年轻代大小=2/1左右（使用-XX:NewRatio可以设置-XX:NewRatio=5，表示年老代/年轻代=5/1）；建议在开发测试环境可以用-Xms和-Xmx分别设置最小值最大值，但是在线上生产环境，-Xms和-Xmx设置的值必须一样，原因与年轻代一样——防止抖动； -Xss这个参数用于设置每个线程的栈内存，默认1M，一般来说是不需要改的。除非代码不多，可以设置的小点，另外一个相似的参数是-XX:ThreadStackSize，这两个参数在1.6以前，都是谁设置在后面，谁就生效；1.6版本以后，-Xss设置在后面，则以-Xss为准，-XXThreadStackSize设置在后面，则主线程以-Xss为准，其它线程以-XX:ThreadStackSize -Xrs减少JVM对操作系统信号（OS Signals）的使用（JDK1.3.1之后才有效），当此参数被设置之后，jvm将不接收控制台的控制handler，以防止与在后台以服务形式运行的JVM冲突（这个用的比较少）。 -Xprof跟踪正运行的程序，并将跟踪数据在标准输出输出；适合于开发环境调试。 -Xnoclassgc关闭针对class的gc功能；因为其阻止内存回收，所以可能会导致OutOfMemoryError错误，慎用； -Xincgc开启增量gc（默认为关闭）；这有助于减少长时间GC时应用程序出现的停顿；但由于可能和应用程序并发执行，所以会降低CPU对应用的处理能力。 -Xloggc:file与-verbose:gc功能类似，只是将每次GC事件的相关情况记录到一个文件中，文件的位置最好在本地，以避免网络的潜在问题。若与verbose命令同时出现在命令行中，则以-Xloggc为准。 非Stable参数（非静态参数）以-XX表示的非Stable参数，虽然在官方文档中是不确定的，不健壮的，各个公司的实现也各有不同，但往往非常实用，所以这部分参数对于GC非常重要。JVM（Hotspot）中主要的参数可以大致分为3类 性能参数（Performance Options）：用于JVM的性能调优和内存分配控制，如初始化内存大小的设置； 行为参数（Behavioral Options）：用于改变JVM的基础行为，如GC的方式和算法的选择； 调试参数（Debugging Options）：用于监控、打印、输出等jvm参数，用于显示jvm更加详细的信息； 对于非Stable参数，使用方法有4种： -XX:+[option] 启用选项 -XX:-[option] 不启用选项 -XX:[option]=[number] 给选项设置一个数字类型值，可跟单位，例如 32k, 1024m, 2g -XX:[option]=[string] 给选项设置一个字符串值，例如-XX:HeapDumpPath=./dump.core 首先介绍性能参数，性能参数往往用来定义内存分配的大小和比例，相比于行为参数和调试参数，一个比较明显的区别是性能参数后面往往跟的有数值，常用如下： 参数及其默认值 描述 -XX:NewSize=2.125m 新生代对象生成时占用内存的默认值 -XX:MaxNewSize=size 新生成对象能占用内存的最大值 -XX:MaxPermSize=64m 方法区所能占用的最大内存（非堆内存） -XX:PermSize=64m 方法区分配的初始内存 -XX:MaxTenuringThreshold=15 对象在新生代存活区切换的次数（坚持过MinorGC的次数，每坚持过一次，该值就增加1），大于该值会进入老年代 -XX:MaxHeapFreeRatio=70 GC后java堆中空闲量占的最大比例，大于该值，则堆内存会减少 -XX:MinHeapFreeRatio=40 GC后java堆中空闲量占的最小比例，小于该值，则堆内存会增加 -XX:NewRatio=2 新生代内存容量与老生代内存容量的比例 -XX:ReservedCodeCacheSize= 32m 保留代码占用的内存容量 -XX:ThreadStackSize=512 设置线程栈大小，若为0则使用系统默认值 -XX:LargePageSizeInBytes=4m 设置用于Java堆的大页面尺寸 -XX:PretenureSizeThreshold= size 大于该值的对象直接晋升入老年代（这种对象少用为好） -XX:SurvivorRatio=8 Eden区域Survivor区的容量比值，如默认值为8，代表Eden：Survivor1：Survivor2=8:1:1 常用的行为参数，主要用来选择使用什么样的垃圾收集器组合，以及控制运行过程中的GC策略等： 参数及其默认值 描述 -XX:+UseSerialGC 启用串行GC，即采用Serial+Serial Old模式 -XX:+UseParallelGC 启用并行GC，即采用Parallel Scavenge+Serial Old收集器组合（-Server模式下的默认组合） -XX:GCTimeRatio=99 设置用户执行时间占总时间的比例（默认值99，即1%的时间用于GC） -XX:MaxGCPauseMillis=time 设置GC的最大停顿时间（这个参数只对Parallel Scavenge有效） -XX:+UseParNewGC 使用ParNew+Serial Old收集器组合 -XX:ParallelGCThreads 设置执行内存回收的线程数，在+UseParNewGC的情况下使用 -XX:+UseParallelOldGC 使用Parallel Scavenge +Parallel Old组合收集器 -XX:+UseConcMarkSweepGC 使用ParNew+CMS+Serial Old组合并发收集，优先使用ParNew+CMS，当用户线程内存不足时，采用备用方案Serial Old收集。 -XX:+DisableExplicitGC 禁止调用System.gc()；但jvm的gc仍然有效 -XX:+ScavengeBeforeFullGC 新生代GC优先于Full GC执行 常用的调试参数，主要用于监控和打印GC的信息： 参数及其默认值 描述 -XX:+CITime 打印消耗在JIT编译的时间 -XX:ErrorFile=./hs_err_pid[pid].log 保存错误日志或者数据到文件中 -XX:+ExtendedDTraceProbes 开启solaris特有的dtrace探针 -XX:HeapDumpPath=./java_pid[pid].hprof 指定导出堆信息时的路径或文件名 -XX:+HeapDumpOnOutOfMemoryError 当首次遭遇OOM时导出此时堆中相关信息 -XX:OnError=”[cmd args&gt;];[cmd args]” 出现致命ERROR之后运行自定义命令 -XX:OnOutOfMemoryError=”[cmd args];[cmd args]” 当首次遭遇OOM时执行自定义命令 -XX:+PrintClassHistogram 遇到Ctrl-Break后打印类实例的柱状信息，与jmap -histo功能相同 -XX:+PrintConcurrentLocks 遇到Ctrl-Break后打印并发锁的相关信息，与jstack -l功能相同 -XX:+PrintCommandLineFlags 打印在命令行中出现过的标记 -XX:+PrintCompilation 当一个方法被编译时打印相关信息 -XX:+PrintGC 每次GC时打印相关信息 -XX:+PrintGC Details 每次GC时打印详细信息 -XX:+PrintGCTimeStamps 打印每次GC的时间戳 -XX:+TraceClassLoading 跟踪类的加载信息 -XX:+TraceClassLoadingPreorder 跟踪被引用到的所有类的加载信息 -XX:+TraceClassResolution 跟踪常量池 -XX:+TraceClassUnloading 跟踪类的卸载信息 -XX:+TraceLoaderConstraints 跟踪类加载器约束的相关信息 这些参数将为我们进行GC的监控与调优提供很大助力，是我们进行GC相关操作的重要工具。 收集器搭配在介绍了常用的配置参数之后，我们将开始真正的JVM实操征程，首先，我们要为应用程序选择一个合适的垃圾收集器组合，及上节中的行为参数。图中两个收集器之间有连线，说明它们可以配合使用新生代收集器如下：Serial收集器： Serial收集器是在client模式下默认的新生代收集器，其收集效率大约是100M左右的内存需要几十到100多毫秒；在client模式下，收集桌面应用的内存垃圾，基本上不影响用户体验。所以，一般的Java桌面应用中，直接使用Serial收集器（不需要配置参数，用默认即可）。ParNew收集器：Serial收集器的多线程版本，这种收集器默认开通的线程数与CPU数量相同，-XX:ParallelGCThreads可以用来设置开通的线程数。可以与CMS收集器配合使用，事实上用-XX:+UseConcMarkSweepGC选择使用CMS收集器时，默认使用的就是ParNew收集器，所以不需要额外设置-XX:+UseParNewGC，设置了也不会冲突，因为会将ParNew+Serial Old作为一个备选方案；如果单独使用-XX:+UseParNewGC参数，则选择的是ParNew+Serial Old收集器组合收集器。一般情况下，在server模式下，如果选择CMS收集器，则优先选择ParNew收集器。Parallel Scavenge收集器：关注的是吞吐量，意味着强调任务更快的完成，而如CMS等关注停顿时间短的收集器，强调的是用户交互体验。在需要关注吞吐量的场合，比如数据运算服务器等，就可以使用Parallel Scavenge收集器。 老年代收集器如下：Serial Old收集器：在1.5版本及以前可以与 Parallel Scavenge结合使用（事实上，也是当时Parallel Scavenge唯一能用的版本），另外就是在使用CMS收集器时的备用方案，发生 Concurrent Mode Failure时使用。如果是单独使用，Serial Old一般用在client模式中。Parallel Old收集器：在1.6版本之后，与 Parallel Scavenge结合使用，以更好的贯彻吞吐量优先的思想，如果是关注吞吐量的服务器，建议使用Parallel Scavenge + Parallel Old 收集器。CMS收集器：这是当前阶段使用很广的一种收集器，国内很多大的互联网公司线上服务器都使用这种垃圾收集器,CMS收集器以获取最短回收停顿时间为目标，非常适合对用户响应比较高的B/S架构服务器。CMSIncrementalMode： CMS收集器变种，属增量式垃圾收集器，在并发标记和并发清理时交替运行垃圾收集器和用户线程。G1 收集器：面向服务器端应用的垃圾收集器，计划未来替代CMS收集器。 一般来说，如果是Java桌面应用，建议采用Serial+Serial Old收集器组合，即：-XX:+UseSerialGC（-client下的默认参数） 在开发/测试环境，可以采用默认参数，即采用Parallel Scavenge+Serial Old收集器组合，即：-XX:+UseParallelGC（-server下的默认参数） 在线上运算优先的环境，建议采用Parallel Scavenge+Serial Old收集器组合，即：-XX:+UseParallelGC 在线上服务响应优先的环境，建议采用ParNew+CMS+Serial Old收集器组合，即：-XX:+UseConcMarkSweepGC 另外在选择了垃圾收集器组合之后，还要配置一些辅助参数，以保证收集器可以更好的工作 选用了ParNew收集器，你可能需要配置4个参数： -XX:SurvivorRatio, -XX:PretenureSizeThreshold, -XX:+HandlePromotionFailure,-XX:MaxTenuringThreshold； 选用了 Parallel Scavenge收集器，你可能需要配置3个参数： -XX:MaxGCPauseMillis，-XX:GCTimeRatio， -XX:+UseAdaptiveSizePolicy ； 选用了CMS收集器，你可能需要配置3个参数： -XX:CMSInitiatingOccupancyFraction， -XX:+UseCMSCompactAtFullCollection, -XX:CMSFullGCsBeforeCompaction； 启动内存分配关于GC有一个常见的疑问是，在启动时，我的内存如何分配？经过前面的学习，已经很容易知道，用-Xmn，-Xmx，-Xms，-Xss，-XX:NewSize，-XX:MaxNewSize，-XX:MaxPermSize，-XX:PermSize，-XX:SurvivorRatio，-XX:PretenureSizeThreshold，-XX:MaxTenuringThreshold就基本可以配置内存启动时的分配情况。但是，具体配置多少？设置小了，频繁GC（甚至内存溢出），设置大了，内存浪费。结合前面对于内存区域和其作用的学习，尽量考虑如下建议： -XX:PermSize尽量比-XX:MaxPermSize小，-XX:MaxPermSize&gt;= 2 * -XX:PermSize, -XX:PermSize&gt; 64m，一般对于4G内存的机器，-XX:MaxPermSize不会超过256m； -Xms = -Xmx（线上Server模式），以防止抖动，大小受操作系统和内存大小限制，如果是32位系统，则一般-Xms设置为1g-2g（假设有4g内存），在64位系统上，没有限制，不过一般为机器最大内存的一半左右； -Xmn，在开发环境下，可以用-XX:NewSize和-XX:MaxNewSize来设置新生代的大小（-XX:NewSize&lt;=-XX:MaxNewSize），在生产环境，建议只设置-Xmn，一般-Xmn的大小是-Xms的1/2左右，不要设置的过大或过小，过大导致老年代变小，频繁Full GC，过小导致minor GC频繁。如果不设置-Xmn，可以采用-XX:NewRatio=2来设置，也是一样的效果； -Xss一般是不需要改的，默认值即可。 -XX:SurvivorRatio一般设置8-10左右，推荐设置为10，也即：Survivor区的大小是Eden区的1/10，一般来说，普通的Java程序应用，一次minorGC后，至少98%-99%的对象，都会消亡，所以，survivor区设置为Eden区的1/10左右，能使Survivor区容纳下10-20次的minor GC才满，然后再进入老年代，这个与 -XX:MaxTenuringThreshold的默认值15次也相匹配的。如果XX:SurvivorRatio设置的太小，会导致本来能通过minor回收掉的对象提前进入老年代，产生不必要的full gc；如果XX:SurvivorRatio设置的太大，会导致Eden区相应的被压缩。 -XX:MaxTenuringThreshold默认为15，也就是说，经过15次Survivor轮换（即15次minor GC），就进入老年代， 如果设置的小的话，则年轻代对象在survivor中存活的时间减小，提前进入年老代，对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象在年轻代的存活时间，增加在年轻代即被回收的概率。需要注意的是，设置了 -XX:MaxTenuringThreshold，并不代表着，对象一定在年轻代存活15次才被晋升进入老年代，它只是一个最大值，事实上，存在一个动态计算机制，计算每次晋入老年代的阈值，取阈值和MaxTenuringThreshold中较小的一个为准。 -XX:PretenureSizeThreshold一般采用默认值即可。 监控工具和方法在JVM运行的过程中，为保证其稳定、高效，或在出现GC问题时分析问题原因，我们需要对GC进行监控。所谓监控，其实就是分析清楚当前GC的情况。其目的是鉴别JVM是否在高效的进行垃圾回收，以及有没有必要进行调优。通过监控GC，我们可以搞清楚很多问题，如： minor GC和full GC的频率； 执行一次GC所消耗的时间； 新生代的对象何时被移到老生代以及花费了多少时间； 每次GC中，其它线程暂停（Stop the world）的时间； 每次GC的效果如何，是否不理想； 监控GC的工具分为2种：命令行工具和图形工具；常用的命令行工具有：注：下面的命令都在JAVA_HOME/bin中，是java自带的命令。如果您发现无法使用，请直接进入Java安装目录调用或者先设置Java的环境变量，一个简单的办法为：直接运行命令 export PATH=$JAVA_HOME/bin:$PATH；另外，一般的，在Linux下，下面的命令需要sudo权限，在windows下，部分命令的部分选项不能使用。还是无法使用，要保证你装的是否是jdk还是jre，jre是不带很多工具的。 jpsjps命令用于查询正在运行的JVM进程，常用的参数为： -q:只输出LVMID，省略主类的名称 -m:输出虚拟机进程启动时传给主类main()函数的参数 -l:输出主类的全类名，如果进程执行的是Jar包，输出Jar路径 -v:输出虚拟机进程启动时JVM参数命令格式:jps [option] [hostid]一个简单的例子：在上图中，有一个vid为309的apache进程在提供web服务。 jstatjstat可以实时显示本地或远程JVM进程中类装载、内存、垃圾收集、JIT编译等数据（如果要显示远程JVM信息，需要远程主机开启RMI支持）。如果在服务启动时没有指定启动参数-verbose:gc，则可以用jstat实时查看gc情况。jstat有如下选项： -class:监视类装载、卸载数量、总空间及类装载所耗费的时间 -gc:监听Java堆状况，包括Eden区、两个Survivor区、老年代、永久代等的容量，以用空间、GC时间合计等信息 -gccapacity:监视内容与-gc基本相同，但输出主要关注java堆各个区域使用到的最大和最小空间 -gcutil:监视内容与-gc基本相同，但输出主要关注已使用空间占总空间的百分比 -gccause:与-gcutil功能一样，但是会额外输出导致上一次GC产生的原因 -gcnew:监视新生代GC状况 -gcnewcapacity:监视内同与-gcnew基本相同，输出主要关注使用到的最大和最小空间 -gcold:监视老年代GC情况 -gcoldcapacity:监视内同与-gcold基本相同，输出主要关注使用到的最大和最小空间 -gcpermcapacity:输出永久代使用到最大和最小空间 -compiler:输出JIT编译器编译过的方法、耗时等信息 -printcompilation:输出已经被JIT编译的方法命令格式:jstat [option vmid [interval[s|ms] [count]]]jstat可以监控远程机器，命令格式中VMID和LVMID特别说明：如果是本地虚拟机进程，VMID和LVMID是一致的，如果是远程虚拟机进程，那么VMID格式是: [protocol:][//]lvmid[@hostname[:port]/servername]，如果省略interval和count，则只查询一次查看gc情况的例子：在图中，命令sudo jstat -gc 309 1000 5代表着：搜集vid为309的java进程的整体gc状态， 每1000ms收集一次，共收集5次；XXXC表示该区容量，XXXU表示该区使用量，各列解释如下：S0C：S0区容量（S1区相同，略）S0U：S0区已使用EC：E区容量EU：E区已使用OC：老年代容量OU：老年代已使用PC：Perm容量PU：Perm区已使用YGC：Young GC（Minor GC）次数YGCT：Young GC总耗时FGC：Full GC次数FGCT：Full GC总耗时GCT：GC总耗时 用gcutil查看内存的例子：图中的各列与用gc参数时基本一致，不同的是这里显示的是已占用的百分比，如S0为86.53，代表着S0区已使用了86.53% jinfo用于查询当前运行这的JVM属性和参数的值。jinfo可以使用如下选项： -flag:显示未被显示指定的参数的系统默认值 -flag [+|-]name或-flag name=value: 修改部分参数 -sysprops:打印虚拟机进程的System.getProperties() 命令格式:jinfo [option] pid jmap用于显示当前Java堆和永久代的详细信息（如当前使用的收集器，当前的空间使用率等） -dump:生成java堆转储快照 -heap:显示java堆详细信息(只在Linux/Solaris下有效) -F:当虚拟机进程对-dump选项没有响应时，可使用这个选项强制生成dump快照(只在Linux/Solaris下有效) -finalizerinfo:显示在F-Queue中等待Finalizer线程执行finalize方法的对象(只在Linux/Solaris下有效) -histo:显示堆中对象统计信息 -permstat:以ClassLoader为统计口径显示永久代内存状态(只在Linux/Solaris下有效) 命令格式:jmap [option] vmid其中前面3个参数最重要，如：查看对详细信息：sudo jmap -heap 309生成dump文件： sudo jmap -dump:file=./test.prof 309部分用户没有权限时，采用admin用户：sudo -u admin -H jmap -dump:format=b,file=文件名.hprof pid查看当前堆中对象统计信息：sudo jmap -histo 309：该命令显示3列，分别为对象数量，对象大小，对象名称，通过该命令可以查看是否内存中有大对象；有的用户可能没有jmap权限：sudo -u admin -H jmap -histo 309 | less jhat用于分析使用jmap生成的dump文件，是JDK自带的工具，使用方法为：jhat -J -Xmx512m [file]不过jhat没有mat好用，推荐使用mat（Eclipse插件： http://www.eclipse.org/mat ），mat速度更快，而且是图形界面。 jstack用于生成当前JVM的所有线程快照，线程快照是虚拟机每一条线程正在执行的方法,目的是定位线程出现长时间停顿的原因。 -F:当正常输出的请求不被响应时，强制输出线程堆栈 -l:除堆栈外，显示关于锁的附加信息 -m:如果调用到本地方法的话，可以显示C/C++的堆栈命令格式:jstack [option] vmid -verbosegc-verbosegc是一个比较重要的启动参数，记录每次gc的日志，下面的表格对比了jstat和-verbosegc： jstat -verbosegc 监控对象 运行在本机的Java应用可以把日志输出到终端上，或者借助jstatd命令通过网络连接远程的Java应用。 只有那些把-verbogc作为启动参数的JVM。 输出信息 堆状态（已用空间，最大限制，GC执行次数/时间，等等） 执行GC前后新生代和老年代空间大小，GC执行时间。 输出时间 Every designated time每次设定好的时间。 每次GC发生的时候。 用途 观察堆空间变化情况 了解单次GC产生的效果。 与-verbosegc配合使用的一些常用参数为： -XX:+PrintGCDetails，打印GC信息，这是-verbosegc默认开启的选项 -XX:+PrintGCTimeStamps，打印每次GC的时间戳 -XX:+PrintHeapAtGC：每次GC时，打印堆信息 -XX:+PrintGCDateStamps (from JDK 6 update 4) ：打印GC日期，适合于长期运行的服务器 -Xloggc:/home/admin/logs/gc.log：制定打印信息的记录的日志位置每条verbosegc打印出的gc日志，都类似于下面的格式：time [GC [&lt;collector&gt;: &lt;starting occupancy1&gt; -&gt; &lt;ending occupancy1&gt;(total occupancy1), &lt;pause time1&gt; secs] &lt;starting occupancy3&gt; -&gt; &lt;ending occupancy3&gt;(total occupancy3), &lt;pause time3&gt; secs]如：这些选项的意义是：time：执行GC的时间，需要添加-XX:+PrintGCDateStamps参数才有；collector：minor gc使用的收集器的名字。starting occupancy1：GC执行前新生代空间大小。ending occupancy1：GC执行后新生代空间大小。total occupancy1：新生代总大小pause time1：因为执行minor GC，Java应用暂停的时间。starting occupancy3：GC执行前堆区域总大小ending occupancy3：GC执行后堆区域总大小total occupancy3：堆区总大小pause time3：Java应用由于执行堆空间GC（包括full GC）而停止的时间。 可视化工具监控和分析GC也有一些可视化工具，比较常见的有JConsole和VisualVM JConsoleJConsole工具在JDK/bin目录下，启动JConsole后，将自动搜索本机运行的jvm进程，不需要jps命令来查询指定。双击其中一个jvm进程即可开始监控，也可使用“远程进程”来连接远程服务器。进入JConsole主界面，有“概述”、“内存”、“线程”、“类”、“VM摘要”和”Mbean”六个页签：内存页签相当于jstat命令，用于监视收集器管理的虚拟机内存(Java堆和永久代)变化趋势，还可在详细信息栏观察全部GC执行的时间及次数。线程页签最后一个常用页签，VM页签，可清楚的了解显示指定的JVM参数及堆信息。 VisualVMVisualVM是一个集成多个JDK命令行工具的可视化工具。VisualVM基于NetBeans平台开发，它具备了插件扩展功能的特性，通过插件的扩展，可用于显示虚拟机进程及进程的配置和环境信息(jps，jinfo)，监视应用程序的CPU、GC、堆、方法区及线程的信息(jstat、jstack)等。VisualVM在JDK/bin目录下。安装插件： 工具- 插件VisualVM主界面在VisualVM中生成dump文件： 调优方法一切都是为了这一步，调优，在调优之前，我们需要记住下面的原则： 多数的Java应用不需要在服务器上进行GC优化； 多数导致GC问题的Java应用，都不是因为我们参数设置错误，而是代码问题； 在应用上线之前，先考虑将机器的JVM参数设置到最优（最适合）； 减少创建对象的数量； 减少使用全局变量和大对象； GC优化是到最后不得已才采用的手段； 在实际使用中，分析GC情况优化代码比优化GC参数要多得多； GC优化的目的有两个: 将转移到老年代的对象数量降低到最小； 减少full GC的执行时间； 为了达到上面的目的，一般地，你需要做的事情有： 减少使用全局变量和大对象； 调整新生代的大小到最合适； 设置老年代的大小为最合适； 选择合适的GC收集器； 在上面的4条方法中，用了几个“合适”，那究竟什么才算合适，一般的，请参考上面“收集器搭配”和“启动内存分配”两节中的建议。但这些建议不是万能的，需要根据您的机器和应用情况进行发展和变化，实际操作中，可以将两台机器分别设置成不同的GC参数，并且进行对比，选用那些确实提高了性能或减少了GC时间的参数。真正熟练的使用GC调优，是建立在多次进行GC监控和调优的实战经验上的，进行监控和调优的一般步骤为： 监控GC的状态使用各种JVM工具，查看当前日志，分析当前JVM参数设置，并且分析当前堆内存快照和gc日志，根据实际的各区域内存划分和GC执行时间，觉得是否进行优化； 分析结果，判断是否需要优化如果各项参数设置合理，系统没有超时日志出现，GC频率不高，GC耗时不高，那么没有必要进行GC优化；如果GC时间超过1-3秒，或者频繁GC，则必须优化；注：如果满足下面的指标，则一般不需要进行GC： Minor GC执行时间不到50ms； Minor GC执行不频繁，约10秒一次； Full GC执行时间不到1s； Full GC执行频率不算频繁，不低于10分钟1次； 调整GC类型和内存分配如果内存分配过大或过小，或者采用的GC收集器比较慢，则应该优先调整这些参数，并且先找1台或几台机器进行beta，然后比较优化过的机器和没有优化的机器的性能对比，并有针对性的做出最后选择； 不断的分析和调整通过不断的试验和试错，分析并找到最合适的参数 全面应用参数如果找到了最合适的参数，则将这些参数应用到所有服务器，并进行后续跟踪。 调优实例上面的内容都是纸上谈兵，下面我们以一些真实例子来进行说明： 实例1笔者昨日发现部分开发测试机器出现异常：java.lang.OutOfMemoryError: GC overhead limit exceeded，这个异常代表：GC为了释放很小的空间却耗费了太多的时间，其原因一般有两个：1，堆太小，2，有死循环或大对象；笔者首先排除了第2个原因，因为这个应用同时是在线上运行的，如果有问题，早就挂了。所以怀疑是这台机器中堆设置太小；使用ps -ef |grep &quot;java&quot;查看，发现：该应用的堆区设置只有768m，而机器内存有2g，机器上只跑这一个java应用，没有其他需要占用内存的地方。另外，这个应用比较大，需要占用的内存也比较多；笔者通过上面的情况判断，只需要改变堆中各区域的大小设置即可，于是改成下面的情况：跟踪运行情况发现，相关异常没有再出现； 实例2一个服务系统，经常出现卡顿，分析原因，发现Full GC时间太长：123jstat -gcutil:S0 S1 E O P YGC YGCT FGC FGCT GCT12.16 0.00 5.18 63.78 20.32 54 2.047 5 6.946 8.993 分析上面的数据，发现Young GC执行了54次，耗时2.047秒，每次Young GC耗时37ms，在正常范围，而Full GC执行了5次，耗时6.946秒，每次平均1.389s，数据显示出来的问题是：Full GC耗时较长，分析该系统的是指发现，NewRatio=9，也就是说，新生代和老生代大小之比为1:9，这就是问题的原因： 新生代太小，导致对象提前进入老年代，触发老年代发生Full GC； 老年代较大，进行Full GC时耗时较大；优化的方法是调整NewRatio的值，调整到4，发现Full GC没有再发生，只有Young GC在执行。这就是把对象控制在新生代就清理掉，没有进入老年代（这种做法对一些应用是很有用的，但并不是对所有应用都要这么做） 实例3一应用在性能测试过程中，发现内存占用率很高，Full GC频繁，使用sudo -u admin -H jmap -dump:format=b,file=文件名.hprof pid来dump内存，生成dump文件，并使用Eclipse下的mat差距进行分析，发现：从图中可以看出，这个线程存在问题，队列LinkedBlockingQueue所引用的大量对象并未释放，导致整个线程占用内存高达378m，此时通知开发人员进行代码优化，将相关对象释放掉即可。 参考 http://www.cnblogs.com/zhguang/p/Java-JVM-GC.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java的内存分配和回收]]></title>
    <url>%2F2015%2F08%2F12%2Fjava-memory-gc%2F</url>
    <content type="text"><![CDATA[java的内存分配和回收，主要都在堆中分配和回收，所以堆内存是本文讲解的重点。 Java内存分配机制Java内存分配和回收的机制概括的说，就是：分代分配，分代回收。对象将根据存活的时间被分为：年轻代（Young Generation）、年老代（Old Generation）、永久代（Permanent Generation，也就是方法区）。如下图 年轻代（Young Generation）对象被创建时，内存的分配首先发生在年轻代（大对象可以直接 被创建在年老代），大部分的对象在创建后很快就不再使用，因此很快变得不可达，于是被年轻代的GC机制清理掉（IBM的研究表明，98%的对象都是很快消 亡的），这个GC机制被称为Minor GC或叫Young GC。注意，Minor GC并不代表年轻代内存不足，它事实上只表示在Eden区上的GC。年轻代上的内存分配是这样的，年轻代可以分为3个区域：Eden区和两个存活区（Survivor 0 、Survivor 1）。内存分配过程如下图 绝大多数刚创建的对象会被分配在Eden区，其中的大多数对象很快就会消亡。Eden区是连续的内存空间，因此在其上分配内存极快； 当Eden区满的时候，执行Minor GC，将消亡的对象清理掉，并将剩余的对象复制到一个存活区Survivor0（此时，Survivor1是空白的，两个Survivor总有一个是空白的）； 此后，每次Eden区满了，就执行一次Minor GC，并将剩余的对象都添加到Survivor0； 当Survivor0也满的时候，将其中仍然活着的对象直接复制到Survivor1，以后Eden区执行Minor GC后，就将剩余的对象添加Survivor1（此时，Survivor0是空白的）。 当两个存活区切换了几次（HotSpot虚拟机默认15次，用-XX:MaxTenuringThreshold控制，大于该值进入老年代）之后，仍然存活的对象（其实只有一小部分，比如，我们自己定义的对象），将被复制到老年代。 从上面的过程可以看出，Eden区是连续的空间，且Survivor总有一个为空。经过一次GC和复制，一个Survivor中保存着当前还活 着的对象，而Eden区和另一个Survivor区的内容都不再需要了，可以直接清空，到下一次GC时，两个Survivor的角色再互换。因此，这种方式分配内存和清理内存的效率都极高，这种垃圾回收的方式就是著名的“停止-复制（Stop-and-copy）”清理法（将Eden区和一个Survivor中仍然存活的对象拷贝到另一个Survivor中），这不代表着停止复制清理法很高效，其实，它也只在这种情况下高效，如果在老年代采用停止复制，则挺悲剧的。在Eden区，HotSpot虚拟机使用了两种技术来加快内存分配。分别是bump-the-pointer和TLAB（Thread-Local Allocation Buffers），这两种技术的做法分别是：由于Eden区是连续的，因此bump-the-pointer技术的核心就是跟踪最后创建的一个对象，在对象创建时，只需要检查最后一个对象后面是否有足够的内存即可，从而大大加快内存分配速度；而对于TLAB技术是对于多线程而言的，将Eden区分为若干段，每个线程使用独立的一段，避免相互影响。TLAB结合bump-the-pointer技术，将保证每个线程都使用Eden区的一段，并快速的分配内存。 年老代（Old Generation）对象如果在年轻代存活了足够长的时间而没有被清理掉（即在几次 Young GC后存活了下来），则会被复制到年老代，年老代的空间一般比年轻代大，能存放更多的对象，在年老代上发生的GC次数也比年轻代少。当年老代内存不足时， 将执行Major GC，也叫 Full GC。 可以使用-XX:+UseAdaptiveSizePolicy开关来控制是否采用动态控制策略，如果动态控制，则动态调整Java堆中各个区域的大小以及进入老年代的年龄。 如果对象比较大（比如长字符串或大数组），Young空间不足，则大对象会直接分配到老年代上（大对象可能触发提前GC，应少用，更应避免使用短命的大对象）。用-XX:PretenureSizeThreshold来控制直接升入老年代的对象大小，大于这个值的对象会直接分配在老年代上。 可能存在年老代对象引用新生代对象的情况，如果需要执行Young GC，则可能需要查询整个老年代以确定是否可以清理回收，这显然是低效的。解决的方法是，年老代中维护一个512 byte的块——”card table“，所有老年代对象引用新生代对象的记录都记录在这里。Young GC时，只要查这里即可，不用再去查全部老年代，因此性能大大提高。 Java GC机制GC机制的基本算法是：分代收集，这个不用赘述。下面阐述每个分代的收集方法。 年轻代事实上，在上一节，已经介绍了新生代的主要垃圾回收方法，在新生代中，使用“停止-复制”算法进行清理，将新生代内存分为2部分，1部分 Eden区较大，1部分Survivor比较小，并被划分为两个等量的部分。每次进行清理时，将Eden区和一个Survivor中仍然存活的对象拷贝到 另一个Survivor中，然后清理掉Eden和刚才的Survivor。 这里也可以发现，停止复制算法中，用来复制的两部分并不总是相等的（传统的停止复制算法两部分内存相等，但新生代中使用1个大的Eden区和2个小的Survivor区来避免这个问题） 由于绝大部分的对象都是短命的，甚至存活不到Survivor中，所以，Eden区与Survivor的比例较大，HotSpot默认是 8:1，即分别占新生代的80%，10%，10%。如果一次回收中，Survivor+Eden中存活下来的内存超过了10%，则需要将一部分对象分配到 老年代。用-XX:SurvivorRatio参数来配置Eden区域Survivor区的容量比值，默认是8，代表Eden：Survivor1：Survivor2=8:1:1. 老年代老年代存储的对象比年轻代多得多，而且不乏大对象，对老年代进行内存清理时，如果使用停止-复制算法，则相当低效。一般，老年代用的算法是标记-整理算法，即：标记出仍然存活的对象（存在引用的），将所有存活的对象向一端移动，以保证内存的连续。在发生Minor GC时，虚拟机会检查每次晋升进入老年代的大小是否大于老年代的剩余空间大小，如果大于，则直接触发一次Full GC，否则，就查看是否设 置了-XX:+HandlePromotionFailure（允许担保失败），如果允许，则只会进行MinorGC，此时可以容忍内存分配失败；如果不允许，则仍然进行Full GC（这代表着如果设置-XX:+HandlePromotionFailure，则触发MinorGC就会同时触发Full GC，哪怕老年代还有很多内存，所以，最好不要这样做）。 方法区（永久代）永久代的回收有两种：常量池中的常量(1.7 已移出方法区)，无用的类信息，常量的回收很简单，没有引用了就可以被回收。对于无用的类进行回收，必须保证3点： 类的所有实例都已经被回收 加载类的ClassLoader已经被回收 类对象的Class对象没有被引用（即没有通过反射引用该类的地方） 永久代的回收并不是必须的，可以通过参数来设置是否对类进行回收。HotSpot提供-Xnoclassgc进行控制使用-verbose，-XX:+TraceClassLoading、-XX:+TraceClassUnLoading可以查看类加载和卸载信息-verbose、-XX:+TraceClassLoading可以在Product版HotSpot中使用；-XX:+TraceClassUnLoading需要fastdebug版HotSpot支持 垃圾收集器在GC机制中，起重要作用的是垃圾收集器，垃圾收集器是GC的具体实现，Java虚拟机规范中对于垃圾收集器没有任何规定，所以不同厂商实现的垃圾 收集器各不相同，HotSpot 1.6版使用的垃圾收集器如下图（图来源于《深入理解Java虚拟机：JVM高级特效与最佳实现》，图中两个收集器之间有连线，说明它们可以配合使用）：在介绍垃圾收集器之前，需要明确一点，就是在新生代采用的停止复制算法中，“停 止（Stop-the-world）”的意义是在回收内存时，需要暂停其他所 有线程的执行。这个是很低效的，现在的各种新生代收集器越来越优化这一点，但仍然只是将停止的时间变短，并未彻底取消停止。 Serial收集器：新生代收集器，使用停止复制算法，使用一个线程进行GC，其它工作线程暂停。使用-XX:+UseSerialGC可以使用Serial+Serial Old模式运行进行内存回收（这也是虚拟机在Client模式下运行的默认值） ParNew收集器：新生代收集器，使用停止复制算法，Serial收集器的多线程版，用多个线程进行GC，其它工作线程暂停，关注缩短垃圾收集时间。使用-XX:+UseParNewGC开关来控制使用ParNew+Serial Old收集器组合收集内存；使用-XX:ParallelGCThreads来设置执行内存回收的线程数。 Parallel Scavenge 收集器：新生代收集器，使用停止复制算法，关注CPU吞吐量，即运行用户代码的时间/总时间，比如：JVM运行100分钟，其中运行用户代码99分钟，垃圾收集1分钟，则吞吐量是99%，这种收集器能最高效率的利用CPU，适合运行后台运算（关注缩短垃圾收集时间的收集器，如CMS，等待时间很少，所以适合用户交互，提高用户体验）。使用-XX:+UseParallelGC开关控制使用 Parallel Scavenge+Serial Old收集器组合回收垃圾（这也是在Server模式下的默认值）；使用-XX:GCTimeRatio来设置用户执行时间占总时间的比例，默认99，即 1%的时间用来进行垃圾回收。使用-XX:MaxGCPauseMillis设置GC的最大停顿时间（这个参数只对Parallel Scavenge有效） Serial Old收集器：老年代收集器，单线程收集器，使用标记整理（整理的方法是Sweep（清理）和Compact（压缩），清理是将废弃的对象干掉，只留幸存的对象，压缩是将移动对象，将空间填满保证内存分为2块，一块全是对象，一块空闲）算法，使用单线程进行GC，其它工作线程暂停（注意，在老年代中进行标记整理算法清理，也需要暂停其它线程），在JDK1.5之前，Serial Parallel Old收集器：老年代收集器，多线程，多线程机制与Parallel Scavenge差不错，使用标记整理（与Serial Old不同，这里的整理是Summary（汇总）和Compact（压缩），汇总的意思就是将幸存的对象复制到预先准备好的区域，而不是像Sweep（清 理）那样清理废弃的对象）算法，在Parallel Old执行时，仍然需要暂停其它线程。Parallel Old在多核计算中很有用。Parallel Old出现后（JDK 1.6），与Parallel Scavenge配合有很好的效果，充分体现Parallel Scavenge收集器吞吐量优先的效果。使用-XX:+UseParallelOldGC开关控制使用Parallel Scavenge +Parallel Old组合收集器进行收集。 CMS（Concurrent Mark Sweep）收集器：老年代收集器，致力于获取最短回收停顿时间，使用标记清除算法，多线程，优点是并发收集（用户线程可以和GC线程同时工作），停顿小。使用-XX:+UseConcMarkSweepGC进行ParNew+CMS+Serial Old进行内存回收，优先使用ParNew+CMS（原因见后面），当用户线程内存不足时，采用备用方案Serial Old收集。CMS收集的方法是：先3次标记，再1次清除，3次标记中前两次是初始标记和重新标记（此时仍然需要停止（stop the world））， 初始标记（Initial Remark）是标记GC Roots能关联到的对象（即有引用的对象），停顿时间很短；并发标记（Concurrent remark）是执行GC Roots查找引用的过程，不需要用户线程停顿；重新标记（Remark）是在初始标记和并发标记期间，有标记变动的那部分仍需要标记，所以加上这一部分 标记的过程，停顿时间比并发标记小得多，但比初始标记稍长。在完成标记之后，就开始并发清除，不需要用户线程停顿。所以在CMS清理过程中，只有初始标记和重新标记需要短暂停顿，并发标记和并发清除都不需要暂停用户线程，因此效率很高，很适合高交互的场合。CMS也有缺点，它需要消耗额外的CPU和内存资源，在CPU和内存资源紧张，CPU较少时，会加重系统负担（CMS默认启动线程数为(CPU数量+3)/4）。另外，在并发收集过程中，用户线程仍然在运行，仍然产生内存垃圾，所以可能产生“浮动垃圾”，本次无法清理，只能下一次Full GC才清理，因此在GC期间，需要预留足够的内存给用户线程使用。所以使用CMS的收集器并不是老年代满了才触发Full GC，而是在使用了一大半（默认68%，即2/3，使用-XX:CMSInitiatingOccupancyFraction来设置）的时候就要进行Full GC，如果用户线程消耗内存不是特别大，可以适当调高-XX:CMSInitiatingOccupancyFraction以降低GC次数，提高性能，如果预留的用户线程内存不够，则会触发Concurrent Mode Failure，此时，将触发备用方案：使用Serial Old 收集器进行收集，但这样停顿时间就长了，因此-XX:CMSInitiatingOccupancyFraction不宜设的过大。还有，CMS采用的是标记清除算法，会导致内存碎片的产生，可以使用-XX：+UseCMSCompactAtFullCollection来设置是否在Full GC之后进行碎片整理，用-XX：CMSFullGCsBeforeCompaction来设置在执行多少次不压缩的Full GC之后，来一次带压缩的Full GC。 G1收集器：在JDK1.7中正式发布，与现状的新生代、老年代概念有很大不同，目前使用较少，不做介绍。 参考：http://www.cnblogs.com/zhguang/p/3257367.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python在不同层级目录import模块的方法]]></title>
    <url>%2F2015%2F02%2F18%2Fpython-import-module%2F</url>
    <content type="text"><![CDATA[使用python进行程序编写时，经常会使用第三方模块包。这种包我们可以通过python setup install 进行安装后，通过import XXX或from XXX import yyy 进行导入。不过如果是自己遍写的依赖包，又不想安装到python的相应目录，可以放到本目录里进行import进行调用；为了更清晰的理清程序之间的关系，例如我们会把这种包放到lib目录再调用。本篇就针对常见的模块调用方法汇总下。 同级目录下的调有程序结构如下：123-- src |-- mod1.py |-- test1.py 若在程序test1.py中导入模块mod1, 则直接使用123import mod1或from mod1 import *; 调用子目录下的模块程序结构如下：12345-- src |-- mod1.py |-- lib | |-- mod2.py |-- test1.py 这时看到test1.py和lib目录（即mod2.py的父级目录），如果想在程序test1.py中导入模块mod2.py ，可以在lib件夹中建立空文件init.py文件(也可以在该文件中自定义输出模块接口)，然后使用：123from lib.mod2 import *或import lib.mod2 调用上级目录下的文件程序结构如下：123456-- src |-- mod1.py |-- lib | |-- mod2.py |-- sub | |-- test2.py 这里想要实现test2.py调用mod1.py和mod2.py ，做法是我们先跳到src目录下面，直接可以调用mod1，然后在lib上当下建一个空文件init.py ，就可以像第二步调用子目录下的模块一样，通过import lib.mod2进行调用了。具体代码如下：1234import syssys.path.append("..")import mod1import lib.mod2]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启用Nginx状态监控]]></title>
    <url>%2F2014%2F10%2F12%2Fnginx-status-monitor%2F</url>
    <content type="text"><![CDATA[编译Nginx添加http_stub_status_module编译Nginx的时候添加参数：–with-http_stub_status_module123456cd nginx-&#123;version&#125;/./configure --prefix=/opt/nginx --with-http_stub_status_module --with-http_ssl_modulemake &amp;&amp; make install 启用nginx status配置修改Nginx配置文件nginx.conf，在HTTP段中添加1vi /opt/nginx/conf/nginx.conf 123456789101112server&#123; listen 80; server_name localhost; location /nginx_status &#123; #主要是这里代表根目录显示信息 stub_status on; access_log off; &#125;&#125; 打开status页面浏览器访问监控页面地址http://{your IP}/nginx-status,显示如下1Active connections: 2 server accepts handled requests 8 8 33 Reading: 0 Writing: 1 Waiting: 1 解析：Active connections //当前 Nginx 正处理的活动连接数。server accepts handledrequests //总共处理了8 个连接 , 成功创建 8 次握手,总共处理了33个请求。Reading //nginx 读取到客户端的 Header 信息数。Writing //nginx 返回给客户端的 Header 信息数。Waiting //开启 keep-alive 的情况下，这个值等于 active – (reading + writing)，意思就是 Nginx 已经处理完正在等候下一次请求指令的驻留连接]]></content>
      <categories>
        <category>ngnix</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx location 语法和优先级]]></title>
    <url>%2F2014%2F10%2F07%2Fnginx-location%2F</url>
    <content type="text"><![CDATA[验证下，记录下，以后不用到处找 语法:location [=|~|~*|^~] /uri/ { … } = 精确匹配，例如 location =/test 只匹配 /test ~ 后面接正则，区分大小写,匹配满足此正则的url ~* 后面接正则，不区分大小写,匹配满足此正则的url ^~ 匹配以什么开头的，例如 location ^~ /test 匹配 /test,/test/1,/test/2 ; 什么都不加的话，类似第四点，location /test 匹配 /test,/test/1,/test/2 ;优先级= 大于 ^~ 大于 ~ 或者~* 大于 什么都不加 假如遇到同级的话 ~ 和~ （他们是同级的）比较的话，就是根据他们在文件的出现顺序。例如location ~ /sb.txtlocation ~ /sb.txt如果访问 http://localhost/sb.txt,就会跳到第一个，访问 http://localhost/SB.txt 跳到第二个，换个顺序location ~* /sb.txtlocation ~ /sb.txt不管访问 http://localhost/sb.txt 还是 http://localhost/SB.txt ,都只会跳到第一个 ^~ 的话，就根据他们的匹配度，例如location ^~ /sb/location ^~ /sb/sb2/如果访问http://localhost/sb/sb2/,就会跳到第二个去处理，因为他更加匹配吧。如果访问http://localhost/sb/，就会跳到第一个处理，如果访问http://localhost/sb/sb3,也会跳到第一个。 什么都不加的话，也是根据他们的匹配度，例如location /sb/location /sb/sb2/如果访问http://localhost/sb/sb2/,就会跳到第二个去处理，因为他更加匹配吧。如果访问http://localhost/sb/，就会跳到第一个处理，如果访问http://localhost/sb/sb3,也会跳到第一个。]]></content>
      <categories>
        <category>ngix</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lvs+keepalived 做负载和热备]]></title>
    <url>%2F2014%2F10%2F07%2Flinux-lvs%2F</url>
    <content type="text"><![CDATA[一直觉得nginx做负载不错了，可发现居然这玩意这么好，特别keepalived热备。 准备工作本实验基于VMware+centos,以下是环境信息：假如有4台服务器master：192.168.153.132backup：192.168.153.133vip（virtual ip 虚拟ip）：192.168.153.100 （实际浏览器访问的地址）realserver 1: 192.168.153.128realserver 2: 192.168.153.129热备的意思就是master坏了，backup能补上去，由于都是使用vip，所以外面浏览器访问没有影响realserver1和2是用来做负载均衡，按照权重来调到相应的服务器来处理请求。lvs和keepalived都要装到master和backup上 下载keepalived和lvs的admin程序1$ wget http://www.keepalived.org/software/keepalived-1.2.13.tar.gz 1$ wget http://www.linuxvirtualserver.org/software/kernel-2.6/ipvsadm-1.26.tar.gz 安装依赖1$ yum -y install libnl* openssl* popt* kernel-devel 解压安装12$ tar -zxvf keepalived-1.2.13.tar.gz$ tar -zxvf ipvsadm-1.26.tar.gz 123$ cd keepalived-1.2.13$ ./configure --prefix=/usr --sysconf=/etc$ make &amp;&amp; make install 12$ cd ipvsadm-1.26$ make &amp;&amp; make install 验证lvs1$ ipvsadm ln 验证keepalived1$ service keepalived start 如果显示无误就代表安装成功了。 配置配置master1$ vi /etc/keepalived/keepalived.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455global_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; state MASTER #表示master interface eth1 #网络接口，有可能是eth0，可以用命令ifconfig查看 virtual_router_id 51 priority 100 #优先级 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.153.100 #vip配置 &#125;&#125;virtual_server 192.168.153.100 80 &#123; delay_loop 6 lb_algo rr #负载算法 lb_kind DR #负载方式 DR，这种方式的话VIP端口和realserver端口必须一致 #persistence_timeout 20 protocol TCP real_server 192.168.153.128 80 &#123; weight 3 #权重 TCP_CHECK &#123; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 connect_port 80&#125;&#125; real_server 192.168.153.129 80 &#123; weight 3 #权重 TCP_CHECK &#123; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 connect_port 80&#125;&#125;&#125; 配置backup基本跟master一致，只是在上面红色的第一处改成BACKUP，第二处改成比master小的值，例如90 重启keepalived1$ service keepalived restart 配置realserver在real server 1和2上，把以下脚本放在/etc/rc.d/init.d/下12345678910111213141516171819202122232425262728293031323334#!/bin/bash# description: Config realserver lo and apply noarp SNS_VIP=192.168.153.100 . /etc/rc.d/init.d/functions case "$1" instart) ifconfig lo:0 $SNS_VIP netmask 255.255.255.255 broadcast $SNS_VIP /sbin/route add -host $SNS_VIP dev lo:0 echo "1" &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore echo "2" &gt;/proc/sys/net/ipv4/conf/lo/arp_announce echo "1" &gt;/proc/sys/net/ipv4/conf/all/arp_ignore echo "2" &gt;/proc/sys/net/ipv4/conf/all/arp_announce sysctl -p &gt;/dev/null 2&gt;&amp;1 echo "RealServer Start OK" ;;stop) ifconfig lo:0 down route del $SNS_VIP &gt;/dev/null 2&gt;&amp;1 echo "0" &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore echo "0" &gt;/proc/sys/net/ipv4/conf/lo/arp_announce echo "0" &gt;/proc/sys/net/ipv4/conf/all/arp_ignore echo "0" &gt;/proc/sys/net/ipv4/conf/all/arp_announce echo "RealServer Stoped" ;;*) echo "Usage: $0 &#123;start|stop&#125;" exit 1esac exit 0 命名成realserver,然后可以用以下命令启动：1$ service realserver start 关闭iptable如果不想关掉的话在master 加上以下规则1-A INPUT -i eth1 -p vrrp -s 192.168.153.133 -j ACCEPT 在backup加上以下规则1-A INPUT -i eth1 -p vrrp -s 192.168.153.132 -j ACCEPT 其实就是让热备之间可以通讯，注意eth1是你的网络接口，如果是eth0就要换成eth0realserver就只要开启80端口就可以了。 验证 开启realserver的web应用程序 执行realserver脚本 开启master和backup的keepalived 用watch ipvsadm -ln在master和backup上可以查看状态如果有两条记录分别指向realserver的ip就代表运行正确了。 浏览器访问vip，看能否正确访问web应用程序。 关掉master 的keepalived，如果还能访问的话，就代表backup接管了。可以在backup的日志上面看到，tail -f /var/log/message如果有transition MASTER的，就代表接管了。 再启动master的keepalived，再到backup的日志上面看到transition BACKUP的，就代表已经交回master了。 关掉其中一个realserver，用watch ipvsadm -ln可以看到该realserver的ip被剔除了。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>lvs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos 启用ftp功能]]></title>
    <url>%2F2014%2F08%2F17%2Fcentos-ftp%2F</url>
    <content type="text"><![CDATA[1.安装vsftpd组件1yum -y install vsftpd 安装完后，有个/etc/vsftpd/vsftpd.conf 文件，用来配置还有自动新建了一个ftp用户和ftp的组，home目录为/var/ftp,默认是nologin（不能登录系统）1cat /etc/passwd | grep ftp 默认ftp服务是没有启动的，用下面命令启动1service vsftpd start 2.安装ftp客户端组件（用来验证是否vsftpd）1yum -y install ftp 执行命令尝试登录1ftp localhost 输入用户名ftp，密码随便（因为默认是允许匿名的） 登录成功，就代表ftp服务可用了。 但是，外网是访问不了的，所以还要继续配置。 3.取消匿名登陆1vi /etc/vsftpd/vsftpd.conf 把第一行的 anonymous_enable=YES ，改为NO重启1service vsftpd restart 4.新建一个用户(ftpuser为用户名，随便就可以)1useradd ftpuser 修改密码（输入两次）1passwd ftpuser 这样一个用户建完，可以用这个登录，记得用普通登录不要用匿名了。登录后默认的路径为 /home/ftpuser. 5.开放21端口 因为ftp默认的端口为21，而centos默认是没有开启的，所以要修改iptables文件1vi /etc/sysconfig/iptables 在行上面有22 -j ACCEPT 下面另起一行输入跟那行差不多的，只是把22换成21，然后：wq保存。还要运行下,重启iptables1service iptables restart 外网是可以访问上去了，可是发现没法返回目录，也上传不了，因为selinux作怪了。6.修改selinux1getsebool -a | grep ftp 执行上面命令，再返回的结果看到两行都是off，代表，没有开启外网的访问12345.... allow_ftpd_full_access off ........ftp_home_dir off 只要把上面都变成on就行执行12setsebool -P allow_ftpd_full_access 1 setsebool -P ftp_home_dir off 1 再重启一下vsftpd1service vsftpd restart 这样应该没问题了（如果，还是不行，看看是不是用了ftp客户端工具用了passive模式访问了，如提示Entering Passive mode，就代表是passive模式，默认是不行的，因为ftp passive模式被iptables挡住了，下面会讲怎么开启，如果懒得开的话，就看看你客户端ftp是否有port模式的选项，或者把passive模式的选项去掉。如果客户端还是不行，看看客户端上的主机的电脑是否开了防火墙，关吧） 7.开启passive模式 默认是开启的，但是要指定一个端口范围，打开vsftpd.conf文件，在后面加上123pasv_min_port=30000pasv_max_port=30999 表示端口范围为30000~30999，这个可以随意改。改完重启一下vsftpd 由于指定这段端口范围，iptables也要相应的开启这个范围，所以像上面那样打开iptables文件 也是在21上下面另起一行，更那行差不多，只是把21 改为30000:30999,然后:wq保存，重启下iptables。这样就搞定了。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[List、Set、Map集合存放null分析]]></title>
    <url>%2F2014%2F08%2F16%2Fjava-collection-null%2F</url>
    <content type="text"><![CDATA[验证下。 上代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package com.sky.code.collection;import java.util.*;import java.util.concurrent.ConcurrentHashMap;public class TestNull &#123; public static void main(String[] args)&#123; System.out.println("测试ArrayList"); List&lt;Object&gt; list = new ArrayList&lt;Object&gt;(); list.add(null); list.add(null); System.out.println(list); System.out.println("测试LinkedList"); List&lt;Object&gt; list1 = new LinkedList&lt;Object&gt;(); list1.add(null); list1.add(null); System.out.println(list1); System.out.println("测试HashSet"); Set&lt;Object&gt; set = new HashSet&lt;Object&gt;(); set.add(null); set.add(null); System.out.println(set); System.out.println("测试TreeSet"); Set&lt;Object&gt; treeSet = new TreeSet&lt;Object&gt;(); try &#123; treeSet.add(null); &#125;catch (Exception e)&#123; System.out.println(e); &#125; System.out.println("测试LinkedHashSet"); Set&lt;Object&gt; set1 = new LinkedHashSet&lt;Object&gt;(); set1.add(null); set1.add(null); System.out.println(set1); System.out.println("测试HashMap"); Map&lt;Object, Object&gt; hashMap = new HashMap&lt;&gt;(); hashMap.put(null, null); hashMap.put(null, null); hashMap.put(1, null); hashMap.put(1, 12); System.out.println(hashMap); System.out.println("测试LinkedHashMap"); Map&lt;Object, Object&gt; linkedHashMap = new LinkedHashMap&lt;&gt;(); linkedHashMap.put(null, null); linkedHashMap.put(null, null); linkedHashMap.put(1, null); linkedHashMap.put(1, 12); System.out.println(linkedHashMap); System.out.println("测试TreeMap"); Map&lt;Object, Object&gt; treemap = new TreeMap&lt;&gt;(); treemap.put(1, null); try &#123; treemap.put(null, 1); &#125; catch (Exception e)&#123; System.out.println(e); &#125; treemap.put(2, 12); System.out.println(treemap); System.out.println("测试ConcurrentHashMap"); Map&lt;Object, Object&gt; concurrentHashMap = new ConcurrentHashMap&lt;&gt;(); try &#123; concurrentHashMap.put(null, 1); &#125;catch (Exception e)&#123; System.out.println(e); &#125; try &#123; concurrentHashMap.put(1, null); &#125; catch (Exception e)&#123; System.out.println(e); &#125; concurrentHashMap.put(1, 12); System.out.println(treemap); System.out.println("测试Hashtable"); Map&lt;Object, Object&gt; hashtable = new Hashtable&lt;&gt;(); try &#123; hashtable.put(null,"null"); &#125; catch (Exception e)&#123; System.out.println(e); &#125; try &#123; hashtable.put(3,null); &#125; catch (Exception e)&#123; System.out.println(e); &#125; &#125;&#125; 结果123456789101112131415161718192021222324测试ArrayList[null, null]测试LinkedList[null, null]测试HashSet[null]测试TreeSetjava.lang.NullPointerException测试LinkedHashSet[null]测试HashMap&#123;null=null, 1=12&#125;测试LinkedHashMap&#123;null=null, 1=12&#125;测试TreeMapjava.lang.NullPointerException&#123;1=null, 2=12&#125;测试ConcurrentHashMapjava.lang.NullPointerExceptionjava.lang.NullPointerException&#123;1=null, 2=12&#125;测试Hashtablejava.lang.NullPointerExceptionjava.lang.NullPointerException 结论很明显tree结构的都不接受key为null,hashtable和concurrenthashmap是key和value都不支持null的，其他都就随便吧。🙂 所有代码在 https://github.com/ejunjsh/java-code/tree/master/com/sky/code/collection]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[javascript在浏览器跨域访问的几种处理方式]]></title>
    <url>%2F2014%2F08%2F02%2Fjquery-cross-domain-ajax%2F</url>
    <content type="text"><![CDATA[这里说的js跨域是指通过js在不同的域之间进行数据传输或通信，比如用ajax向一个不同的域请求数据。只要协议、域名、端口有任何一个不同，都被当作是不同的域。下表给出了相对http://store.company.com/dir/page.html 同源检测的结果: URL 结果 原因 http://store.company.com/dir2/other.html 成功 http://store.company.com/dir/inner/another.html 成功 https://store.company.com/dir/inner/another.html 失败 协议不同 http://store.company.com:81/dir/inner/another.html 失败 端口不同 http://news.company.com/dir/inner/another.html 失败 域名不同 要解决跨域的问题，我们可以使用以下几种方法： 通过jsonp在js中，我们直接用XMLHttpRequest请求不同域上的数据时，是不可以的。但是，在页面上引入不同域上的js脚本文件却是可以的，jsonp正是利用这个特性来实现的。比如，有个a.html页面，它里面的代码需要利用ajax获取一个不同域上的json数据，假设这个json数据地址是http://example.com/data.php, 那么a.html中的代码就可以这样：123456&lt;scirpt&gt;function dosomething(jsondata)&#123; //处理获得的json数据&#125;&lt;/scirpt&gt;&lt;scirpt src="http://example.com/data.php?callback=dosomething"&gt;&lt;/scirpt&gt; 我们看到获取数据的地址后面还有一个callback参数，按惯例是用这个参数名，但是你用其他的也一样。当然如果获取数据的jsonp地址页面不是你自己能控制的，就得按照提供数据的那一方的规定格式来操作了。因为是当做一个js文件来引入的，所以http://example.com/data.php返回的必须是一个能执行的js文件，所以这个页面的php代码可能是这样的:12345&lt;?php$callback=&amp;_GET[`callback`];//获得回调函数名$data=array('a','b','c');//要返回的数据echo $callback.'('.json_encode($data).')';//输出?&gt; 最终那个页面输出的结果是:1dosomething(['a','b','c']) 所以通过http://example.com/data.php?callback=dosomething得到的js文件，就是我们之前定义的dosomething函数,并且它的参数就是我们需要的json数据，这样我们就跨域获得了我们需要的数据。 这样jsonp的原理就很清楚了，通过script标签引入一个js文件，这个js文件载入成功后会执行我们在url参数中指定的函数，并且会把我们需要的json数据作为参数传入。所以jsonp是需要服务器端的页面进行相应的配合的。 知道jsonp跨域的原理后我们就可以用js动态生成script标签来进行跨域操作了，而不用特意的手动的书写那些script标签。如果你的页面使用jquery，那么通过它封装的方法就能很方便的来进行jsonp操作了。12345&lt;scirpt&gt;$.getJSON('http://example.com/data.php?callback=?',function(jsondata)&#123; ////处理获得的json数据&#125;);&lt;/scirpt&gt; 原理是一样的，只不过我们不需要手动的插入script标签以及定义回掉函数。jquery会自动生成一个全局函数来替换callback=?中的问号，之后获取到数据后又会自动销毁，实际上就是起一个临时代理函数的作用。$.getJSON方法会自动判断是否跨域，不跨域的话，就调用普通的ajax方法；跨域的话，则会以异步加载js文件的形式来调用jsonp的回调函数。 通过XHR2HTML5中提供的XMLHTTPREQUEST Level2（及XHR2）已经实现了跨域访问。但ie10以下不支持只需要在服务端填上响应头123header(&quot;Access-Control-Allow-Origin:*&quot;);/*星号表示所有的域都可以接受，*/header(&quot;Access-Control-Allow-Methods:GET,POST&quot;);]]></content>
      <categories>
        <category>javascript</category>
      </categories>
      <tags>
        <tag>javascirpt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用cglib生成的代理类取不到注解的问题]]></title>
    <url>%2F2014%2F07%2F19%2Fjava-cglib%2F</url>
    <content type="text"><![CDATA[经常用cglib来创建代理类来实现aop的功能，可是，当想用反射来取得代理类所代理的类的注解的时候，却怎么也取不到。。。。 然后搜了下stackoverflow，http://stackoverflow.com/questions/1706751/retain-annotations-on-cglib-proxies用@Inherited,注解自己的注解（绕~~）12345@Inherited@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface MyAnnotation &#123;&#125; 然后用这个注解注解你要代理的类，那样通过反射可以拿到被代理的注解。原来CGLIB 返回的代理类是被代理的类的子类，加上这个标志就可以令子类继承这个注解，@Inherited 字面意思就是有继承的意思。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven resource 记录]]></title>
    <url>%2F2014%2F02%2F20%2Fmaven-resouce%2F</url>
    <content type="text"><![CDATA[今天遇到maven打包的时候发现在main/java里面的xml没有打包进jar上。 上网搜了下，maven默认打包main/resource的资源，要想打包main/java的要像下面这样配。12345678910111213&lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;include&gt;**/*.tld&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; 搞定收工。。。不知道有没有更好到方法呢？]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins + git+maven做持续集成]]></title>
    <url>%2F2014%2F02%2F20%2Fjenkins-jenkins-git-maven%2F</url>
    <content type="text"><![CDATA[1.下个jenkins，官网去下 http://jenkins-ci.org/，里面提供war包下载，直接部署到tomcat什么上面吧。2.部署成功后打开网站例如：http://localhost/jenkin，默认是不带git的插件的，所以先去下一个先，点击主页的右侧“系统管理”=&gt;&quot;管理插件&quot;=&gt;“可选插件” 找到”git plungin” 然后点击直接安装。（这可能要花点时间）3.下完git插件后就要配环境了，还是点击右侧“系统管理”=&gt;“系统设置” 主要配jdk和maven的环境。（把自动安装勾掉就可以输路径了），保存下就可以了。 4.点击右侧“新建”=&gt;“构建一个maven项目” 输入名字到下一步如下图勾上“丢弃旧的构建”，按照自己的需要配置，否则很占硬盘。配置git仓库（如果是私有库，必须添加一个Credentials，点击右侧Add，在弹出界面录入帐号密码）接下来配置定时构建（勾上Build periodically,图中设置是每15分钟一次），配置要执行的maven命令 clean install (mvn不用输)保存后，一个构建就可以了（可以立即构建试试，也可以定时执行）。jenkins提供了一堆的页面去展示构建的过程，很不错。如果web程序想自动部署到本地的tomcat，可以试下cargo插件，加上下面代码到项目pom上。下面代码改下路径就可以了。当然也可以部署到远程，就不贴了。12345678910111213141516171819202122&lt;plugin&gt; &lt;groupId&gt;org.codehaus.cargo&lt;/groupId&gt; &lt;artifactId&gt;cargo-maven2-plugin&lt;/artifactId&gt; &lt;version&gt;1.4.5&lt;/version&gt; &lt;configuration&gt; &lt;container&gt; &lt;containerId&gt;tomcat7x&lt;/containerId&gt; &lt;home&gt;/opt/apache-tomcat-7.0.47&lt;/home&gt; &lt;/container&gt; &lt;configuration&gt; &lt;type&gt;existing&lt;/type&gt; &lt;home&gt;/opt/apache-tomcat-7.0.47&lt;/home&gt; &lt;/configuration&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;tomcat-deploy&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt;&lt;goal&gt;deploy&lt;/goal&gt;&lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 这样一个持续集成就配好了。想想那边提交代码，另一边就自动部署到tomcat上，爽歪歪了。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jenkins</tag>
        <tag>git</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[find 命令]]></title>
    <url>%2F2014%2F02%2F14%2Flinux-find%2F</url>
    <content type="text"><![CDATA[mark一下，当笔记，以后忘了查看 查找文件find ./ -type f 查找目录find ./ -type d 查找名字为test的文件或目录find ./ -name test 查找名字符合正则表达式的文件,注意前面的‘.’(查找到的文件带有目录)find ./ -regex .so.*.gz 查找目录并列出目录下的文件(为找到的每一个目录单独执行ls命令，没有选项-print时文件列表前一行不会显示目录名称)find ./ -type d -print -exec ls {} \; 查找目录并列出目录下的文件(为找到的每一个目录单独执行ls命令,执行命令前需要确认)find ./ -type d -ok ls {} \; 查找目录并列出目录下的文件(将找到的目录添加到ls命令后一次执行，参数过长时会分多次执行)find ./ -type d -exec ls {} + 查找文件名匹配.c的文件find ./ -name \.c 打印test文件名后，打印test文件的内容find ./ -name test -print -exec cat {} \; 不打印test文件名，只打印test文件的内容find ./ -name test -exec cat {} \; 查找文件更新日时在距现在时刻二天以内的文件find ./ -mtime -2 查找文件更新日时在距现在时刻二天以上的文件find ./ -mtime +2 查找文件更新日时在距现在时刻一天以上二天以内的文件find ./ -mtime 2 查找文件更新日时在距现在时刻二分以内的文件find ./ -mmin -2 查找文件更新日时在距现在时刻二分以上的文件find ./ -mmin +2 查找文件更新日时在距现在时刻一分以上二分以内的文件find ./ -mmin 2 查找文件更新时间比文件abc的内容更新时间新的文件find ./ -newer abc 查找文件访问时间比文件abc的内容更新时间新的文件find ./ -anewer abc 查找空文件或空目录find ./ -empty 查找空文件并删除find ./ -empty -type f -print -delete 查找权限为644的文件或目录(需完全符合)find ./ -perm 664 查找用户/组权限为读写，其他用户权限为读(其他权限不限)的文件或目录find ./ -perm -664 查找用户有写权限或者组用户有写权限的文件或目录find ./ -perm /220find ./ -perm /u+w,g+wfind ./ -perm /u=w,g=w 查找所有者权限有读权限的目录或文件find ./ -perm -u=r 查找用户组权限有读权限的目录或文件find ./ -perm -g=r 查找其它用户权限有读权限的目录或文件find ./ -perm -o=r 查找所有者为lzj的文件或目录find ./ -user lzj 查找组名为gname的文件或目录find ./ -group gname 查找文件的用户ID不存在的文件find ./ -nouser 查找文件的组ID不存在的文件find ./ -nogroup 查找有执行权限但没有可读权限的文件find ./ -executable ! -readable 查找文件size小于10个字节的文件或目录find ./ -size -10c 查找文件size等于10个字节的文件或目录find ./ -size 10c 查找文件size大于10个字节的文件或目录find ./ -size +10c 查找文件size小于10k的文件或目录find ./ -size -10k 查找文件size小于10M的文件或目录find ./ -size -10M 查找文件size小于10G的文件或目录find ./ -size -10G]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>find</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crontab 命令]]></title>
    <url>%2F2014%2F02%2F13%2Flinux-crontab%2F</url>
    <content type="text"><![CDATA[一般编写调度让系统帮你定时做事情有两种方式： 在/etc目录下有一个crontab文件，这里存放有系统运行的一些调度程序,可以把自己的调度写在这里。 每个用户可以用crontab命令建立自己的调度,调度文件存放在以用户名为名的文件/usr/spool/cron/crontabs/username。 crontab命令有三种形式的命令行结构： crontab [-u user] [file] crontab [-u user] [-e|-l|-r] crontab -l -u [-e|-l|-r] 第一个命令行中，file是命令文件的名字。如果在命令行中指定了这个文件，那么执行crontab命令，则将这个文件拷贝到crontabs目录下；如果在命令行中没有制定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将他们也存放在crontab目录下。 命令行中-r选项的作用是从/usr/spool/cron/crontabs目录下删除用户定义的文件crontab； 命令行中-l选项的作用是显示用户crontab文件的内容。 使用命令crontab -u user -e命令编辑用户user的cron(c)作业。用户通过编辑文件来增加或修改任何作业请求。 执行命令crontab -u user -r即可删除当前用户的所有的cron作业。 作业与它们预定的时间储存在文件/usr/spool/cron/crontabs/username里。username使用户名，在相应的文件中存放着该用户所要运行的命令。命令执行的结果，无论是标准输出还是错误输出，都将以邮件形式发给用户。文件里的每一个请求必须包含以spaces和tabs分割的六个域。前五个字段可以取整数值，指定何时开始工作，第六个域是字符串，称为命令字段，其中包括了crontab调度执行的命令。 第一道第五个字段的整数取值范围及意义是： 0～59 表示分 1～23 表示小时 1～31 表示日 1～12 表示月份 0～6 表示星期（其中0表示星期日） /usr/lib/cron/cron.allow表示谁能使用crontab命令。如果它是一个空文件表明没有一个用户能安排作业。如果这个文件不存在，而有另外一个文件/usr/lib/cron/cron.deny,则只有不包括在这个文件中的用户才可以使用crontab命令。如果它是一个空文件表明任何用户都可安排作业。两个文件同时存在时cron.allow优先，如果都不存在，只有超级用户可以安排作业。]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>crontab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jQuery.extend 函数详解]]></title>
    <url>%2F2014%2F02%2F05%2Fjquery-extend%2F</url>
    <content type="text"><![CDATA[Jquery的扩展方法extend是我们在写插件的过程中常用的方法，该方法有一些重载原型，在此，我们一起去了解了解。 如何使用Jquery的扩展方法原型是: 1extend(dest,src1,src2,src3...); 它的含义是将src1,src2,src3…合并到dest中,返回值为合并后的dest,由此可以看出该方法合并后，是修改了dest的结构的。如果想要得到合并的结果却又不想修改dest的结构，可以如下使用：1var newSrc=$.extend(&#123;&#125;,src1,src2,src3...)//也就是将"&#123;&#125;"作为dest参数。 这样就可以将src1,src2,src3…进行合并，然后将合并结果返回给newSrc了。如下例：1var result=$.extend(&#123;&#125;,&#123;name:"Tom",age:21&#125;,&#123;name:"Jerry",sex:"Boy"&#125;) 那么合并后的结果1result=&#123;name:"Jerry",age:21,sex:"Boy"&#125; 也就是说后面的参数如果和前面的参数存在相同的名称，那么后面的会覆盖前面的参数值。 省略dest参数上述的extend方法原型中的dest参数是可以省略的，如果省略了，则该方法就只能有一个src参数，而且是将该src合并到调用extend方法的对象中去，如： $.extend(src)该方法就是将src合并到jquery的全局对象中去，如：123$.extend(&#123; hello:function()&#123;alert('hello');&#125; &#125;); 就是将hello方法合并到jquery的全局对象中。 $.fn.extend(src)该方法将src合并到jquery的实例对象中去，如:123$.fn.extend(&#123; hello:function()&#123;alert('hello');&#125; &#125;); 就是将hello方法合并到jquery的实例对象中。 举个例子1234567$.extend(&#123;net:&#123;&#125;&#125;);```` 这是在jquery全局对象中扩展一个net命名空间。 ````javascript$.extend($.net,&#123; hello:function()&#123;alert('hello');&#125; &#125;) 这是将hello方法扩展到之前扩展的Jquery的net命名空间中去。 Jquery的extend方法还有一个重载原型1234567extend(boolean,dest,src1,src2,src3...)```` 第一个参数boolean代表是否进行深度拷贝，其余参数和前面介绍的一致，什么叫深层拷贝，我们看一个例子： ````javascriptvar result=$.extend( true, &#123;&#125;, &#123; name: &quot;John&quot;, location: &#123;city: &quot;Boston&quot;,county:&quot;USA&quot;&#125; &#125;, &#123; last: &quot;Resig&quot;, location: &#123;state: &quot;MA&quot;,county:&quot;China&quot;&#125; &#125; ); 我们可以看出src1中嵌套子对象location:{city:”Boston”},src2中也嵌套子对象location:{state:”MA”},第一个深度拷贝参数为true，那么合并后的结果就是：12result=&#123;name:"John",last:"Resig", location:&#123;city:"Boston",state:"MA",county:"China"&#125;&#125; 也就是说它会将src中的嵌套子对象也进行合并，而如果第一个参数boolean为false，我们看看合并的结果是什么，如下：1234var result=$.extend( false, &#123;&#125;, &#123; name: "John", location:&#123;city: "Boston",county:"USA"&#125; &#125;, &#123; last: "Resig", location: &#123;state: "MA",county:"China"&#125; &#125; ); 那么合并后的结果就是:1result=&#123;name:"John",last:"Resig",location:&#123;state:"MA",county:"China"&#125;&#125; 以上就是$.extend()在项目中经常会使用到的一些细节。]]></content>
      <categories>
        <category>jQuery</category>
      </categories>
      <tags>
        <tag>jQuery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于jQuery新的事件绑定机制on()的使用技巧]]></title>
    <url>%2F2014%2F01%2F11%2Fjquery-on%2F</url>
    <content type="text"><![CDATA[本篇文章介绍了，关于jQuery新的事件绑定机制on()的使用技巧。需要的朋友参考下今天浏览jQuery的deprecated列表，发现live()和die()在里面了，赶紧看了一下，发现从jQuery1.7开始，jQuery引入了全新的事件绑定机制，on()和off()两个函数统一处理事件绑定。因为在此之前有bind(), live(), delegate()等方法来处理事件绑定，jQuery从性能优化以及方式统一方面考虑决定推出新的函数来统一事件绑定方法并且替换掉以前的方法。 用法1on(events,[selector],[data],fn) events:一个或多个用空格分隔的事件类型和可选的命名空间，如”click”或”keydown.myPlugin” 。selector:一个选择器字符串用于过滤器的触发事件的选择器元素的后代。如果选择器为null或省略，当它到达选定的元素，事件总是触发。data:当一个事件被触发时要传递event.data给事件处理函数。fn:该事件被触发时执行的函数。 false 值也可以做一个函数的简写，返回false。 替换bind()当第二个参数’selector’为null时，on()和bind()其实在用法上基本上没有任何区别了，所以我们可以认为on()只是比bind()多了一个可选的’selector’参数，所以on()可以非常方便的换掉bind() 替换live()在1.4之前相信大家非常喜欢使用live(),因为它可以把事件绑定到当前以及以后添加的元素上面，当然在1.4之后delegate()也可以做类似的事情了。live()的原理很简单，它是通过document进行事件委派的，因此我们也可以使用on()通过将事件绑定到document来达到live()一样的效果。 live()写法123 $('#list li').live('click', '#list li', function() &#123; //function code here. &#125;); on()写法123$(document).on('click', '#list li', function() &#123; //function code here.&#125;); 这里的关键就是第二个参数’selector’在起作用了。它是一个过滤器的作用，只有被选中元素的后代元素才会触发事件。 替换delegate()delegate()是1.4引入的，目的是通过祖先元素来代理委派后代元素的事件绑定问题，某种程度上和live()优点相似。只不过live()是通过document元素委派，而delegate则可以是任意的祖先节点。使用on()实现代理的写法和delegate()基本一致。 delegate()的写法123$('#list').delegate('li', 'click', function() &#123; //function code here. &#125;); on()写法123$('#list').on('click', 'li', function() &#123; //function code here.&#125;); 貌似第一个和第二个参数的顺序颠倒了一下，别的基本一样。 总结jQuery推出on()的目的有2个，一是为了统一接口，二是为了提高性能，所以从现在开始用on()替换bind(), live(), delegate吧。尤其是不要再用live()了，因为它已经处于不推荐使用列表了，随时会被干掉。如果只绑定一次事件，那接着用one()吧，这个没有变化。]]></content>
      <categories>
        <category>jQuery</category>
      </categories>
      <tags>
        <tag>jQuery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux awk命令详解]]></title>
    <url>%2F2014%2F01%2F06%2Flinux-awk%2F</url>
    <content type="text"><![CDATA[简介awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。 awk有3个不同版本: awk、nawk和gawk，未作特别说明，一般指gawk，gawk 是 AWK 的 GNU 版本。 awk其名称得自于它的创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母。实际上 AWK 的确拥有自己的语言： AWK 程序设计语言 ， 三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。 使用方法1awk &apos;&#123;pattern + action&#125;&apos; &#123;filenames&#125; 尽管操作可能会很复杂，但语法总是这样，其中 pattern 表示 AWK 在数据中查找的内容，而 action 是在找到匹配内容时所执行的一系列命令。花括号（{}）不需要在程序中始终出现，但它们用于根据特定的模式对一系列指令进行分组。 pattern就是要表示的正则表达式，用斜杠括起来。 awk语言的最基本功能是在文件或者字符串中基于指定规则浏览和抽取信息，awk抽取信息后，才能进行其他文本操作。完整的awk脚本通常用来格式化文本文件中的信息。 通常，awk是以文件的一行为处理单位的。awk每接收文件的一行，然后执行相应的命令，来处理文本。 调用awk有三种方式调用awk 1.命令行方式awk [-F field-separator] ‘commands’ input-file(s)其中，commands 是真正awk命令，[-F域分隔符]是可选的。 input-file(s) 是待处理的文件。在awk中，文件的每一行中，由域分隔符分开的每一项称为一个域。通常，在不指名-F域分隔符的情况下，默认的域分隔符是空格。 2.shell脚本方式将所有的awk命令插入一个文件，并使awk程序可执行，然后awk命令解释器作为脚本的首行，一遍通过键入脚本名称来调用。相当于shell脚本首行的：#!/bin/sh可以换成：#!/bin/awk 3.将所有的awk命令插入一个单独文件，然后调用：1awk -f awk-script-file input-file(s) 其中，-f选项加载awk-script-file中的awk脚本，input-file(s)跟上面的是一样的。 本章重点介绍命令行方式。 入门实例假设last -n 5的输出如下 123456[root@www ~]# last -n 5 &lt;==仅取出前五行root pts/1 192.168.1.100 Tue Feb 10 11:21 still logged inroot pts/1 192.168.1.100 Tue Feb 10 00:46 - 02:28 (01:41)root pts/1 192.168.1.100 Mon Feb 9 11:41 - 18:30 (06:48)dmtsai pts/1 192.168.1.100 Mon Feb 9 11:41 - 11:41 (00:00)root tty1 Fri Sep 5 14:09 - 14:10 (00:01) 如果只是显示最近登录的5个帐号 123456#last -n 5 | awk &apos;&#123;print $1&#125;&apos;rootrootrootdmtsairoot awk工作流程是这样的：读入有’\n’换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域。默认域分隔符是”空白键” 或 “[tab]键”,所以$1表示登录用户，$3表示登录用户ip,以此类推。 如果只是显示/etc/passwd的账户 12345#cat /etc/passwd |awk -F &apos;:&apos; &apos;&#123;print $1&#125;&apos; rootdaemonbinsys 这种是awk+action的示例，每行都会执行action{print $1}。 -F指定域分隔符为’:’。 如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以tab键分割 12345#cat /etc/passwd |awk -F &apos;:&apos; &apos;&#123;print $1&quot;\t&quot;$7&#125;&apos;root /bin/bashdaemon /bin/shbin /bin/shsys /bin/sh 如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以逗号分割,而且在所有行添加列名name,shell,在最后一行添加”blue,/bin/nosh”。 12345678cat /etc/passwd |awk -F &apos;:&apos; &apos;BEGIN &#123;print &quot;name,shell&quot;&#125; &#123;print $1&quot;,&quot;$7&#125; END &#123;print &quot;blue,/bin/nosh&quot;&#125;&apos;name,shellroot,/bin/bashdaemon,/bin/shbin,/bin/shsys,/bin/sh....blue,/bin/nosh awk工作流程是这样的：先执行BEGING，然后读取文件，读入有/n换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域， $0则表示所有域,$1表示第一个域,$n表示第n个域 ,随后开始执行模式所对应的动作action。接着开始读入第二条记录······直到所有的记录都读完，最后执行END操作。 搜索/etc/passwd有root关键字的所有行 12#awk -F: &apos;/root/&apos; /etc/passwdroot:x:0:0:root:/root:/bin/bash 这种是pattern的使用示例，匹配了pattern(这里是root)的行才会执行action(没有指定action，默认输出每行的内容)。 搜索支持正则，例如找root开头的: awk -F: ‘/^root/‘ /etc/passwd 搜索/etc/passwd有root关键字的所有行，并显示对应的shell 12# awk -F: &apos;/root/&#123;print $7&#125;&apos; /etc/passwd /bin/bash 这里指定了action{print $7} awk内置变量awk有许多内置变量用来设置环境信息，这些变量可以被改变，下面给出了最常用的一些变量。 1234567891011ARGC 命令行参数个数ARGV 命令行参数排列ENVIRON 支持队列中系统环境变量的使用FILENAME awk浏览的文件名FNR 浏览文件的记录数FS 设置输入域分隔符，等价于命令行 -F选项NF 浏览记录的域的个数NR 已读的记录数OFS 输出域分隔符ORS 输出记录分隔符RS 控制记录分隔符 此外, $0变量是指整条记录。$1表示当前行的第一个域,$2表示当前行的第二个域,……以此类推。 统计/etc/passwd:文件名，每行的行号，每行的列数，对应的完整行内容: 12345#awk -F &apos;:&apos; &apos;&#123;print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF &quot;,linecontent:&quot;$0&#125;&apos; /etc/passwdfilename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/bashfilename:/etc/passwd,linenumber:2,columns:7,linecontent:daemon:x:1:1:daemon:/usr/sbin:/bin/shfilename:/etc/passwd,linenumber:3,columns:7,linecontent:bin:x:2:2:bin:/bin:/bin/shfilename:/etc/passwd,linenumber:4,columns:7,linecontent:sys:x:3:3:sys:/dev:/bin/sh 使用printf替代print,可以让代码更加简洁，易读 1awk -F &apos;:&apos; &apos;&#123;printf(&quot;filename:%10s,linenumber:%s,columns:%s,linecontent:%s\n&quot;,FILENAME,NR,NF,$0)&#125;&apos; /etc/passwd print和printfawk中同时提供了print和printf两种打印输出的函数。 其中print函数的参数可以是变量、数值或者字符串。字符串必须用双引号引用，参数用逗号分隔。如果没有逗号，参数就串联在一起而无法区分。这里，逗号的作用与输出文件的分隔符的作用是一样的，只是后者是空格而已。 printf函数，其用法和c语言中printf基本相似,可以格式化字符串,输出复杂时，printf更加好用，代码更易懂。 awk编程 变量和赋值 除了awk的内置变量，awk还可以自定义变量。 下面统计/etc/passwd的账户人数1234awk &apos;&#123;count++;print $0;&#125; END&#123;print &quot;user count is &quot;, count&#125;&apos; /etc/passwdroot:x:0:0:root:/root:/bin/bash......user count is 40 count是自定义变量。之前的action{}里都是只有一个print,其实print只是一个语句，而action{}可以有多个语句，以;号隔开。 这里没有初始化count，虽然默认是0，但是妥当的做法还是初始化为0: 12345awk &apos;BEGIN &#123;count=0;print &quot;[start]user count is &quot;, count&#125; &#123;count=count+1;print $0;&#125; END&#123;print &quot;[end]user count is &quot;, count&#125;&apos; /etc/passwd[start]user count is 0root:x:0:0:root:/root:/bin/bash...[end]user count is 40 统计某个文件夹下的文件占用的字节数12ls -l |awk &apos;BEGIN &#123;size=0;&#125; &#123;size=size+$5;&#125; END&#123;print &quot;[end]size is &quot;, size&#125;&apos;[end]size is 8657198 如果以M为单位显示:12ls -l |awk &apos;BEGIN &#123;size=0;&#125; &#123;size=size+$5;&#125; END&#123;print &quot;[end]size is &quot;, size/1024/1024,&quot;M&quot;&#125;&apos; [end]size is 8.25889 M 注意，统计不包括文件夹的子目录。 条件语句awk中的条件语句是从C语言中借鉴来的，见如下声明方式： 12345678910111213141516171819if (expression) &#123; statement; statement; ... ...&#125;if (expression) &#123; statement;&#125; else &#123; statement2;&#125;if (expression) &#123; statement1;&#125; else if (expression1) &#123; statement2;&#125; else &#123; statement3;&#125; 统计某个文件夹下的文件占用的字节数,过滤4096大小的文件(一般都是文件夹):12ls -l |awk &apos;BEGIN &#123;size=0;print &quot;[start]size is &quot;, size&#125; &#123;if($5!=4096)&#123;size=size+$5;&#125;&#125; END&#123;print &quot;[end]size is &quot;, size/1024/1024,&quot;M&quot;&#125;&apos; [end]size is 8.22339 M 循环语句 awk中的循环语句同样借鉴于C语言，支持while、do/while、for、break、continue，这些关键字的语义和C语言中的语义完全相同。 数组 因为awk中数组的下标可以是数字和字母，数组的下标通常被称为关键字(key)。值和关键字都存储在内部的一张针对key/value应用hash的表格里。由于hash不是顺序存储，因此在显示数组内容时会发现，它们并不是按照你预料的顺序显示出来的。数组和变量一样，都是在使用时自动创建的，awk也同样会自动判断其存储的是数字还是字符串。一般而言，awk中的数组用来从记录中收集信息，可以用于计算总和、统计单词以及跟踪模板被匹配的次数等等。 显示/etc/passwd的账户 12345678awk -F &apos;:&apos; &apos;BEGIN &#123;count=0;&#125; &#123;name[count] = $1;count++;&#125;; END&#123;for (i = 0; i &lt; NR; i++) print i, name[i]&#125;&apos; /etc/passwdrootdaemonbinsyssyncgames......]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux sed命令详解]]></title>
    <url>%2F2014%2F01%2F05%2Flinux-sed%2F</url>
    <content type="text"><![CDATA[功能说明： 利用script来处理文本文件。 语 法： sed [-hnV][-e script][-f script文件][文本文件] 补充说明： sed可依照script的指令，来处理、编辑文本文件。 参 数：-e script或—expression=script 以选项中指定的script来处理输入的文本文件。-f script文件或—file=script文件 以选项中指定的script文件来处理输入的文本文件。-h或—help 显示帮助。-n或—quiet或–silent 仅显示script处理后的结果。-V或—version 显示版本信息。 例 子：1$ sed -e 's/123/1234/' a.txt 将a.txt文件中所有行中的123用1234替换（-e表示命令以命令行的方式执行；参数s，表示执行替换操作） 1$ sed -e '3,5 a4' a.txt 将a.txt文件中的3行到5行之间所有行的后面添加一行内容为4的行（参数a，表示添加行，参数a后面指定添加的内容） 1$ sed -e '1 s/12/45/' a.txt 把第一行的12替换成45 1$ sed -i "s/oldstring/newstring/g" `grep oldstring -rl yourdir` 批量处理通过grep搜索出来的所有文档，将这些文档中所有的oldstring用newstring替换（-i参数表示直接对目标文件操作） 1$ sed -n 's/^test/mytest/p' example.file (-n)选项和p标志一起使用表示只打印那些发生替换的行。也就是说，如果某一行开头的test被替换成mytest，就打印它。(^这是正则表达式中表示开头，该符号后面跟的就是开头的字符串)（参数p表示打印行） 1$ sed 's/^wangpan/&amp;19850715/' example.file 表示被替换换字符串被找到后，被替换的字符串通过＆符号连接给出的字符串组成新字符传替换被替换的字符串,所有以wangpan开头的行都会被替换成它自已加19850715，变成wangpan19850715 1$ sed -n 's/\(love\)able/\1rs/p' example.file love被标记为1，所有loveable会被替换成lovers，而且替换的行会被打印出来。需要将这条命令分解，s/是表示替换操作，(love)表示选中love字符串，(love)able/表示包含loveable的行，(love)able/\l表示love字符串标记为1，表示在替换过程中不变。rs/表示替换的目标字符串。这条命令的操作含义：只打印替换了的行 1$ sed 's#10#100#g' example.file 不论什么字符，紧跟着s命令的都被认为是新的分隔符，所以，“#”在这里是分隔符，代替了默认的“/”分隔符。表示把所有10替换成100。 1$ sed -n '/love/,/unlove/p' example.file 只打印包含love字符串行到包含unlove字符串行之间的所有行（确定行的范围就是通过逗号实现的） 1$ sed -n '5,/^wang/p' example 只打印从第五行开始到第一个包含以wang开始的行之间的所有行 1$ sed '/love/,/unlove/s/$/wangpan/' example.file 对于包含love字符串的行到包含unlove字符串之间的行，每行的末尾用字符串wangpan替换。字符串$/表示以字符串结尾的行，$/表示每一行的结尾，s/$/wangpan/表示每一行的结尾添加wangpan字符串 1$ sed -e '11,53d' -e 's/wang/pan/' example.file (-e)选项允许在同一行里执行多条命令。如例子所示，第一条命令删除11至53行，第二条命令用pan替换wang。命令的执行顺序对结果有影响。如果两个命令都是替换命令，那么第一个替换命令将影响第二个替换命令的结果。(参数d，表示删除指定的行) 1$ sed --expression='s/wang/pan/' --expression='/love/d' example.file 一个比-e更好的命令是–expression。它能给sed表达式赋值。 1$ sed '/wangpan/r file' example.file file里的内容被读进来，显示在与wangpan匹配的行后面，如果匹配多行，则file的内容将显示在所有匹配行的下面。参数r，表示读出文件，后面空格紧跟文件名称 1$ sed -n '/test/w file' example.file 在example.file中所有包含test的行都被写入file里。参数w，表示将匹配的行写入到指定的文件file中 1$ sed '/^test/a\oh! My god!' example.file ‘oh! My god!’被追加到以test开头的行的后面，sed要求参数a后面有一个反斜杠。 1$ sed '/test/i\oh! My god!' example.file ‘oh! My god!’被追加到包含test字符串行的前面，参数i表示添加指定内容到匹配行的前面，sed要求参数i后面有一个反斜杠 1$ sed '/test/&#123; n; s/aa/bb/; &#125;' example.file 如果test被匹配，则移动到匹配行的下一行，替换这一行的aa，变为bb。参数n，表示读取匹配行的下一个输入行，用下一个命令处理新的行而不是匹配行。Sed要求参数n后跟分号 1$ sed '1,10y/abcde/ABCDE/' example.file 把1—10行内所有abcde转变为大写，注意，正则表达式元字符不能使用这个命令。参数y，表示把一个字符翻译为另外的字符（但是不用于正则表达式） 1$ sed -i 's/now/right now/g' test_sed_command.txt 表示直接操作文件test_sed_command.txt，将文件test_sed_command.txt中所有的now用right now替换。参数-i，表示直接操作修改文件，不输出。 1$ sed '2q' test_sed_command.txt 在打印完第2行后，就直接退出sed。参数q，表示退出 1$ sed -e '/old/h' -e '/girl-friend/G' test_sed_command.txt 首先了解参数h，拷贝匹配成功行的内容到内存中的缓冲区。在了解参数G，获得内存缓冲区的内容，并追加到当前模板块文本的后面。上面命令行的含义：将包含old字符串的行的内容保存在缓冲区中，然后将缓冲区的内容拿出来添加到包含girl-friend字符串行的后面。隐含要求搜集到缓冲区的匹配行在需要添加行的前面。 1$ sed -e '/test/h' -e '/wangpan/x' example.file 将包含test字符串的行的内容保存在缓冲区中，然后再将缓冲区的内容替换包含wangpan字符串的行。参数x，表示行替换操作。隐含要求搜集到缓冲区的匹配行在需要被替换行的前面。]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>sed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux grep命令详解]]></title>
    <url>%2F2014%2F01%2F04%2Flinux-grep%2F</url>
    <content type="text"><![CDATA[简介grep (global search regular expression(RE) and print out the line,全面搜索正则表达式并把行打印出来)是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 Unix的grep家族包括grep、egrep和fgrep。egrep和fgrep的命令只跟grep有很小不同。egrep是grep的扩展，支持更多的re元字符， fgrep就是fixed grep或fast grep，它们把所有的字母都看作单词，也就是说，正则表达式中的元字符表示回其自身的字面意义，不再特殊。linux使用GNU版本的grep。它功能更强，可以通过-G、-E、-F命令行选项来使用egrep和fgrep的功能。 grep常用用法grep [-acinv] [–color=auto] ‘搜寻字符串’ filename选项与参数：-a ：将 binary 文件以 text 文件的方式搜寻数据-c ：计算找到 ‘搜寻字符串’ 的次数-i ：忽略大小写的不同，所以大小写视为相同-n ：顺便输出行号-v ：反向选择，亦即显示出没有 ‘搜寻字符串’ 内容的那一行！–color=auto ：可以将找到的关键词部分加上颜色的显示喔！ 将/etc/passwd，有出现 root 的行取出来1234567grep root /etc/passwdroot:x:0:0:root:/root:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologin或cat /etc/passwd | grep root root:x:0:0:root:/root:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologin 将/etc/passwd，有出现 root 的行取出来,同时显示这些行在/etc/passwd的行号123grep -n root /etc/passwd1:root:x:0:0:root:/root:/bin/bash30:operator:x:11:0:operator:/root:/sbin/nologin 在关键字的显示方面，grep 可以使用 –color=auto 来将关键字部分使用颜色显示。 这可是个很不错的功能啊！但是如果每次使用 grep 都得要自行加上 –color=auto 又显的很麻烦～ 此时那个好用的 alias 就得来处理一下啦！你可以在 ~/.bashrc 内加上这行：alias grep=&#39;grep --color=auto&#39;再以source ~/.bashrc来立即生效即可喔！ 这样每次运行 grep 他都会自动帮你加上颜色显示啦 将/etc/passwd，将没有出现 root 的行取出来123grep -v root /etc/passwdroot:x:0:0:root:/root:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologin 将/etc/passwd，将没有出现 root 和nologin的行取出来123grep -v root /etc/passwd | grep -v nologinroot:x:0:0:root:/root:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologin 用 dmesg 列出核心信息，再以 grep 找出内含 eth 那行,要将捉到的关键字显色，且加上行号来表示：12345[root@www ~]# dmesg | grep -n --color=auto 'eth'247:eth0: RealTek RTL8139 at 0xee846000, 00:90:cc:a6:34:84, IRQ 10248:eth0: Identified 8139 chip type 'RTL-8139C'294:eth0: link up, 100Mbps, full-duplex, lpa 0xC5E1305:eth0: no IPv6 routers present 你会发现除了 eth 会有特殊颜色来表示之外，最前面还有行号喔！在关键字的显示方面，grep 可以使用 –color=auto 来将关键字部分使用颜色显示。 这可是个很不错的功能啊！但是如果每次使用 grep 都得要自行加上 –color=auto 又显的很麻烦～ 此时那个好用的 alias 就得来处理一下啦！你可以在 ~/.bashrc 内加上这行：alias grep=&#39;grep --color=auto&#39;再以source ~/.bashrc来立即生效即可喔！ 这样每次运行 grep 他都会自动帮你加上颜色显示啦 用 dmesg 列出核心信息，再以 grep 找出内含 eth 那行,在关键字所在行的前两行与后三行也一起捉出来显示12345678[root@www ~]# dmesg | grep -n -A3 -B2 --color=auto 'eth'245-PCI: setting IRQ 10 as level-triggered246-ACPI: PCI Interrupt 0000:00:0e.0[A] -&gt; Link [LNKB] ...247:eth0: RealTek RTL8139 at 0xee846000, 00:90:cc:a6:34:84, IRQ 10248:eth0: Identified 8139 chip type 'RTL-8139C'249-input: PC Speaker as /class/input/input2250-ACPI: PCI Interrupt 0000:00:01.4[B] -&gt; Link [LNKB] ...251-hdb: ATAPI 48X DVD-ROM DVD-R-RAM CD-R/RW drive, 2048kB Cache, UDMA(66) 如上所示，你会发现关键字 247 所在的前两行及 248 后三行也都被显示出来！这样可以让你将关键字前后数据捉出来进行分析啦！ 根据文件内容递归查找目录123grep ‘energywise’ * #在当前目录搜索带'energywise'行的文件grep -r ‘energywise’ * #在当前目录及其子目录下搜索'energywise'行的文件grep -l -r ‘energywise’ * #在当前目录及其子目录下搜索'energywise'行的文件，但是不显示匹配的行，只显示匹配的文件 grep与正规表达式字符类 字符类的搜索：如果我想要搜寻 test 或 taste 这两个单字时，可以发现到，其实她们有共通的 ‘t?st’ 存在～这个时候，我可以这样来搜寻：123[root@www ~]# grep -n 't[ae]st' regular_express.txt8:I can't finish the test.9:Oh! The soup taste good. 其实 [] 里面不论有几个字节，他都谨代表某一个字节， 所以，上面的例子说明了，我需要的字串是tast或test两个字串而已！ 字符类的反向选择 [^] ：如果想要搜索到有 oo 的行，但不想要 oo 前面有 g，如下12345[root@www ~]# grep -n '[^g]oo' regular_express.txt2:apple is my favorite food.3:Football game is not use feet only.18:google is the best tools for search keyword.19:goooooogle yes! 第 2,3 行没有疑问，因为 foo 与 Foo 均可被接受！ 但是第 18 行明明有 google 的 goo 啊～别忘记了，因为该行后面出现了 tool 的 too 啊！所以该行也被列出来～ 也就是说， 18 行里面虽然出现了我们所不要的项目 (goo) 但是由於有需要的项目 (too) ， 因此，是符合字串搜寻的喔！ 至於第 19 行，同样的，因为 goooooogle 里面的 oo 前面可能是 o ，例如： go(ooo)oogle ，所以，这一行也是符合需求的！ 字符类的连续：再来，假设我 oo 前面不想要有小写字节，所以，我可以这样写 [^abcd….z]oo ， 但是这样似乎不怎么方便，由於小写字节的 ASCII 上编码的顺序是连续的， 因此，我们可以将之简化为底下这样：12[root@www ~]# grep -n '[^a-z]oo' regular_express.txt3:Football game is not use feet only. 也就是说，当我们在一组集合字节中，如果该字节组是连续的，例如大写英文/小写英文/数字等等， 就可以使用[a-z],[A-Z],[0-9]等方式来书写，那么如果我们的要求字串是数字与英文呢？ 呵呵！就将他全部写在一起，变成：[a-zA-Z0-9]。 我们要取得有数字的那一行，就这样：123[root@www ~]# grep -n '[0-9]' regular_express.txt5:However, this dress is about $ 3183 dollars.15:You are the best is mean you are the no. 1. 行首与行尾字节 ^ $行首字符：如果我想要让 the 只在行首列出呢？ 这个时候就得要使用定位字节了！我们可以这样做：12[root@www ~]# grep -n '^the' regular_express.txt12:the symbol '*' is represented as start. 此时，就只剩下第 12 行，因为只有第 12 行的行首是 the 开头啊～此外， 如果我想要开头是小写字节的那一行就列出呢？可以这样：12345678[root@www ~]# grep -n '^[a-z]' regular_express.txt2:apple is my favorite food.4:this dress doesn't fit me.10:motorcycle is cheap than car.12:the symbol '*' is represented as start.18:google is the best tools for search keyword.19:goooooogle yes!20:go! go! Let's go. 如果我不想要开头是英文字母，则可以是这样：123[root@www ~]# grep -n '^[^a-zA-Z]' regular_express.txt1:"Open Source" is a good mechanism to develop programs.21:# I am VBird ^ 符号，在字符类符号(括号[])之内与之外是不同的！ 在 [] 内代表反向选择，在 [] 之外则代表定位在行首的意义！ 那如果我想要找出来，行尾结束为小数点 (.) 的那一行：12345678910111213[root@www ~]# grep -n '\.$' regular_express.txt1:"Open Source" is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.4:this dress doesn't fit me.10:motorcycle is cheap than car.11:This window is clear.12:the symbol '*' is represented as start.15:You are the best is mean you are the no. 1.16:The world &lt;Happy&gt; is the same with "glad".17:I like dog.18:google is the best tools for search keyword.20:go! go! Let's go. 特别注意到，因为小数点具有其他意义(底下会介绍)，所以必须要使用转义字符(\)来加以解除其特殊意义！ 找出空白行：12[root@www ~]# grep -n '^$' regular_express.txt22: 因为只有行首跟行尾 (^$)，所以，这样就可以找出空白行啦！ 任意一个字节 . 与重复字节 *这两个符号在正则表达式的意义如下： . (小数点)：代表一定有一个任意字节的意思；* (星号)：代表重复前一个字符， 0 到无穷多次的意思，为组合形态 假设我需要找出 g??d 的字串，亦即共有四个字节， 起头是 g 而结束是 d ，我可以这样做：1234[root@www ~]# grep -n 'g..d' regular_express.txt1:"Open Source" is a good mechanism to develop programs.9:Oh! The soup taste good.16:The world &lt;Happy&gt; is the same with "glad". 因为强调 g 与 d 之间一定要存在两个字节，因此，第 13 行的 god 与第 14 行的 gd 就不会被列出来啦！ 如果我想要列出有 oo, ooo, oooo 等等的数据， 也就是说，至少要有两个(含) o 以上，该如何是好？ 因为 * 代表的是重复 0 个或多个前面的 RE 字符的意义， 因此 o* 代表的是：拥有空字节或一个 o 以上的字节，因此，grep -n &#39;o*&#39; regular_express.txt将会把所有的数据都列印出来终端上！ 当我们需要至少两个 o 以上的字串时，就需要 ooo* ，亦即是：1234567[root@www ~]# grep -n 'ooo*' regular_express.txt1:"Open Source" is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.9:Oh! The soup taste good.18:google is the best tools for search keyword.19:goooooogle yes! 如果我想要字串开头与结尾都是 g，但是两个 g 之间仅能存在至少一个 o ，亦即是 gog, goog, gooog…. 等等，那该如何？123[root@www ~]# grep -n 'goo*g' regular_express.txt18:google is the best tools for search keyword.19:goooooogle yes! 如果我想要找出 g 开头与 g 结尾的行，当中的字符可有可无123456[root@www ~]# grep -n 'g.*g' regular_express.txt1:"Open Source" is a good mechanism to develop programs.14:The gd software is a library for drafting programs.18:google is the best tools for search keyword.19:goooooogle yes!20:go! go! Let's go. 因为是代表 g 开头与 g 结尾，中间任意字节均可接受，所以，第 1, 14, 20 行是可接受的喔！ 这个 .* 的 RE 表示任意字符是很常见的. 如果我想要找出任意数字的行？因为仅有数字，所以就成为：123[root@www ~]# grep -n '[0-9][0-9]*' regular_express.txt5:However, this dress is about $ 3183 dollars.15:You are the best is mean you are the no. 1. 限定连续 RE 字符范围 {}我们可以利用 . 与 RE 字符及 * 来配置 0 个到无限多个重复字节， 那如果我想要限制一个范围区间内的重复字节数呢？ 举例来说，我想要找出两个到五个 o 的连续字串，该如何作？这时候就得要使用到限定范围的字符 {} 了。 但因为 { 与 } 的符号在 shell 是有特殊意义的，因此， 我们必须要使用字符 \ 来让他失去特殊意义才行。 至於 {} 的语法是这样的，假设我要找到两个 o 的字串，可以是：1234567[root@www ~]# grep -n 'o\&#123;2\&#125;' regular_express.txt1:"Open Source" is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.9:Oh! The soup taste good.18:google is the best tools for search ke19:goooooogle yes! 假设我们要找出 g 后面接 2 到 5 个 o ，然后再接一个 g 的字串，他会是这样：12[root@www ~]# grep -n 'go\&#123;2,5\&#125;g' regular_express.txt18:google is the best tools for search keyword. 如果我想要的是 2 个 o 以上的 goooo….g 呢？除了可以是 gooo*g ，也可以是：123[root@www ~]# grep -n 'go\&#123;2,\&#125;g' regular_express.txt18:google is the best tools for search keyword.19:goooooogle yes! 扩展grep(grep -E 或者 egrep)：使用扩展grep的主要好处是增加了额外的正则表达式元字符集。 打印所有包含NW或EA的行。如果不是使用egrep，而是grep，将不会有结果查出。123egrep 'NW|EA' testfile northwest NW Charles Main 3.0 .98 3 34 eastern EA TB Savage 4.4 .84 5 20 对于标准grep，如果在扩展元字符前面加\，grep会自动启用扩展选项-E。123grep 'NW\|EA' testfilenorthwest NW Charles Main 3.0 .98 3 34eastern EA TB Savage 4.4 .84 5 20 搜索所有包含一个或多个3的行。12345678egrep '3+' testfilegrep -E '3+' testfilegrep '3\+' testfile #这3条命令将会northwest NW Charles Main 3.0 .98 3 34western WE Sharon Gray 5.3 .97 5 23northeast NE AM Main Jr. 5.1 .94 3 13central CT Ann Stephens 5.7 .94 5 13 搜索所有包含0个或1个小数点字符的行。1234567egrep '2\.?[0-9]' testfile grep -E '2\.?[0-9]' testfilegrep '2\.\?[0-9]' testfile #首先含有2字符，其后紧跟着0个或1个点，后面再是0和9之间的数字。western WE Sharon Gray 5.3 .97 5 23southwest SW Lewis Dalsass 2.7 .8 2 18eastern EA TB Savage 4.4 .84 5 20 搜索一个或者多个连续的no的行。123456egrep '(no)+' testfilegrep -E '(no)+' testfilegrep '\(no\)\+' testfile #3个命令返回相同结果，northwest NW Charles Main 3.0 .98 3 34northeast NE AM Main Jr. 5.1 .94 3 13north NO Margot Weber 4.5 .89 5 9 不使用正则表达式 fgrep 查询速度比grep命令快，但是不够灵活：它只能找固定的文本，而不是规则表达式。 如果你想在一个文件或者输出中找到包含星号字符的行12345fgrep '*' /etc/profilefor i in /etc/profile.d/*.sh ; do或grep -F '*' /etc/profilefor i in /etc/profile.d/*.sh ; do]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jQuery的.bind()、.live()和.delegate()之间区别]]></title>
    <url>%2F2014%2F01%2F04%2Fjquery-bind-delegate%2F</url>
    <content type="text"><![CDATA[摘要: jQuery的.bind()、.live()和.delegate()之间的区别并非总是那么明显的，然而，如果我们对所有的不同之处都有清晰的理解的话，那么这将会有助于我们编写出更加简洁的代码，以及防止在交互应用中弹出错误。 DOM树 首先，可视化一个HMTL文档的DOM树是很有帮助的。一个简单的HTML页面看起来就像是这个样子： 事件冒泡(又称事件传播) 当我们点击一个链接时，其触发了链接元素的单击事件，该事件则引发任何我们已绑定到该元素的单击事件上的函数的执行。 1$('a').bind('click', function() &#123; alert("That tickles!") &#125;); 因此一个单击操作会触发alert函数的执行。 click事件接着会向树的根方向传播，广播到父元素，然后接着是每个祖先元素，只要是它的某个后代元素上的单击事件被触发，事件就会传给它。 在操纵DOM的语境中，document是根节点。 现在我们可以较容易地说明.bind()、.live()和.delegate()的不同之处了。 .bind() 1$('a').bind('click', function() &#123; alert("That tickles!") &#125;); 这是最简单的绑定方法了。JQuery扫描文档找出所有的$(‘a’)元素，并把alert函数绑定到每个元素的click事件上。 .live() 1$('a').live('click', function() &#123; alert("That tickles!") &#125;); JQuery把alert函数绑定到$(document)元素上，并使用’click’和’a’作为参数。任何时候只要有事件冒泡到document节点上，它就查看该事件是否是一个click事件，以及该事件的目标元素与’a’这一CSS选择器是否匹配，如果都是的话，则执行函数。 live方法还可以被绑定到具体的元素(或“context”)而不是document上，像这样： 1$('a', $('#container')[0]).live(...); .delegate() 1$('#container').delegate('a', 'click', function() &#123; alert("That tickles!") &#125;); JQuery扫描文档查找$(‘#container’)，并使用click事件和’a’这一CSS选择器作为参数把alert函数绑定到$(‘#container’)上。任何时候只要有事件冒泡到$(‘#container’)上，它就查看该事件是否是click事件，以及该事件的目标元素是否与CCS选择器相匹配。如果两种检查的结果都为真的话，它就执行函数。 可以注意到，这一过程与.live()类似，但是其把处理程序绑定到具体的元素而非document这一根上。精明的JS’er们可能会做出这样的结论，即$(‘a’).live() == $(document).delegate(‘a’)，是这样吗?嗯，不，不完全是。 为什么.delegate()要比.live()好用 基于几个原因，人们通常更愿意选用jQuery的delegate方法而不是live方法。考虑下面的例子： 123$('a').live('click', function() &#123; blah() &#125;); // 或者 $(document).delegate('a', 'click', function() &#123; blah() &#125;); 速度 后者实际上要快过前者，因为前者首先要扫描整个的文档查找所有的$(‘a’)元素，把它们存成jQuery对象。尽管live函数仅需要把’a’作为串参数传递以用做之后的判断，但是$()函数并未“知道”被链接的方法将会是.live()。 而另一方面，delegate方法仅需要查找并存储$(document)元素。 一种寻求避开这一问题的方法是调用在$(document).ready()之外绑定的live，这样它就会立即执行。在这种方式下，其会在DOM获得填充之前运行，因此就不会查找元素或是创建jQuery对象了。 灵活性和链能力 live函数也挺令人费解的。想想看，它被链到$(‘a’)对象集上，但其实际上是在$(document)对象上发生作用。由于这个原因，它能够试图以一种吓死人的方式来把方法链到自身上。实际上，我想说的是，以$.live(‘a’,…)这一形式作为一种全局性的jQuery方法，live方法会更具意义一些。 仅支持CSS选择器 最后一点，live方法有一个非常大的缺点，那就是它仅能针对直接的CSS选择器做操作，这使得它变得非常的不灵活。 欲了解更多关于CSS选择器的缺点，请参阅Exploring jQuery .live() and .die()一文。 更新：感谢Hacker News上的pedalpete和后面评论中的Ellsass提醒我加入接下来的这一节内容。 为什么选择.live()或.delegate()而不是.bind() 毕竟，bind看起来似乎更加的明确和直接，难道不是吗?嗯，有两个原因让我们更愿意选择delegate或live而不是bind： 为了把处理程序附加到可能还未存在于DOM中的DOM元素之上。因为bind是直接把处理程序绑定到各个元素上，它不能把处理程序绑定到还未存在于页面中的元素之上。 如果你运行了$(‘a’).bind(…)，而后新的链接经由AJAX加入到了页面中，则你的bind处理程序对于这些新加入的链接来说是无效的。而另一方面live和delegate则是被绑定到另一个祖先节点上，因此其对于任何目前或是将来存在于该祖先元素之内的元素都是有效的。 或者为了把处理程序附加到单个元素上或是一小组元素之上，监听后代元素上的事件而不是循环遍历并把同一个函数逐个附加到DOM中的100个元素上。把处理程序附加到一个(或是一小组)祖先元素上而不是直接把处理程序附加到页面中的所有元素上，这种做法带来了性能上的好处。 停止传播 最后一个我想做的提醒与事件传播有关。通常情况下，我们可以通过使用这样的事件方法来终止处理函数的执行： 12345$('a').bind('click', function(e) &#123; e.preventDefault(); // 或者 e.stopPropagation(); &#125;); 不过，当我们使用live或是delegate方法的时候，处理函数实际上并没有在运行，需要等到事件冒泡到处理程序实际绑定的元素上时函数才会运行。而到此时为止，我们的其他的来自.bind()的处理函数早已运行了。 原文地址：http://developer.51cto.com/art/201103/249694.htm]]></content>
      <categories>
        <category>jQuery</category>
      </categories>
      <tags>
        <tag>jQuery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正确理解ThreadLocal]]></title>
    <url>%2F2014%2F01%2F04%2Fthreadlocal-understanding%2F</url>
    <content type="text"><![CDATA[首先，ThreadLocal 不是用来解决共享对象的多线程访问问题的，一般情况下，通过ThreadLocal.set() 到线程中的对象是该线程自己使用的对象，其他线程是不需要访问的，也访问不到的。 各个线程中访问的是不同的对象。 另外，说ThreadLocal使得各线程能够保持各自独立的一个对象，并不是通过ThreadLocal.set()来实现的，而是通过每个线程中的new 对象 的操作来创建的对象，每个线程创建一个，不是什么对象的拷贝或副本。 通过ThreadLocal.set()将这个新创建的对象的引用保存到各线程的自己的一个map中，每个线程都有这样一个map，执行ThreadLocal.get()时，各线程从自己的map中取出放进去的对象，因此取出来的是各自自己线程中的对象，ThreadLocal实例是作为map的key来使用的。如果ThreadLocal.set()进去的东西本来就是多个线程共享的同一个对象，那么多个线程的ThreadLocal.get()取得的还是这个共享对象本身，还是有并发访问问题。 下面来看一个hibernate中典型的ThreadLocal的应用： 1234567891011121314private static final ThreadLocal threadSession = new ThreadLocal(); public static Session getSession() throws InfrastructureException &#123; Session s = (Session) threadSession.get(); try &#123; if (s == null) &#123; s = getSessionFactory().openSession(); threadSession.set(s); &#125; &#125; catch (HibernateException ex) &#123; throw new InfrastructureException(ex); &#125; return s; &#125; 可以看到在getSession()方法中，首先判断当前线程中有没有放进去session，如果还没有，那么通过sessionFactory().openSession()来创建一个session，再将session set到线程中，实际是放到当前线程的ThreadLocalMap这个map中，这时，对于这个session的唯一引用就是当前线程中的那个ThreadLocalMap（下面会讲到），而threadSession作为这个值的key，要取得这个session可以通过threadSession.get()来得到，里面执行的操作实际是先取得当前线程中的ThreadLocalMap，然后将threadSession作为key将对应的值取出。这个session相当于线程的私有变量，而不是public的。显然，其他线程中是取不到这个session的，他们也只能取到自己的ThreadLocalMap中的东西。要是session是多个线程共享使用的，那还不乱套了。试想如果不用ThreadLocal怎么来实现呢？可能就要在action中创建session，然后把session一个个传到service和dao中，这可够麻烦的。或者可以自己定义一个静态的map，将当前thread作为key，创建的session作为值，put到map中，应该也行，这也是一般人的想法，但事实上，ThreadLocal的实现刚好相反，它是在每个线程中有一个map，而将ThreadLocal实例作为key，这样每个map中的项数很少，而且当线程销毁时相应的东西也一起销毁了，不知道除了这些还有什么其他的好处。 总之，ThreadLocal不是用来解决对象共享访问问题的，而主要是提供了保持对象的方法和避免参数传递的方便的对象访问方式。归纳了两点：1。每个线程中都有一个自己的ThreadLocalMap类对象，可以将线程自己的对象保持到其中，各管各的，线程可以正确的访问到自己的对象。2。将一个共用的ThreadLocal静态实例作为key，将不同对象的引用保存到不同线程的ThreadLocalMap中，然后在线程执行的各处通过这个静态ThreadLocal实例的get()方法取得自己线程保存的那个对象，避免了将这个对象作为参数传递的麻烦。 当然如果要把本来线程共享的对象通过ThreadLocal.set()放到线程中也可以，可以实现避免参数传递的访问方式，但是要注意get()到的是那同一个共享对象，并发访问问题要靠其他手段来解决。但一般来说线程共享的对象通过设置为某类的静态变量就可以实现方便的访问了，似乎没必要放到线程中。 ThreadLocal的应用场合，我觉得最适合的是按线程多实例（每个线程对应一个实例）的对象的访问，并且这个对象很多地方都要用到。 下面来看看ThreadLocal的实现原理（jdk1.5源码）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124public class ThreadLocal&lt;T&gt; &#123; /** * ThreadLocals rely on per-thread hash maps attached to each thread * (Thread.threadLocals and inheritableThreadLocals). The ThreadLocal * objects act as keys, searched via threadLocalHashCode. This is a * custom hash code (useful only within ThreadLocalMaps) that eliminates * collisions in the common case where consecutively constructed * ThreadLocals are used by the same threads, while remaining well-behaved * in less common cases. */ private final int threadLocalHashCode = nextHashCode(); /** * The next hash code to be given out. Accessed only by like-named method. */ private static int nextHashCode = 0; /** * The difference between successively generated hash codes - turns * implicit sequential thread-local IDs into near-optimally spread * multiplicative hash values for power-of-two-sized tables. */ private static final int HASH_INCREMENT = 0x61c88647; /** * Compute the next hash code. The static synchronization used here * should not be a performance bottleneck. When ThreadLocals are * generated in different threads at a fast enough rate to regularly * contend on this lock, memory contention is by far a more serious * problem than lock contention. */ private static synchronized int nextHashCode() &#123; int h = nextHashCode; nextHashCode = h + HASH_INCREMENT; return h; &#125; /** * Creates a thread local variable. */ public ThreadLocal() &#123; &#125; /** * Returns the value in the current thread's copy of this thread-local * variable. Creates and initializes the copy if this is the first time * the thread has called this method. * * @return the current thread's value of this thread-local */ public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) return (T)map.get(this); // Maps are constructed lazily. if the map for this thread // doesn't exist, create it, with this ThreadLocal and its // initial value as its only entry. T value = initialValue(); createMap(t, value); return value; &#125; /** * Sets the current thread's copy of this thread-local variable * to the specified value. Many applications will have no need for * this functionality, relying solely on the &#123;@link #initialValue&#125; * method to set the values of thread-locals. * * @param value the value to be stored in the current threads' copy of * this thread-local. */ public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); &#125; /** * Get the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @return the map */ ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals; &#125; /** * Create the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @param firstValue value for the initial entry of the map * @param map the map to store. */ void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue); &#125; ....... /** * ThreadLocalMap is a customized hash map suitable only for * maintaining thread local values. No operations are exported * outside of the ThreadLocal class. The class is package private to * allow declaration of fields in class Thread. To help deal with * very large and long-lived usages, the hash table entries use * WeakReferences for keys. However, since reference queues are not * used, stale entries are guaranteed to be removed only when * the table starts running out of space. */ static class ThreadLocalMap &#123; ........ &#125;&#125; 可以看到ThreadLocal类中的变量只有这3个int型： 123private final int threadLocalHashCode = nextHashCode(); private static int nextHashCode = 0; private static final int HASH_INCREMENT = 0x61c88647; 而作为ThreadLocal实例的变量只有 threadLocalHashCode 这一个，nextHashCode 和HASH_INCREMENT 是ThreadLocal类的静态变量，实际上HASH_INCREMENT是一个常量，表示了连续分配的两个ThreadLocal实例的threadLocalHashCode值的增量，而nextHashCode 的表示了即将分配的下一个ThreadLocal实例的threadLocalHashCode 的值。 可以来看一下创建一个ThreadLocal实例即new ThreadLocal()时做了哪些操作，从上面看到构造函数ThreadLocal()里什么操作都没有，唯一的操作是这句： 1private final int threadLocalHashCode = nextHashCode(); 那么nextHashCode()做了什么呢： 12345private static synchronized int nextHashCode() &#123; int h = nextHashCode; nextHashCode = h + HASH_INCREMENT; return h; &#125; 就是将ThreadLocal类的下一个hashCode值即nextHashCode的值赋给实例的threadLocalHashCode，然后nextHashCode的值增加HASH_INCREMENT这个值。 因此ThreadLocal实例的变量只有这个threadLocalHashCode，而且是final的，用来区分不同的ThreadLocal实例，ThreadLocal类主要是作为工具类来使用，那么ThreadLocal.set()进去的对象是放在哪儿的呢？ 看一下上面的set()方法，两句合并一下成为 1ThreadLocalMap map = Thread.currentThread().threadLocals; 这个ThreadLocalMap 类是ThreadLocal中定义的内部类，但是它的实例却用在Thread类中： 12345678public class Thread implements Runnable &#123; ...... /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; ......&#125; 再看这句：12if (map != null) map.set(this, value); 也就是将该ThreadLocal实例作为key，要保持的对象作为值，设置到当前线程的ThreadLocalMap 中，get()方法同样大家看了代码也就明白了，ThreadLocalMap 类的代码太多了，我就不帖了，自己去看源码吧。 贴个例子吧： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class Student &#123; private int age = 0; //年龄 public int getAge() &#123; return this.age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; public class ThreadLocalDemo implements Runnable &#123; //创建线程局部变量studentLocal，在后面你会发现用来保存Student对象 private final static ThreadLocal studentLocal = new ThreadLocal(); public static void main(String[] agrs) &#123; ThreadLocalDemo td = new ThreadLocalDemo(); Thread t1 = new Thread(td, "a"); Thread t2 = new Thread(td, "b"); t1.start(); t2.start(); &#125; public void run() &#123; accessStudent(); &#125; /** * 示例业务方法，用来测试 */ public void accessStudent() &#123; //获取当前线程的名字 String currentThreadName = Thread.currentThread().getName(); System.out.println(currentThreadName + " is running!"); //产生一个随机数并打印 Random random = new Random(); int age = random.nextInt(100); System.out.println("thread " + currentThreadName + " set age to:" + age); //获取一个Student对象，并将随机数年龄插入到对象属性中 Student student = getStudent(); student.setAge(age); System.out.println("thread " + currentThreadName + " first read age is:" + student.getAge()); try &#123; Thread.sleep(500); &#125; catch (InterruptedException ex) &#123; ex.printStackTrace(); &#125; System.out.println("thread " + currentThreadName + " second read age is:" + student.getAge()); &#125; protected Student getStudent() &#123; //获取本地线程变量并强制转换为Student类型 Student student = (Student) studentLocal.get(); //线程首次执行此方法的时候，studentLocal.get()肯定为null if (student == null) &#123; //创建一个Student对象，并保存到本地线程变量studentLocal中 student = new Student(); studentLocal.set(student); &#125; return student; &#125;&#125; 结果：12345678a is running! thread a set age to:76 b is running! thread b set age to:27 thread a first read age is:76 thread b first read age is:27 thread a second read age is:76 thread b second read age is:27 原文地址：http://www.iteye.com/topic/103804]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>threadlocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vi/vim 编辑器命令]]></title>
    <url>%2F2014%2F01%2F03%2Flinux-vi%2F</url>
    <content type="text"><![CDATA[vi/vim 用法笔记。 按 i 进入编辑模式 （一开始是命令模式的） 按 esc 进入命令模式 在命令模式中输入:wq 保存 （write and quit） 在命令模式中输入:q 退出 在命令模式中输入:q! 强制退出 (不保存) 在命令模式中输入/后接关键字搜索，n下一个匹配，N上一个匹配 在命令模式中输入gg 回到顶部，GG到底部 在命令模式中输入dd 删除当前行 在命令模式中输入x 删除后一个字符，X删除前一个字符 在命令模式中输入u undo最后一次修改 在命令模式中输入yy 复制当前行 在命令模式中输入p 粘贴到光标后面，P粘贴到光标前面 未完待续…]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vi</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2014%2F01%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to my blog, This is my first post.so hello world]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>hello</tag>
        <tag>world</tag>
      </tags>
  </entry>
</search>
